Universal Approximation Theorem: Classical and Modern Perspectives

Classical Formulation Review

The given statement is essentially a correct summary of the classical
Universal Approximation Theorem as first proven by Cybenko
(1989). Cybenko showed that a single-hidden-layer feedforward network
with a finite number of neurons can approximate any continuous
function on a compact domain (typically the unit hypercube in
$\mathbb{R}^n$) to arbitrary accuracy, provided the activation
function in the hidden layer is a continuous, non-constant sigmoidal
function ￼. Here sigmoidal means a bounded, monotonically increasing
continuous function that saturates to different limits as $x \to
\pm\infty$ (for example, the logistic sigmoid or $\tanh$) ￼. In plain
terms, Cybenko’s theorem established that with one hidden layer and
any continuous sigmoidal nonlinearity, one can uniformly approximate
any target continuous function on $[0,1]^n$ as closely as desired ￼
￼. This matches the statement given by the user, which assumes a
non-constant, bounded, monotonic continuous activation – conditions
under which the original theorem holds.

However, the formulation in the statement, while basically technically
correct, omits some details and is not the most general form of the
theorem. A more precise phrasing is: for any continuous function $f$
on a compact domain in $\mathbb{R}^n$ and any $\varepsilon>0$, there
exists a feedforward network with one hidden layer (finite number of
neurons) and a suitable activation $\sigma$ (non-constant, bounded,
continuous, monotonic) such that the network’s output uniformly
approximates $f$ within error $\varepsilon$ ￼. Important to note is
that the network must be allowed to have as many neurons as needed –
the theorem guarantees existence of a finite-width network for any
desired accuracy, but it does not say that a single fixed-size network
can approximate every continuous function. In other words, the number
of neurons required typically grows with the complexity of the target
function and the desired accuracy. The statement “with just a single
hidden layer containing a finite number of neurons” is true in the
sense that the network need not be infinitely wide, but one may need
to increase that finite number for more complex functions or tighter
approximation error.

Two other subtle conditions in the classical theorem deserve
mention. First, the theorem assumes the input domain is compact (often
taken as $[0,1]^n$ or any compact subset of $\mathbb{R}^n$) and the
approximation is measured in the uniform norm (supremum norm) on that
domain ￼. The user’s statement says “continuous multidimensional
function” without explicitly stating the domain or norm; implicitly,
it’s the space $C(K)$ of continuous functions on a compact $K \subset
\mathbb{R}^n$, with uniform convergence. Without the compactness
assumption, one cannot guarantee uniform approximation over an
unbounded domain (no finite network can uniformly approximate an
arbitrary continuous function on all of $\mathbb{R}^n$ without
additional conditions, since the function might grow unbounded or
oscillate). Secondly, the network architecture assumed in the
classical theorem includes a linear output layer. The hidden layer
uses the non-linear activation, and then the outputs of hidden neurons
are combined linearly to produce the final output. This is usually
understood, but not always explicitly stated – the theorem does not
hold if the output were passed through a non-linearity that restricts
the class of representable functions (in Cybenko’s and Hornik’s
results, the output is typically a linear combination of hidden-unit
activations) ￼ ￼.

Another point is that the classical theorem’s conditions on the
activation function (namely “non-constant, bounded, and
monotonically-increasing continuous”) are sufficient but not
necessary. Cybenko chose a sigmoidal activation in his proof, and
Hornik et al. (1989) independently showed a similar result for
so-called “squashing” functions (which are likewise bounded and
monotonic) ￼. These conditions were convenient for those proofs (often
using the Stone–Weierstrass theorem or functional analysis
techniques). But later research clarified that the essential
requirement is that the activation function be non-polynomial. In
1991, Hornik showed that it is not a special property of sigmoids per
se, but the general two-layer network architecture that confers
universality – in fact, a wide range of activation functions can lead
to universal approximation ￼. Specifically, a famous result by Leshno,
Lin, Pinkus, and Schocken (1993) proved that a feedforward network is
a universal approximator if and only if the activation function is not
a polynomial (on any interval) ￼. If the activation were a polynomial,
the network’s output is just a polynomial in the input, and such
networks cannot approximate most continuous functions. But any
continuous, non-polynomial activation (even if it’s not bounded or not
monotonic) gives a network that is dense in $C(K)$. This
necessary-and-sufficient condition subsumes the earlier “bounded,
monotonic” assumptions as a special case. For example, Hornik (1991)
noted that many activation functions – including sigmoids, $\tanh$,
Gaussians, etc. – work, and that “it is not the specific choice of the
activation function but rather the multilayer feed-forward
architecture itself” that yields the universal approximation property,
as long as the activation is not trivial ￼. In summary, the user’s
formulation is a correct statement of one classical version (for
sigmoidal activations), but it is not the most general form. Missing
is the clarification that any non-polynomial activation will do (the
function need not be bounded or strictly monotonic; e.g. $\sin(x)$ or
ReLU also work, as discussed below), and that one typically considers
approximation on compact domains in the uniform norm ￼ ￼.

It’s also worth mentioning what the classical theorem does not
guarantee. It does not guarantee an efficient approximation or a
specific method to find the correct parameters – it is an existence
theorem. The proofs are non-constructive: for any $\varepsilon$ one
knows there exists a network of some size $N(\varepsilon)$ that
achieves error $\varepsilon$, but the theorems do not give a formula
for $N(\varepsilon)$ nor a way to learn or compute the required
weights. Additionally, the classical UAT was stated for continuous
target functions. If a target function is not continuous, a
single-hidden-layer continuous network might not approximate it well
in a uniform sense (for instance, approximating a jump discontinuity
uniformly is impossible with continuous outputs – there will always be
some maximal error at the jump). In those cases one must resort to
weaker notions of approximation (e.g. $L^p$ approximation or
convergence in measure, as discussed next). Finally, the universal
approximation theorems initially were for networks of arbitrary width
but a fixed single hidden layer (depth 2). The role of depth was not
part of the classical theorem – the classical results all say “one
hidden layer suffices” (given enough neurons). We will see how modern
extensions consider depth and other function spaces.

In summary, the statement given is an accurate description of the
classical UAT in the case of a sigmoidal activation: yes, a
one-hidden-layer network with a finite (though potentially large)
number of neurons can uniformly approximate any continuous
multi-dimensional function on a compact domain, under the stated
conditions on the activation ￼ ￼. The only amendments for completeness
would be: (i) explicitly restrict to continuous functions on a compact
set (for uniform approximation), (ii) clarify that “arbitrary
accuracy” means for every $\varepsilon>0$ there exists a (possibly
large) finite network achieving that error, and (iii) note that the
activation being “non-constant, bounded, and monotonically increasing”
is one common assumption (covering the standard sigmoids like logistic
or $\tanh$) but not the weakest possible condition – essentially any
non-polynomial, non-constant activation works ￼. With these
clarifications, the statement aligns well with the classical theorem
of Cybenko (1989) ￼ and Hornik et al. (1989) in its intent and scope.

Modern Extensions

Activation functions beyond bounded sigmoids: One key development
after the classical results was recognizing that unbounded or
non-monotonic activation functions can also yield universal
approximation. The universal approximation property is much more
general than “sigmoids.” As noted, Hornik (1991) and later Leshno et
al. (1993) showed that as long as the activation is not a polynomial,
a single-hidden-layer network can approximate any continuous function
￼. This means that popular unbounded activations like the ReLU
($\sigma(x)=\max{0,x}$) or even less regular functions can still
produce a universal approximator. ReLU is non-constant,
piecewise-linear (certainly not a polynomial globally), and indeed it
satisfies the conditions for universality in the modern sense. In
fact, one can construct piecewise linear approximations to any
continuous function using ReLU neurons. Intuitively, a single ReLU
neuron gives a kink (a linear region changing slope at one point); by
combining many ReLUs, we can form a piecewise linear function with
arbitrarily many segments, which can approximate a given continuous
curve as finely as needed. Thus, yes – ReLU networks are universal
approximators of continuous functions on compact sets, even though
ReLU is not bounded ￼. The earlier “bounded, monotonic” requirement
turned out to be overly restrictive: what really matters is that the
set of functions generated by the network is rich enough (in technical
terms, the linear spans of $\sigma(w^T x + b)$ are dense in
$C(K)$). ReLU passes this test because it’s not a trivial (polynomial)
function. We could also mention that even discontinuous activation
like the Heaviside step function can lead to universal approximation:
a step function is actually the limit of sigmoids, and a network with
step activations can produce piecewise-constant approximations to a
continuous target. By making enough small step jumps, one can
approximate any continuous function uniformly (this is analogous to
how a Riemann sum of step functions can approximate an integral
curve). In fact, it’s known that a single-layer network with threshold
units is a universal approximator in a limiting sense – formally, any
continuous function can be approximated by a sequence of threshold
networks (or by a threshold network with sufficiently many neurons)
￼. The step activation is discontinuous, so the approximation might
not be uniformly perfect unless one allows an increasing number of
neurons, but in practice one can get arbitrarily close. More
rigorously, any activation that can approximate a sigmoid (even a
discontinuous one like a step which is the limit of sigmoids) will
inherit the universal approximation capability ￼. In summary, modern
treatments remove the necessity for boundedness or monotonicity: the
broad condition is that the activation must be non-constant and not a
polynomial ￼. Under that condition, a one-hidden-layer network can
densely approximate continuous functions. This covers ReLU (unbounded,
piecewise-linear), leaky ReLU, ELU, sinusoid ($\sin x$ is periodic and
bounded but non-monotonic), Gaussian radial functions, etc. There are
even results for discontinuous activations: e.g., a network with a
step activation can approximate any Borel measurable function in the
limit (using probability metrics) ￼, and one can often smooth or
mollify a discontinuous activation to apply the classical results ￼.

Is a single hidden layer sufficient for ReLU?  Yes, in terms of
existence of an approximating network, a single hidden layer
(depth-$2$ network) is sufficient for universal approximation, even
with ReLU activations. There is nothing about ReLU that fundamentally
requires more layers to approximate continuous functions – depth-$2$
is enough to be a universal approximator (since ReLU is
non-polynomial). However, recent research has revealed some nuances
when considering width constraints. If we fix a hard limit on the
number of ReLU neurons (the width), there are limits to what a shallow
network can do. A 2017 result by Lu et al. showed that a ReLU network
of width $n+1$ (where $n$ is the input dimension) is sufficient to
approximate any continuous function in $\mathbb{R}^n$, but a network
of width $\leq n$ is not universal ￼ ￼. In fact, they proved a “phase
transition” in expressiveness: a ReLU network with width $n$ or less
cannot approximate all continuous functions on $\mathbb{R}^n$
(technically, all but a measure-zero subset of continuous functions
require width $>n$), whereas width $n+4$ (in their proof, later
improved to $n+1$ by other authors) suffices for universality ￼ ￼. For
example, if you have a single-input ($n=1$) function, you need at
least 2 ReLU neurons in one hidden layer to approximate an arbitrary
continuous function on [0,1]; a width-1 network (essentially one ReLU
plus linear output) is not universal because it can only produce one
breakpoint in a piecewise linear output. In general, with $n$ inputs,
having $n+1$ ReLU neurons allows the network to carve out flexible
polyhedral regions in $\mathbb{R}^n$ needed to separate and
approximate any target function’s variation ￼. But these results don’t
contradict the original UAT – they rather refine it by identifying
minimal architectures. In practice, the classical UAT assumes you can
make the hidden layer as wide as needed. If one hidden layer is
allowed to grow without bound in width, ReLU does not need extra
layers for universality. To summarize: a single hidden layer is
sufficient for ReLU networks to be universal approximators (no
additional depth is required for the existence of an approximating
network) ￼. Depth, however, can trade off with width in interesting
ways (discussed next). If we restrict depth to 2, we must allow
arbitrary width for the theorem to hold; conversely, if we severely
restrict width (e.g., fix width = $n$), then we do need additional
depth to regain universality for multivariate functions ￼.

Benefits of depth (modern perspective): While one hidden layer can
approximate anything given enough neurons, deeper networks can be much
more parameter-efficient or expressive for certain types of
functions. Modern research in approximation theory of neural networks
has shown that there are functions which a shallow network would need
exponentially many neurons to approximate, whereas a deeper network
(with more layers but far fewer neurons per layer) can approximate the
same functions with exponentially fewer units (this is often called a
“depth efficiency” or “depth separation” result). For example,
Telgarsky (2016) exhibited specific continuous functions that are
effectively hard for shallow networks (requiring an exponential number
of units in terms of $1/\varepsilon$), but easy for deep networks
(requiring only a polynomial number of units) – intuitively, these are
highly oscillatory or compositional functions where additional layers
capture hierarchical structure more efficiently. Yarotsky (2017)
provided rigorous upper and lower bounds on approximation error rates,
showing that deep ReLU networks approximate smooth functions much more
efficiently than shallow ones ￼. In one result, Yarotsky constructs a
family of 1-dimensional Lipschitz functions and shows a fixed-depth-6
ReLU network can approximate them with far fewer pieces than a
single-hidden-layer network would need ￼. More generally, for
functions in certain smoothness or Hölder classes on $[0,1]^d$, a
depth-$L$ ReLU network can achieve an approximation error
$\varepsilon$ with a number of weights that grows as a polynomial in
$1/\varepsilon$, whereas any depth-$2$ network might require a much
larger (sometimes exponential in $1/\varepsilon$ or in $d$) number of
weights to achieve the same accuracy ￼ ￼. In essence, deeper networks
are still universal approximators (since they can simulate a shallow
network if needed), but they add extra power: they can realize certain
compositions and shared features that approximate complex functions
with significantly fewer resources. One can say the Universal
Approximation Theorem is qualitatively true for shallow and deep
networks alike – depth does not expand the space of functions that can
be approximated to some accuracy (both shallow and deep can
approximate any continuous function given enough units), but depth can
greatly reduce the cost (number of units or weights) to achieve a
given accuracy for specific function families ￼. Recent papers by Zhou
Lu et al. (2017) and Hanin & Sellke (2018) formalized the notion of
“width vs. depth” by showing, as mentioned, that a certain minimal
width is needed for universality, and conversely that networks of
sufficient width and minimal depth are universal ￼, while other works
provide explicit approximation rate bounds for deep networks ￼. In
summary, modern results uphold that a single hidden layer is
sufficient for approximation (universality holds), but they also
emphasize that extra depth can be leveraged to approximate functions
more efficiently (with far fewer neurons) than a single huge layer
would require ￼.

Approximation beyond continuous functions (L^p spaces and measurable
functions): The classical UAT was stated for continuous target
functions and uniform approximation. A natural question is: can neural
networks approximate functions that are not continuous, for example
functions in $L^p$ (which may have discontinuities or be defined only
almost-everywhere)? The answer is largely yes, with some
qualifications on how we measure error. In their 1989 paper, Hornik,
Stinchcombe, and White actually proved a stronger result: not only are
networks with one hidden layer dense in $C(K)$ (continuous functions
on compact $K$) under uniform convergence, they are also dense in
$L^p$ spaces and even in the space of measurable functions under
convergence in probability ￼. Specifically, for any Borel measurable
target function $f(x)$ (even discontinuous) and any probability
measure $\mu$ on the input space, a single-hidden-layer network can
approximate $f$ arbitrarily well in the $L^p(\mu)$ norm or in
$\mu$-measure (this means for any $\delta>0$, the network can be
chosen such that the measure of the set ${x: |f(x) -
\text{network}(x)| > \delta}$ is as small as desired) ￼. Hornik et
al. showed that if $\sigma$ is a squashing function (bounded
non-constant), then the network can approximate any measurable
function to arbitrary accuracy in this sense, making it a universal
approximator not just on $C(K)$ but also on $L^p$ spaces ￼. In plainer
terms, even if a function has jump discontinuities, a neural network
can approximate it as closely as you want except on a set of
arbitrarily small measure. For example, a step function target can be
approached by a neural network that almost everywhere matches the
step, only having small errors around the jump (and those error
regions can be made arbitrarily narrow in measure). This is a powerful
extension: it implies neural networks are dense in $L^p([0,1]^n)$ for
$1 \le p < \infty$, given suitable activations. Indeed, corollaries of
Hornik’s work noted that if you restrict the input to a compact set
and consider (say) the $L^2$ metric, one-hidden-layer networks with a
wide class of activations can approximate any square-integrable
function to arbitrary $L^2$ accuracy ￼. The universal approximation
property thus covers not just nice continuous functions, but also
ugly, highly irregular functions in an average-error sense. To avoid
confusion, continuous target functions are needed for uniform
(sup-norm) approximation; if a function is not continuous, you
typically relax to $L^p$ approximation. But as soon as you allow that
weaker criterion, neural networks have dense coverage of a huge
function space (basically all $L^p$ functions on compact domains, or
all measurable functions up to sets of measure zero) ￼. This
underscores that the “universal” approximation property is extremely
broad – essentially any function you might encounter in practice
(which would likely be at least Lebesgue measurable) can be
approximated by a neural network as closely as you’d like, provided
you choose an appropriate error metric.

It is also interesting to note extensions to noncompact domains or
specialized function classes. The universal approximation theorem has
been extended in various ways: for instance, there are results
guaranteeing approximation on $\mathbb{R}^n$ (noncompact) by
introducing a decay condition or working in $L^p(\mathbb{R}^n)$
instead of uniform norm ￼. There are also variants for different
network architectures: e.g., networks with radial basis functions
(Gaussian activations in a hidden layer) are universal approximators
of continuous functions as well ￼. Recurrent networks and
convolutional networks have their own versions of universal
approximation theorems under suitable assumptions ￼. Additionally, the
Kolmogorov–Arnold representation theorem provides a constructive
universal representation with two hidden layers (though with a very
specific, non-sigmoidal activation function), which was an early
theoretical result showing that two hidden layers suffice to exactly
represent any continuous function of $n$ variables using $(2n+1)$
neurons in each layer ￼. That result is more of theoretical interest
(the constructed activation is highly non-standard), but it
historically influenced neural network research by showing depth-3
networks have strong expressive power. Modern universal approximation
results, however, do not require such exotic activations – standard
activations like ReLU, $\tanh$, logistic, softplus, etc., all yield
universal function approximators in one hidden layer or more ￼.

Summary of activation conditions: In various settings, different mild conditions are assumed on the activation function $\sigma$:
	•	For the classical shallow network UAT (Cybenko, Hornik, etc.), a typical condition is: $\sigma:\mathbb{R}\to\mathbb{R}$ must be continuous, bounded, and not a trivial (constant or linear) function. Monotonicity was assumed by Cybenko (since he focused on sigmoidals) but later shown to be unnecessary ￼. Boundedness was in Cybenko’s assumption; Hornik (1991) allowed even unbounded continuous functions as long as they are non-polynomial ￼. The most general condition is: $\sigma$ should be non-polynomial (and at least locally integrable or continuous to avoid pathological cases). If this holds, a single-hidden-layer network with $\sigma$ is dense in $C(K)$ ￼. If $\sigma$ is also nice (e.g., continuous and non-constant), then not only $C(K)$ but also $L^p(K)$ density holds ￼.
	•	For deep networks, any activation that works for shallow also works for deep (they can only gain approximation power with more layers). In fact, even if an activation is polynomial, a deep network could in theory stack those polynomials and produce non-polynomial effects (though a fixed architecture with polynomial activation is still just a polynomial overall, so it’s not universal unless depth is allowed to grow with the approximation task – a scenario not usually considered).
	•	ReLU ($\max{0,x}$): This activation is piecewise linear and unbounded. It is continuous (though not differentiable at 0) and non-polynomial. By the Leshno/Pinkus criterion, a one-hidden-layer ReLU network is a universal approximator ￼. No additional conditions like boundedness or monotonicity are needed (ReLU is monotonic non-decreasing, but that wasn’t crucial – it’s mentioned just to contrast that earlier proofs liked monotonic sigmoid functions, while ReLU shows monotonic but unbounded works too).
	•	Polynomial activations: These are the only ones ruled out – e.g., if $\sigma(x)=x$ (a linear activation), the network is just a linear model, not universal. If $\sigma(x)=x^2$, a single-layer network outputs a polynomial in the inputs (of degree 2), which cannot approximate arbitrary continuous functions (it’s a very limited family). Thus, polynomial $\sigma$ fail universality ￼.
	•	Smoothness: Interestingly, smoothness of $\sigma$ is not required for universality. Even a non-differentiable activation like ReLU or a barely continuous one like a sawtooth wave (which is periodic and non-polynomial) can be universal. Some results (e.g., using Stone–Weierstrass) might assume $\sigma$ is continuous to ensure the approximation space is nice, but in principle as long as one can approximate a continuous activation by your chosen activation, you can transfer the UAT. For example, a discontinuous step function can approximate a sigmoid; hence a network with step units can emulate a sigmoid-network to arbitrary precision, establishing universality for step activations as well ￼.
	•	In specialized two-layer cases (e.g., Maiorov & Pinkus 1998), certain analytic monotonic activations were constructed to achieve universality with two hidden layers of fixed small width – but that’s beyond the classical scope and into current research frontiers ￼ ￼.

To conclude, the classical UAT taught us that a single-hidden-layer network with a suitable activation (sigmoid or others) can approximate any continuous function on a compact set as well as we want ￼. The modern view broadens this by removing overly strict conditions on $\sigma$ (e.g. ReLU works just fine ￼) and by exploring the trade-offs between network width and depth. We now understand that while depth-2 networks are universal, deeper networks can achieve the same approximations with significantly fewer neurons ￼. Moreover, the approximation capability isn’t confined to continuous functions; it extends to $L^p$ functions and even to arbitrary measurable functions in an appropriate sense ￼. These results, from Cybenko (1989) and Hornik (1989/91) to more recent works by Lu et al. (2017) ￼, Hanin & Sellke (2018), and Yarotsky (2017/2018) ￼, collectively paint a picture that neural networks are extremely expressive: any function we might care to approximate (continuous or otherwise) can in principle be represented to arbitrary accuracy by a network, given sufficient resources. The practical questions then center on how large or deep a network is needed for a given task – and that is where modern research continues to refine the theorem with quantitative estimates and efficiency considerations ￼ ￼.

Sources:
	•	G. Cybenko (1989). Approximation by superpositions of a sigmoidal function. MCSS 2, 303–314. (Universal approximation with one hidden layer and sigmoidal activation) ￼ ￼.
	•	K. Hornik, M. Stinchcombe, H. White (1989). Multilayer feedforward networks are universal approximators. Neural Networks 2(5), 359–366. (Extended UAT to any non-constant continuous activation; density in $C(K)$ and $L^p$ spaces) ￼ ￼.
	•	K. Hornik (1991). Approximation capabilities of multilayer feedforward networks. Neural Networks 4(2), 251–257. (Showed architecture is what matters; any non-polynomial activation yields universality) ￼.
	•	M. Leshno, V. Lin, A. Pinkus, S. Schocken (1993). Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural Networks 6(6), 861–867. (Necessary and sufficient condition for UAT is that activation is not a polynomial) ￼.
	•	A. Pinkus (1999). Approximation theory of the MLP model. Acta Numerica 8, 143–195. (Survey of universal approximation results and polynomial condition) ￼.
	•	Zhou Lu, et al. (2017). The expressive power of neural networks: A view from the width. NeurIPS 2017. (Proved width-$n+4$ ReLU networks are universal in $\mathbb{R}^n$, and width-$n$ are not; depth efficiency results) ￼.
	•	D. Yarotsky (2017). Error bounds for approximations with deep ReLU networks. Neural Networks 94, 103–114. (Established that deep ReLU nets approximate smooth functions with far fewer resources than shallow nets) ￼.
	•	D. Yarotsky (2018). Optimal approximation by very deep ReLU networks. Conference on Learning Theory (COLT) 2018. (Characterized optimal approximation rates and the necessity of depth growing with complexity) ￼.
	•	Other references and reviews:  ￼ ￼ ￼ (Universal approximation variants and extensions).
