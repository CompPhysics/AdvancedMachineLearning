\documentclass[11pt,a4paper]{article}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{physics}
\usepackage{graphicx}

\geometry{margin=1in}

\title{\textbf{Discriminative and Generative Deep Learning Models:\\
A Mathematical and Computational Study}}
\author{}
\date{}

\begin{document}
\maketitle

\section*{Project Overview}

Deep learning methods can broadly be divided into \emph{discriminative
models}, which learn decision boundaries for labeled data, and
\emph{generative models}, which learn probability distributions over
data. Convolutional neural networks (CNNs) dominate modern
classification tasks, while generative models such as variational
autoencoders (VAEs), Boltzmann machines, and diffusion models provide
probabilistic descriptions of data and enable synthesis, uncertainty
quantification, and representation learning.

The goal of this project is to develop a unified mathematical and
computational understanding of these model classes. Students will
analyze classification and generative learning as optimization
problems over high-dimensional function spaces, emphasizing
probabilistic modeling, variational principles, and numerical
optimization.



\section{Classification with Convolutional Neural Networks}

A convolutional neural network defines a parametric mapping
\begin{equation}
f_\theta : \mathbb{R}^{H \times W \times C} \rightarrow \{1,\dots,K\},
\end{equation}
where inputs are structured data (e.g.\ images) and outputs are class labels.

Mathematically, CNNs combine:
\begin{itemize}
  \item Convolutional linear operators with local receptive fields,
  \item Nonlinear activation functions,
  \item Pooling and subsampling operations.
\end{itemize}

Students will analyze:
\begin{itemize}
  \item Convolutions as structured sparse linear maps,
  \item Translation equivariance and symmetry reduction,
  \item Parameter sharing and its effect on sample complexity.
\end{itemize}

The classification problem is formulated as empirical risk minimization with cross-entropy loss,
\begin{equation}
\mathcal{L}_{\text{clf}}(\theta) = -\frac{1}{N}\sum_{i=1}^N \log p_\theta(y_i \mid x_i),
\end{equation}
and optimized using stochastic gradient descent.

\section{Probabilistic Generative Modeling}

Generative models aim to learn an approximation $p_\theta(x)$ to an unknown data distribution. This project considers three complementary paradigms.

\subsection{Variational Autoencoders}

VAEs introduce latent variables $z$ and define
\begin{equation}
p_\theta(x,z) = p_\theta(x \mid z)p(z),
\end{equation}
with training based on variational inference.

Students will derive the evidence lower bound (ELBO),
\begin{equation}
\mathcal{L}_{\text{VAE}} = \mathbb{E}_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)] - \mathrm{KL}(q_\phi(z\mid x)\|p(z)),
\end{equation}
and analyze:
\begin{itemize}
  \item Encoder--decoder architectures,
  \item Reparameterization trick and differentiability,
  \item Trade-offs between reconstruction accuracy and latent regularization.
\end{itemize}

\subsection{Boltzmann Machines}

Boltzmann machines define energy-based models
\begin{equation}
p_\theta(x) = \frac{1}{Z_\theta} e^{-E_\theta(x)},
\end{equation}
where $Z_\theta$ is the partition function.

The project examines:
\begin{itemize}
  \item Energy landscapes and statistical mechanics analogies,
  \item Maximum likelihood learning and gradient structure,
  \item Approximate inference methods such as contrastive divergence.
\end{itemize}

Connections between Boltzmann machines and variational principles are emphasized.

\subsection{Diffusion Models}

Diffusion models define a forward noising process and a learned reverse-time denoising process. Training minimizes a denoising objective equivalent to variational inference.

Students will analyze:
\begin{itemize}
  \item Stochastic differential equations and discretization,
  \item Score matching and reverse-time dynamics,
  \item Relations to Langevin sampling and thermodynamics.
\end{itemize}

Diffusion models are interpreted as iterative numerical solvers for sampling from complex distributions.

\section{Discriminative vs Generative Learning}

A central theme of the project is the comparison between discriminative and generative objectives.

Topics include:
\begin{itemize}
  \item Decision boundaries vs density estimation,
  \item Sample efficiency and representation learning,
  \item Uncertainty quantification and out-of-distribution detection.
\end{itemize}

Students will explore how latent representations learned by generative models can support downstream classification tasks.

\section{Implementation and Experiments}

The practical component consists of implementing:
\begin{itemize}
  \item A CNN for image classification,
  \item At least one generative model (VAE, Boltzmann machine, or diffusion model).
\end{itemize}

Experiments will use standard labeled datasets and focus on:
\begin{itemize}
  \item Classification accuracy and confusion structure,
  \item Quality of generated samples,
  \item Latent-space geometry and interpolation.
\end{itemize}

Computational results are interpreted through the lens of the mathematical models.

\section*{Expected Outcomes}

By completing this project, students will:
\begin{itemize}
  \item Understand deep learning through probabilistic and variational principles,
  \item Connect classification and generation within a unified framework,
  \item Analyze neural networks as numerical optimization problems,
  \item Gain insight applicable to physics-inspired machine learning, statistical inference, and scientific data analysis.
\end{itemize}

\end{document}

