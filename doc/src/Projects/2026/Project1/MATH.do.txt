TITLE: Project 1
AUTHOR: "FYS5429/9429":"https://www.uio.no/studier/emner/matnat/fys/FYS5429/index-eng.html", Advanced machine learning and data analysis for the physical sciences, University of Oslo, Norway
DATE: Spring semester 2026, deadline March 20

======= Theoretical Foundations and Practical Implementation of Neural Networks for Regression and Classification =======

===== Objective =====

This project aims to bridge theoretical mathematics with practical
deep learning by developing neural networks for both regression and
classification tasks. The objective is to demonstrate how core
mathematical concepts such as linear algebra, multivariate calculus,
optimization theory, and probability, directly inform the design and
training of neural networks. In essence, familiar math tools (matrix
operations, derivatives, gradient-based optimization, probabilistic
loss functions) form the backbone of deep learning ￼. For example, we
will show how matrix algebra is used to compute neuron activations,
how calculus (chain rule) is employed in backpropagation to compute
gradients, how optimization theory guides weight updates via gradient
descent, and how probability underlies loss functions like
cross-entropy. By tying theory to implementation, the project will
illustrate that every practical behavior of a neural network
(convergence, generalization, etc.) has a mathematical explanation.

===== Building and Training Neural Networks (Regression & Classification) =====

To achieve the objective, the project will have a strong experimental component using PyTorch or TensorFlow. The plan includes:
* Neural Network Implementation: Develop two feedforward neural network models from scratch – one for a regression task and one for a classification task. Both models will be designed as multilayer perceptrons (MLPs) with one or more hidden layers (the depth may be adjusted to explore theoretical effects). The classification network will use a final softmax layer (for multi-class output) or sigmoid (for binary output), while the regression network will output a single continuous value (linear output neuron).
* Framework and Tools: Use PyTorch or TensorFlow to build and train these networks. This ensures we can leverage automatic differentiation to compute gradients (but we will also derive the gradients manually for a simple case to validate the theory). Training code will be modular to allow experimenting with different settings (activation functions, initialization schemes, etc.).
* Datasets: Train the classification network on a well-known labeled dataset (see Datasets section for suggestions like MNIST or CIFAR-10) and the regression network on a regression dataset (e.g. Boston Housing or California Housing). Data will be split into training, validation, and test sets to allow rigorous evaluation. We will perform necessary preprocessing (normalization of features, etc.) so that training is stable.
* Training Procedure: For each network, implement mini-batch gradient descent training with backpropagation. We will use an optimizer like stochastic gradient descent (SGD) or Adam; however, to keep theoretical clarity, initial experiments might use plain SGD to closely match mathematical analysis. Each epoch, the network’s weights will be updated by moving in the direction of negative gradient of the loss (with respect to the weights) scaled by a learning rate ￼. The general update rule for a weight $w$ will be:
$$w \leftarrow w - \eta \frac{\partial L}{\partial w},$$
where $L$ is the loss and $\eta$ is the learning rate ￼.
* Parallel Regression and Classification Workflows: The project will tackle regression and classification in parallel, highlighting their differences. For classification, use a suitable loss (cross-entropy) and evaluation metric (accuracy, etc.), whereas for regression use mean squared error loss. The training loop for each will record metrics on both training and validation sets for analysis.
* Iterative Experimentation: The plan includes systematically varying key parameters (initialization method, activation functions, regularization strength, etc.) and observing effects on convergence and performance. This will yield empirical evidence to support the theoretical discussions. For example, one experiment will train two identical networks with different weight initializations (one with a theoretically grounded method like Xavier, another with a naive initialization) to compare convergence behavior.


===== In-Depth Analysis of Key Theoretical Components =====

A significant part of the project is a rigorous mathematical analysis
of several key components of neural networks. In the written report,
we will devote sections to each of the following topics, explaining
the underlying theory and connecting it to our implementation results:

=== Weight Initialization and Convergence Behavior ===

Proper weight initialization is critical for training deep
networks. We will investigate how different initialization schemes
affect the convergence speed and stability of training. In theory, if
weights are too large or too small at start, the network can suffer
from exploding or vanishing activations as signals propagate through
layers. We will explain and implement common strategies: Xavier
(Glorot) initialization for layers with sigmoid/tanh activations, and
He initialization for ReLU activations. The Xavier/Glorot method
scales initial weights by the number of inputs and outputs of a layer
to maintain roughly equal variance of activations across layers ￼
￼. This helps ensure signals neither explode nor vanish as they move
forward or backward through the network ￼. In fact, “Xavier
initialization suggests that for each layer, the variance of outputs
is not affected by the number of inputs, and the variance of gradients
is not affected by the number of outputs”, helping to mitigate
vanishing/exploding gradients and accelerating convergence ￼. We will
illustrate this by monitoring activation distributions and gradient
norms in the network for different initialization
schemes. Additionally, we will discuss symmetry breaking, that is initializing
all weights to the same value is problematic because neurons would
remain identical; thus small randomization is needed to break symmetry
(we will show mathematically why identical initial weights lead to
zero gradient for certain weight directions ). Expected findings:
networks with Xavier/He initialization train much faster or attain
better accuracy than those with poorly scaled initialization,
confirming theory.

=== Gradient Descent and Backpropagation ===

This section will provide a deep dive into gradient-based
optimization. We will derive the backpropagation equations for our
neural network by applying the chain rule of calculus. Backpropagation
is the algorithm that efficiently computes the gradient of the loss
with respect to every weight in the network by moving backward from
the output layer . In essence, we will show that $\frac{\partial
L}{\partial w_{ij}^{(l)}}$ (the gradient of loss w.r.t. a weight in
layer $l$) can be computed by multiplying the error signal from the
layer $(l+1)$ by the activation of neuron $i$ in layer $l$ (and any
derivative of the activation function). We’ll start from the output
layer and propagate the errors back through each layer – this is where
multivariate calculus plays a key role (applying the chain rule
through the composed functions that make up the network) ￼. We will
relate this to the implementation: frameworks like PyTorch
automatically perform this backpropagation, but we will manually
verify it for a simple 2-layer network. Moreover, we will discuss
variants of gradient descent (batch vs mini-batch vs stochastic) and
how the choice of learning rate affects convergence (too small $\eta$
leads to slow progress, too large can cause divergence or
oscillation). The theoretical guarantee for convex problems is that
gradient descent converges to a global minimum under certain
conditions, but for non-convex neural network loss surfaces, only
local convergence can be guaranteed. Nonetheless, in practice, even
stochastic gradient descent (SGD) often finds good minima in deep
networks. We’ll cite that backpropagation combined with SGD
efficiently adjusts weights to minimize the loss ￼, and we will
present training curves (loss vs. epochs) to analyze convergence
behavior under various settings (learning rate schedules, momentum,
etc.).

=== Loss Landscapes and Optimization Challenges ===

Neural network training involves navigating a high-dimensional, non-convex loss landscape. We will theoretically discuss properties of these landscapes and the challenges they pose:
* Non-convexity and Local Minima: Unlike simple convex problems, a neural net’s loss function can have many local minima and saddle points due to its non-linear layered structure. However, recent theoretical insights suggest that in very high dimensions, most local minima have performance close to the global minimum, and poor local minima are rare; instead, saddle points are more common obstacles ￼. We’ll explain that in high-dimensional parameter spaces, a random initialization followed by SGD is unlikely to get truly “stuck” in a bad isolated minimum – plateaus or saddle points (where gradients are near zero but not minima) are more likely to slow training ￼.
* Flat vs. Sharp Minima: We will explore the concept that “flatter” minima (where the loss doesn’t increase rapidly in any parameter direction) tend to generalize better than “sharp” minima. Intuitively, flat minima imply the model is not overly sensitive to small changes in weights, suggesting it has found a robust solution ￼. We’ll connect this to theory by discussing the Hessian matrix of second derivatives: a flat minimum has small eigenvalues (low curvature) whereas a sharp minimum has some large eigenvalues. As an experiment, we might visualize a 2D slice of the loss surface around a found solution (using techniques from recent papers) to qualitatively see flat vs. sharp regions.
* Vanishing/Exploding Gradients: We will mathematically show why very deep networks can suffer vanishing gradients (gradients becoming exponentially small in earlier layers) or exploding gradients (blowing up) depending on weights and activation choices. This ties back to initialization and choice of activation. We’ll demonstrate this by measuring gradient norms in a toy deep network before and after applying proper initialization or normalization. The theory (supported by our earlier cite on Xavier initialization) is that maintaining controlled signal variance in forward and backward passes prevents these issues ￼. Techniques like batch normalization could also be touched upon as ways to reshape the loss landscape to be smoother and easier to optimize.
* Optimization Algorithms: Beyond basic SGD, we will briefly discuss how advanced optimizers (Momentum, RMSprop, Adam) attempt to deal with the challenges of the landscape by smoothing oscillations or adapting learning rates. For completeness, the project can compare plain SGD to Adam in training the networks and note differences (e.g. Adam might converge faster initially, but sometimes generalization differs).

By analytically and empirically examining the loss landscape, the project will highlight why training neural networks is difficult and how mathematical insights guide practical solutions. For instance, we expect to observe that with a high learning rate the training loss might bounce (indicating the optimizer jumping across a narrow ravine in the loss surface), whereas a properly tuned rate or using momentum leads to smoother descent into a minimum.

=== Activation Functions: Differentiability and Stability ===

Activation functions introduce the crucial non-linearities in neural networks. We will analyze common activation functions (sigmoid, tanh, ReLU, GeLU etc.) in terms of their mathematical properties (differentiability, range, gradient behavior) and impact on network training:
* Sigmoid ($\sigma$) and Tanh: These are smooth, differentiable functions that squash input into a limited range. Sigmoid outputs between 0 and 1, while tanh outputs between -1 and 1 (zero-centered). We will show $\sigma’(z) = \sigma(z)(1-\sigma(z))$, meaning gradients are maximal at moderate inputs but vanish at the extremes (both $\sigma$ and tanh gradients approach 0 for large $|z|$). This vanishing gradient problem can severely slow training in deeper layers, as early layers get almost no gradient if neurons saturate ￼ ￼. We will demonstrate this by observing that deep networks with sigmoid activations struggle to propagate error signals (e.g., training a deep sigmoid network might stall – an experiment could be comparing it to a ReLU network). Nonetheless, tanh being zero-centered often performs better than sigmoid in practice ￼ ￼. We will mathematically reason why zero-centered outputs help (gradients don’t go all in one direction for all weights, aiding optimization).
* ReLU (Rectified Linear Unit): ReLU$(z) = \max(0,z)$ is not differentiable at $z=0$ (we will mention this corner-case, though in implementation we define the derivative at 0 as 0 or 1 by convention). ReLU has a derivative of 1 for $z>0$ and 0 for $z<0$. This simple form has huge practical benefits: it does not saturate for positive inputs, so it mitigates vanishing gradients (gradients are 1 for active neurons) ￼. We will highlight that ReLU enables training of deeper networks by keeping gradients alive, but comes with the issue that neurons can “die” during training: if a neuron’s weights cause it to never receive positive inputs, it outputs 0 and its gradient is 0, so it may never update further ￼. This is the dying ReLU problem – we’ll possibly show an example of a neuron that went inactive and never recovered. We will also analyze ReLU’s unbounded output (which can cause some neurons to produce very large activations – we’ll discuss if that affected any training runs). ReLU’s piecewise linear nature makes it very computationally efficient and it has become the default choice for many networks.
* Other Activations: Briefly, we will mention alternatives like Leaky ReLU (which allows a small negative slope to avoid truly zero gradients for $z<0$) ￼ ￼, or Softplus (a smooth approximation to ReLU), and Softmax for multi-class output. The softmax function will be analyzed in the classification network context as it converts raw scores to a probability distribution over classes; we’ll relate it to probability theory and cross-entropy loss. If applicable, we might include a short experiment replacing ReLU with leaky ReLU or ELU to observe any differences, reinforcing the theory that small changes in activation function can sometimes solve problems like dying ReLUs at the cost of other trade-offs.

=== Regularization Methods (L1, L2, Dropout) ===

To address overfitting and improve generalization, we will incorporate and study various regularization techniques, examining their mathematical formulation and effect on the model:
* L2 Regularization (Weight Decay): We add a penalty term $\frac{\lambda}{2}\sum w^2$ to the loss (for all weights $w$) which analytically corresponds to a Gaussian prior on weights (probabilistic view) and encourages smaller weights. In practice, L2 regularization pulls weights toward zero continuously, preventing any single weight from growing too large. This tends to make the model smoother and less likely to overfit. Mathematically, we will show that performing gradient descent with an L2 term results in the update: $w \leftarrow w - \eta(\frac{\partial L}{\partial w} + \lambda w)$, which looks like weights decaying by a factor each step (hence “weight decay”). Effect on model: We’ll demonstrate that with L2, the model’s weights distribution has smaller variance and that often the validation performance improves (to verify, we can compare training with and without L2 on our tasks). We will mention that L2 does not force weights exactly to zero, but rather shrinks them while allowing all weights to contribute (important features keep larger weights) ￼. It’s known to improve generalization by keeping the model simpler (in a norm sense).
* L1 Regularization: We add a term $\lambda \sum |w|$ to the loss. L1 regularization tends to drive many weights to exact zero, effectively performing feature selection by inducing sparsity ￼ ￼. We’ll outline the mathematical reason: the L1 penalty creates a “diamond-shaped” contour for the constraint which often intersects steepest descent at coordinate axes (where some weights = 0). In comparison to L2’s circular contours, L1’s geometry favors sparse solutions ￼. We plan to run an experiment on a smaller problem (perhaps a regression on a dataset with many features) to see if L1 elimination of weights improves interpretability or if it hurts training dynamics (L1 can make the loss surface less smooth). The report will note that L1 can help interpretability by zeroing out useless inputs but can be harder to optimize (not differentiable at 0; subgradient methods are used, which the frameworks handle). We expect to observe some weights become exactly 0 when using L1, confirming the theory of sparsity ￼.
* Dropout: We will incorporate dropout layers in the networks to analyze this stochastic regularization technique. Dropout randomly drops (sets to zero) a fraction of neurons’ activations during each training batch ￼. For example, with a dropout rate of 0.5, each neuron has a 50% chance of being inactive on a given forward pass ￼. This means each update is training a slightly different thinned network, and effectively we are ensembling many subnetworks that share weights ￼. We will mathematically discuss how dropout corresponds to adding noise and how at test time an approximate averaging is done by using the full network with scaled weights. The project will include an experiment where we train the model with dropout and compare its training and validation curves to a no-dropout model. Typically, dropout increases training loss (since it makes the model less efficient to train, as it can’t rely on any one feature) but decreases overfitting, leading to lower validation loss if overfitting was present. We will confirm this behavior in our results. Additionally, from a theoretical standpoint, we will mention that dropout can be seen as adding noise to the network’s activations, which has connections to an L2-like effect (as per some research, dropout roughly corresponds to an adaptive weight decay). Our findings will likely show that dropout helps when the network is large relative to the data (preventing co-adaptation of neurons ￼), but if the network is small and data is sufficient, dropout might not give a significant advantage.
* Other Regularization Ideas: Briefly, we might note techniques like early stopping (monitor validation loss and stop training when it starts increasing – a practical regularization by limiting over-training) and data augmentation (for image tasks, generating variations of training samples to expand data). While not mathematically heavy, early stopping can be discussed as keeping the model in a simpler regime (not overly fitting noise).

In summarizing regularization, we will emphasize that each method can be framed in mathematical terms (L1/L2 as adding terms to the objective, dropout as sampling from an implicit distribution of networks) and that our experiments support the theory: models with appropriate regularization achieve better generalization (higher validation performance) than those without, especially in presence of limited data. For instance, we expect to report that using L2 or dropout reduced the gap between training and test accuracy in the classification task, confirming it controlled overfitting.

===== Datasets for Classification and Regression =====

Selecting well-structured, public datasets is important to ensure the project’s experiments are meaningful and reproducible. We will use one dataset for classification and one for regression, with the following candidates (feel free to replace).

=== Classification ===

* MNIST Handwritten Digits: A classic benchmark with 70,000 images of handwritten digits (0–9) in 10 classes ￼ ￼. Each image is 28×28 pixels grayscale. MNIST is ideal for a first classification task – it’s large enough (60k training, 10k test) to train a non-trivial network, yet simple enough for quick experiments. It’s a labeled dataset (each image has an associated digit label) and is very well-studied ￼. The project can use MNIST to demonstrate building a simple multi-layer network (or even a convolutional neural net extension if desired) and achieving high accuracy, while analyzing mistakes and learning curves.
* CIFAR-10 Image Dataset: This is a step up in complexity – 60,000 32×32 color images across 10 object classes (airplane, car, bird, etc.) ￼. CIFAR-10’s images are more complex than MNIST and would allow testing a deeper network or the effect of convolutional layers (if we choose to incorporate them). Using CIFAR-10 can show how our theoretical insights scale to a slightly more complex scenario. (50000 images would be used for training, 10000 for testing) ￼.
* Alternate options: If focusing on non-image data, we could consider the UCI “Wine Quality” dataset (where the task could be cast as classification of wine quality scores into categories) or the Iris dataset (a very small classic dataset for classifying flower species). However, Iris is too small for a deep learning approach; it might be used for a quick demonstration. For the main project, MNIST will likely be the primary classification dataset due to its popularity and ease of use, possibly with mention of a second for extension.

=== Regression Datasets: ===
* California Housing Dataset: A larger modern dataset (20,640 samples, 8 numerical features) derived from California census data ￼. Each sample corresponds to a district in California with features like median income, population, median house age, etc., and the target is median house value ￼ ￼. This could be used to demonstrate scalability – training a neural network on a larger regression task. The dataset is clean (no missing values, all continuous features) ￼. Using this, we can produce richer experiments, like training/validation curves with more data and perhaps spatial analysis of errors.
* Other possibilities: Energy efficiency dataset (predict heating/cooling load of buildings from structural features), Concrete Compressive Strength dataset (8 features, ~1000 samples), or Air Quality dataset. These are also structured regression tasks. We will likely stick with one primary regression task (Boston or California housing) to maintain focus, but mention others as potential extensions.
* Other datasets of your choice

For each chosen dataset, the proposal will detail why it’s suitable (e.g., MNIST is a standard for classification and will allow us to demonstrate our network achieving ~98% accuracy, which we can analyze in terms of errors and learning behavior; Boston Housing allows easy interpretation of regression outputs and checking if the network can capture relationships that linear regression might miss, etc.). We will also outline any preprocessing: for instance, scaling inputs to zero-mean, unit-variance for regression, one-hot encoding of labels for classification (though frameworks handle that via loss functions), and shuffling data, etc. Ensuring the data is in a good format will help the training go smoothly and make the mathematical assumptions (like features roughly normalized for better gradient descent behavior) valid.

===== Evaluation and Performance Metrics =====

To rigorously evaluate the models and compare their performance, we will employ a range of metrics appropriate to each task, along with validation strategies:
* Classification Metrics: The primary metric will be accuracy – the fraction of correctly classified examples on the test set. Accuracy is intuitive but can be misleading if classes are imbalanced. Therefore, we will also compute the precision, recall, and F1-score for the classification task. Precision is the proportion of predicted positives that are actually correct, while recall is the proportion of actual positives that were correctly identified ￼ ￼. The F1-score is the harmonic mean of precision and recall, providing a single measure of model balance ￼. Using these metrics gives a more nuanced view: for example, if our network predicts one class too often, precision for that class will drop. We will present a confusion matrix to show how the model confuses different classes (especially if we use CIFAR-10 or MNIST, seeing which digits or objects get mixed up can be insightful). If the dataset is imbalanced, we might report class-weighted or macro-averaged F1 scores to ensure each class is fairly considered. Additionally, for binary classification (if any), we can discuss ROC-AUC (Area Under the Receiver Operating Characteristic Curve) to evaluate the trade-off between true positive rate and false positive rate across thresholds. However, with something like MNIST (10 classes), we’ll likely stick to precision/recall per class and overall accuracy/F1. All these metrics will be computed on the held-out test set to gauge generalization.
* Regression Metrics: We will evaluate regression performance using Mean Squared Error (MSE) and Mean Absolute Error (MAE). MSE (average of squared errors) heavily penalizes large errors, and is the loss our network will likely be training to minimize. MAE (average of absolute errors) provides a more intuitive “average error magnitude” which is robust to outliers to a degree (less than MSE). We will also report the Coefficient of Determination ($R^2$), which indicates the proportion of variance in the target explained by the model ￼. An $R^2$ of 1 means perfect prediction, 0 means it’s no better than predicting the mean, and negative values indicate worse-than-mean performance ￼. $R^2$ is useful for interpretation – we’ll use it to discuss how much of the variability in (say) house prices our network can capture. If we use the Boston Housing dataset, we can compare our neural network’s $R^2$ to that of a simple linear regression as a baseline. We will be careful to use a separate test set for final metric reporting to avoid any bias.
* Training vs. Validation Curves: During training, we will monitor metrics on the training and validation sets each epoch. Plotting the loss curve (and accuracy curve for classification) over epochs will be a key part of analysis. This will show convergence behavior (did the loss plateau? Did it decrease smoothly or oscillate?). Moreover, by comparing training and validation loss, we can detect overfitting (training loss keeps dropping but validation loss starts rising). We will use this to justify certain regularization choices – e.g., if we see overfitting, we apply dropout or L2 and show that the gap between training and validation loss reduces.
* Cross-Validation (if feasible): For robustness, we might perform $k$-fold cross-validation on the regression dataset (since it’s smaller) to ensure our results are not an artifact of a particular train-test split. For the classification (large dataset), we rely on standard train/val/test splits but could mention any hyperparameter tuning was done on validation and test was truly held-out.

All performance results will be presented with tables or plots in the report for clarity. For example, a table might list accuracy, precision, recall, F1 for the classification model on test data, and another table might list MSE, MAE, $R^2$ for the regression model. We will also compare our models to simple baselines: e.g., for classification, a baseline could be random guessing (10% accuracy on MNIST) or a simple logistic regression; for regression, baseline could be predicting the mean value (for which $R^2=0$ by definition). This gives context to how much the neural network is improving over trivial solutions.


===== Introduction to numerical projects =====

Here follows a brief recipe and recommendation on how to write a report for each
project.

  * Give a short description of the nature of the problem and the eventual  numerical methods you have used.

  * Describe the algorithm you have used and/or developed. Here you may find it convenient to use pseudocoding. In many cases you can describe the algorithm in the program itself.

  * Include the source code of your program. Comment your program properly.

  * If possible, try to find analytic solutions, or known limits in order to test your program when developing the code.

  * Include your results either in figure form or in a table. Remember to        label your results. All tables and figures should have relevant captions        and labels on the axes.

  * Try to evaluate the reliabilty and numerical stability/precision of your results. If possible, include a qualitative and/or quantitative discussion of the numerical stability, eventual loss of precision etc.

  * Try to give an interpretation of you results in your answers to  the problems.

  * Critique: if possible include your comments and reflections about the  exercise, whether you felt you learnt something, ideas for improvements and  other thoughts you've made when solving the exercise. We wish to keep this course at the interactive level and your comments can help us improve it.

  * Try to establish a practice where you log your work at the  computerlab. You may find such a logbook very handy at later stages in your work, especially when you don't properly remember  what a previous test version  of your program did. Here you could also record  the time spent on solving the exercise, various algorithms you may have tested or other topics which you feel worthy of mentioning.






===== Format for electronic delivery of report and programs =====

The preferred format for the report is a PDF file. You can also use DOC or postscript formats or as an ipython notebook file.  As programming language we prefer that you choose between C/C++, Fortran2008 or Python. The following prescription should be followed when preparing the report:

  * Send us an email in order  to hand in your projects with a link to your GitHub/Gitlab repository.

  * In your GitHub/GitLab or similar repository, please include a folder which contains selected results. These can be in the form of output from your code for a selected set of runs and input parameters.


Finally, 
we encourage you to collaborate. Optimal working groups consist of 
2-3 students. You can then hand in a common report. 






