TITLE: Project 1
AUTHOR: "FYS5429/9429":"https://www.uio.no/studier/emner/matnat/fys/FYS5429/index-eng.html", Advanced machine learning and data analysis for the physical sciences, University of Oslo, Norway
DATE: Spring semester 2026, deadline March 20


======= Discriminative and Generative Deep Learning Models: A Mathematical and Computational Study =======


===== Project Overview =====

Deep learning methods can broadly be divided into _discriminative
models_, which learn decision boundaries for labeled data, and
_generative models_, which learn probability distributions over
data. Convolutional neural networks (CNNs) dominate modern
classification tasks, while generative models such as variational
autoencoders (VAEs), Boltzmann machines, and diffusion models provide
probabilistic descriptions of data and enable synthesis, uncertainty
quantification, and representation learning.

The goal of this project is to develop a unified mathematical and
computational understanding of these model classes. Students will
analyze classification and generative learning as optimization
problems over high-dimensional function spaces, emphasizing
probabilistic modeling, variational principles, and numerical
optimization.



===== Classification with Convolutional Neural Networks =====

A convolutional neural network defines a parametric mapping
!bt
\begin{equation}
f_\theta : \mathbb{R}^{H \times W \times C} \rightarrow \{1,\dots,K\},
\end{equation}
!et
where inputs are structured data (e.g.\ images) and outputs are class labels.

Mathematically, CNNs combine:
* Convolutional linear operators with local receptive fields,
* Nonlinear activation functions,
* Pooling and subsampling operations.

Here the aim is to focus on
* Convolutions as structured sparse linear maps,
* Translation equivariance and symmetry reduction,
* Parameter sharing and its effect on sample complexity.

The classification problem is formulated as empirical risk minimization with cross-entropy loss,
!bt
\begin{equation}
\mathcal{L}_{\text{clf}}(\theta) = -\frac{1}{N}\sum_{i=1}^N \log p_\theta(y_i \mid x_i),
\end{equation}
!et
and optimized using stochastic gradient descent.

===== Probabilistic Generative Modeling =====

Generative models aim to learn an approximation $p_\theta(x)$ to an unknown data distribution. This project considers three complementary paradigms.

=== Variational Autoencoders ===

VAEs introduce latent variables $z$ and define
!bt
\begin{equation}
p_\theta(x,z) = p_\theta(x \mid z)p(z),
\end{equation}
!et
with training based on variational inference.

Here you  will derive the evidence lower bound (ELBO),
!bt
\begin{equation}
\mathcal{L}_{\text{VAE}} = \mathbb{E}_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)] - \mathrm{KL}(q_\phi(z\mid x)\|p(z)),
\end{equation}
!et
and analyze:
* Encoder--decoder architectures,
* Reparameterization trick and differentiability,
* Trade-offs between reconstruction accuracy and latent regularization.

=== Boltzmann Machines ===

Boltzmann machines define energy-based models
!bt
\begin{equation}
p_\theta(x) = \frac{1}{Z_\theta} e^{-E_\theta(x)},
\end{equation}
!et
where $Z_\theta$ is the partition function.

Here we can  examine:
* Energy landscapes and statistical mechanics analogies,
* Maximum likelihood learning and gradient structure,
* Approximate inference methods such as contrastive divergence.

Connections between Boltzmann machines and variational principles are emphasized.

=== Implementation and Experiments ===

The practical component consists of implementing:
* A CNN for image classification,
* At least one generative model (VAE or Boltzmann machine, and if you wish to you could also include diffusion models).


Experiments will use standard labeled datasets (select the dataset of your choice):
* Classification accuracy and confusion structure,
* Quality of generated samples,
* Latent-space geometry and interpolation.

Computational results are interpreted through the lens of the mathematical models.

===== Expected Outcomes =====

By completing this project, you will:
* Understand deep learning through probabilistic and variational principles,
* Connect classification and generation within a unified framework,
* Analyze neural networks as numerical optimization problems,
* Gain insight applicable to physics-inspired machine learning, statistical inference, and scientific data analysis.


===== Introduction to numerical projects =====

Here follows a brief recipe and recommendation on how to write a report for each
project.

  * Give a short description of the nature of the problem and the eventual  numerical methods you have used.

  * Describe the algorithm you have used and/or developed. Here you may find it convenient to use pseudocoding. In many cases you can describe the algorithm in the program itself.

  * Include the source code of your program. Comment your program properly.

  * If possible, try to find analytic solutions, or known limits in order to test your program when developing the code.

  * Include your results either in figure form or in a table. Remember to        label your results. All tables and figures should have relevant captions        and labels on the axes.

  * Try to evaluate the reliabilty and numerical stability/precision of your results. If possible, include a qualitative and/or quantitative discussion of the numerical stability, eventual loss of precision etc.

  * Try to give an interpretation of you results in your answers to  the problems.

  * Critique: if possible include your comments and reflections about the  exercise, whether you felt you learnt something, ideas for improvements and  other thoughts you've made when solving the exercise. We wish to keep this course at the interactive level and your comments can help us improve it.

  * Try to establish a practice where you log your work at the  computerlab. You may find such a logbook very handy at later stages in your work, especially when you don't properly remember  what a previous test version  of your program did. Here you could also record  the time spent on solving the exercise, various algorithms you may have tested or other topics which you feel worthy of mentioning.






===== Format for electronic delivery of report and programs =====

The preferred format for the report is a PDF file. You can also use DOC or postscript formats or as an ipython notebook file.  As programming language we prefer that you choose between C/C++, Fortran2008 or Python. The following prescription should be followed when preparing the report:

  * Send us an email in order  to hand in your projects with a link to your GitHub/Gitlab repository.

  * In your GitHub/GitLab or similar repository, please include a folder which contains selected results. These can be in the form of output from your code for a selected set of runs and input parameters.


Finally, 
we encourage you to collaborate. Optimal working groups consist of 
2-3 students. You can then hand in a common report. 






