\documentclass{article}
\usepackage{hyperref}
\begin{document}
\title{Deep Learning Models for Temporal Symptom Dynamics in Ecological Momentary Assessment Data}
\author{}
\date{}
\maketitle

\section{Introduction and Background}
Ecological Momentary Assessment (EMA) is a method of collecting repeated self-report data on participants’ psychological states and behaviors in real time and naturalistic settings ￼ ￼.  Unlike traditional retrospective surveys, EMA captures experiences as they occur, greatly reducing recall bias and enhancing ecological validity ￼ ￼.  Modern EMA studies often involve mobile devices that prompt participants multiple times per day to report on mood, stress, symptoms, and context ￼ ￼.  This yields high-frequency time series of symptoms (e.g.\ mood, anxiety, fatigue) and related variables.  Such intensive longitudinal data can reveal fine-grained temporal dynamics and context-dependence of symptoms ￼ ￼.  For example, psychological states like loneliness or sadness can fluctuate many times within a single day ￼.  Modeling these dynamics is important for understanding and predicting mental health trajectories, but poses challenges due to irregular sampling, missing responses, and measurement noise.

The openESM (Open Experience Sampling Method) database is an open repository of harmonized EMA datasets that supports such research ￼ ￼.  As of late 2025, openESM contains on the order of 60 studies encompassing roughly 16,000 individuals and over 740,000 total observations ￼ ￼.  These datasets include a variety of symptom and affect measures (often on Likert scales) along with timestamps and sometimes passive sensor data ￼ ￼.  The rich metadata and API (in R/Python) allow researchers to filter for relevant constructs and download standardized data ￼ ￼.  In this project, we will leverage openESM data to build and evaluate deep learning models that capture temporal symptom dynamics in EMA.

\section{Research Objectives}
The primary objective of this project is to develop advanced neural models that learn from high-frequency EMA symptom data to characterize and predict individuals’ momentary symptom trajectories.  Specifically, we aim to (1) implement sequence models (e.g.\ RNNs, Transformers, temporal CNNs) tailored to EMA time series, (2) handle irregular sampling and noise inherent in real-world EMA, and (3) evaluate the models’ ability to accurately forecast symptoms and produce useful latent representations.  We will use PyTorch and TensorFlow/Keras for model implementation, with scikit-learn supporting preprocessing (e.g.\ scaling, imputation).  By benchmarking these models on openESM datasets, we seek to demonstrate the utility of deep learning approaches for analyzing intensive longitudinal mental health data, as compared to traditional statistical models.

\section{Research Questions and Hypotheses}
We formulate the following research questions:
	1.	\textbf{Predictive Accuracy:} Can deep sequence models predict future symptom levels from past EMA records more accurately than baseline approaches (e.g.\ autoregressive or mixed-effects models)? We hypothesize that recurrent and attention-based models will capture nonlinear temporal patterns and yield lower prediction error ￼ ￼.  For instance, Wall et al. (2024) found that RNNs predicted momentary suicidal ideation more accurately (lower RMSE, higher pseudo-$R^2$) than mixed-effects models in an EMA study ￼ ￼. Similarly, we expect our models to significantly outperform simple lag-based predictors in capturing daily symptom fluctuations.
	2.	\textbf{Symptom Dynamics:} What types of temporal dependencies and structures do EMA symptom trajectories exhibit? We will investigate whether models learn inertia (auto-correlation) or cross-lag relationships (e.g.\ how changes in one symptom affect another) between time steps.  We hypothesize that variables like mood and stress show predictable transitions on the order of hours or days (as real-time EMA data imply ￼). Deep models (e.g.\ LSTM, GRU) are well-suited to capture such sequential dependencies and handle irregular time gaps ￼ ￼.
	3.	\textbf{Irregular Sampling and Missingness:} EMA datasets often have non-uniform intervals (e.g.\ some prompts missed or delays) and missing responses. We will explore architectures that incorporate time-gap information explicitly. For example, GRU-D (an RNN variant) uses an exponential decay mechanism to account for elapsed time between observations ￼. We will test methods like time-aware RNNs, Neural ODEs (which model continuous-time dynamics ￼), and time-embedding in Transformers. We hypothesize that models incorporating timing and missingness will outperform naive interpolation or imputation strategies.
	4.	\textbf{Representation Quality:} Can unsupervised or semi-supervised learning on EMA data yield useful latent representations of symptom dynamics? We will experiment with autoencoder and variational models. We hypothesize that if the data contain recurring patterns or latent states (e.g.\ stable mood vs. crisis episodes), the learned representations will cluster meaningfully. We may use reconstruction error or clustering metrics to assess representation quality.

Overall, we expect that deep learning approaches (RNNs, Temporal CNNs, Transformers) will be effective for EMA time series, as suggested by recent work in other domains ￼ ￼.

\section{OpenESM Dataset and Data Considerations}
We will use the openESM repository as our data source ￼ ￼.  Relevant datasets include longitudinal studies with symptom or affect items collected multiple times per day. For example, the Fried (2022) dataset (openESM ID 0001) tracked 80 university students with 4 prompts per day over 14 days, measuring variables like “Worried,” “Nervous,” and “Anhedonia” on 5-point scales ￼ ￼. Another example is Jang (2024) which recorded panic-related symptoms (binary panic trigger, anxiety ratings) once per day along with passive heart-rate features ￼ ￼. Across datasets, variables include numeric ratings (e.g.\ anxiety level), binary indicators (e.g.\ exercise yes/no), and occasionally passive sensor measures (heart rate, steps) ￼ ￼.

Key data characteristics to consider are: (a) \emph{Temporal resolution}: EMA datasets vary from several times a day to daily sampling. Data may be equally spaced (fixed schedules) or irregular (random prompts) ￼ ￼. For instance, some studies deliver 4 fixed beeps per day ￼, while others use random or event-based prompts. (b) \emph{Variable types}: Most self-report items are ordinal rating scales; there may be multiple correlated symptom dimensions. (c) \emph{Missingness}: Nonresponse is common (e.g.\ participants skip prompts), leading to implicit missingness. The metadata (e.g.\ “Implicit Missingness: yes” on openESM) flags such cases ￼. (d) \emph{Noise}: Self-report responses can be noisy or biased; sensor data have measurement noise. We will likely need methods robust to outliers and uncertainty. (e) \emph{Inter-subject heterogeneity}: Participants differ in baseline symptoms and variability. Given these issues, models must account for irregular sampling (e.g.\ time gaps) and missing values, and possibly normalize across individuals.

\section{Proposed Methodology}
We will implement several advanced time-series learning architectures using PyTorch or TensorFlow/Keras, supported by scikit-learn for preprocessing. Candidate models include:
	•	\textbf{Recurrent Neural Networks (RNNs)}: LSTM or GRU networks that process sequences of symptom vectors. These naturally capture temporal dependencies and can handle variable-length input. As noted by Kang et al., gated RNNs can be adapted to missing patterns and time intervals in irregular series ￼. We will also try variants like GRU-D, which incorporates elapsed-time decay to handle uneven gaps ￼.
	•	\textbf{Transformers/Attention models}: Self-attention architectures (e.g.\ time-series Transformers) allow modeling long-range dependencies and ignore recency bias. Recent work has extended transformers to irregular time series by encoding time gaps or using continuous attention mechanisms ￼ ￼. We plan to include a Transformer model that takes timestamp embeddings or time-delta tokens.
	•	\textbf{Temporal Convolutional Networks (TCNs)}: 1D convolutional networks with dilations (causal convolutions) can capture sequence information and long memory effects ￼ ￼. We will experiment with TCN architectures as they often train faster and avoid RNN vanishing gradients. Temporal CNNs have been shown to achieve strong performance on forecasting tasks.
	•	\textbf{Continuous-time models / Neural ODEs}: Since EMA is naturally continuous, we will explore neural ODE or Neural CDE approaches, where a latent state evolves via differential equations between observations ￼ ￼. These models can generate trajectories at any time resolution and may better handle irregular sampling. A latent ODE model (with an RNN encoder) can provide continuous-time predictions.
	•	\textbf{Denoising Autoencoders / Representation learning}: We may include sequence-to-sequence autoencoders (e.g.\ an LSTM autoencoder) to learn compact embeddings of symptom time series. Training such models with added noise or dropout can enhance robustness.

To handle irregular intervals and missingness, we will compare strategies such as: (a) imputing missing values (e.g.\ forward-fill or interpolation) as a baseline; (b) appending time-gap features to each time step input (e.g.\ elapsed minutes since last observation); (c) using specialized layers like GRU-D that decay states between events ￼; (d) employing continuous-time latent dynamics (Neural ODE/CDE) ￼ ￼. Training will use standard loss functions (e.g.\ MSE for continuous symptoms, cross-entropy for categorical) with regularization (dropout, weight decay) to mitigate overfitting and noise sensitivity.  Hyperparameters will be tuned via cross-validation or a validation split per subject.

\section{Evaluation Strategies}
Model performance will be evaluated both quantitatively and qualitatively. For \emph{supervised prediction} tasks (e.g.\ forecasting next mood rating), we will measure predictive accuracy using metrics like RMSE, MAE for continuous outcomes, or accuracy/AUC for categorical outcomes. We will perform train/validation/test splits at the person or time-block level to assess generalization.  As an example, Wall et al. (2024) reported that RNN models achieved lower RMSE and higher pseudo-$R^2$ than mixed models when predicting daily suicidal ideation ￼ ￼; we will use similar metrics to compare our models against simpler baselines (e.g.\ persistence or linear mixed models). For \emph{unsupervised or generative} models (autoencoders, latent ODEs), we will compute reconstruction error on held-out data. We may also evaluate the quality of learned representations by downstream tasks, such as using latent embeddings to predict an independent outcome or clustering individuals. Representation quality might be judged via silhouette scores or how well clusters align with clinical subgroups (if labels available).

Model interpretability is also important. We may analyze attention weights or hidden state dynamics to identify which past symptoms most influence predictions. Additionally, we can examine consistency of model parameters across subjects (e.g.\ factor loadings) to check whether patterns are general or highly individual.

\section{Potential Extensions and Applications}
Beyond the core forecasting task, this work opens avenues for several extensions. First, the learned sequence models could be adapted for \emph{anomaly detection}: sudden deviations in symptom trajectories (e.g.\ onset of a panic attack) may be flagged when prediction error exceeds a threshold. Second, personalized mental health forecasting is a natural extension: models trained per individual could be used for just-in-time adaptive interventions (delivering support when predicted risk is high). Third, the latent representations from autoencoders or sequence embeddings could facilitate \emph{symptom clustering} across individuals, revealing subgroups with similar dynamics. For example, one might discover clusters of people whose symptoms fluctuate strongly versus those who remain relatively stable. Finally, the methods could integrate passive sensing modalities (if available in openESM, like actigraphy) for multimodal prediction.

In summary, this proposal aims to leverage openESM’s rich EMA datasets to build and evaluate state-of-the-art neural models for temporal symptom modeling. By addressing the challenges of irregular sampling and noise, and by rigorously evaluating predictions and representations, this project will contribute to methodological advances in EMA data analysis and to applications in mental health monitoring and forecasting.

\end{document}
