\documentclass[11pt,a4paper]{article}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{physics}
\usepackage{graphicx}

\geometry{margin=1in}

\title{\textbf{Reinforcement Learning, Generative Models, and PDEs:\\
A Mathematical Project in Control and Inference}}
\author{}
\date{}

\begin{document}
\maketitle

\section*{Project Overview}

Reinforcement learning (RL) and modern generative models are
increasingly understood through the lens of partial differential
equations (PDEs), stochastic processes, and variational
principles. Reinforcement learning is closely related to optimal
control and Hamilton--Jacobi--Bellman (HJB) equations, while
generative models such as diffusion models and score-based methods are
connected to Fokker--Planck equations, stochastic differential
equations (SDEs), and gradient flows in probability space.

The goal of this project is to develop a unified mathematical
understanding of reinforcement learning and generative learning as
PDE-driven optimization problems. Students will analyze value
functions, policies, and probability densities as solutions to PDEs,
and compare how control and inference emerge from related mathematical
structures.



\section{Reinforcement Learning and Optimal Control}

Reinforcement learning problems are commonly formulated as Markov decision processes, but in the continuous-state and continuous-time limit they are naturally described by stochastic control theory.

Consider a controlled stochastic differential equation
\begin{equation}
dX_t = f(X_t,u_t)\,dt + \sigma(X_t)\,dW_t,
\end{equation}
where $u_t$ is a control policy. The objective is to minimize the expected cost functional
\begin{equation}
J(u) = \mathbb{E}\left[ \int_0^T \ell(X_t,u_t)\,dt + g(X_T) \right].
\end{equation}

The associated value function
\begin{equation}
V(x,t) = \inf_u \mathbb{E}_{x,t} \left[ \int_t^T \ell(X_s,u_s)\,ds + g(X_T) \right]
\end{equation}
satisfies the Hamilton--Jacobi--Bellman (HJB) equation
\begin{equation}
\partial_t V + \min_u \left\{ \ell(x,u) + \nabla V \cdot f(x,u) \right\}
+ \frac{1}{2}\mathrm{Tr}\!\left(\sigma\sigma^T \nabla^2 V\right) = 0.
\end{equation}

\subsection*{Derivation Task 1}
Derive the HJB equation from the dynamic programming principle for the continuous-time control problem.

\section{Deep Reinforcement Learning as PDE Approximation}

In practical reinforcement learning, the value function $V(x)$ or action-value function $Q(x,u)$ is approximated by a neural network $V_\theta(x)$. Learning corresponds to minimizing a residual of the Bellman equation,
\begin{equation}
\mathcal{L}(\theta) = \mathbb{E}\left[ \left( \mathcal{T}V_\theta - V_\theta \right)^2 \right],
\end{equation}
where $\mathcal{T}$ denotes the Bellman operator.

From a PDE perspective:
\begin{itemize}
  \item Neural networks act as nonlinear trial spaces,
  \item Training corresponds to a Galerkin or collocation method,
  \item Instabilities arise from nonlinearity and bootstrapping.
\end{itemize}

\subsection*{Derivation Task 2}
Show that the Bellman operator is a contraction in the discounted case and explain why this property is generally lost under nonlinear function approximation.

\section{Generative Models and Forward--Backward PDEs}

Generative models aim to learn a probability density $\rho(x)$ rather than an optimal control. Many modern generative models are governed by diffusion processes
\begin{equation}
dX_t = b(X_t,t)\,dt + \sqrt{2\beta^{-1}}\,dW_t,
\end{equation}
whose probability density evolves according to the Fokker--Planck equation
\begin{equation}
\partial_t \rho = -\nabla \cdot (b\rho) + \beta^{-1}\Delta \rho.
\end{equation}

Diffusion models learn the \emph{reverse-time dynamics}, which can be written as
\begin{equation}
dX_t = \left[ b(X_t,t) - 2\beta^{-1}\nabla \log \rho_t(X_t) \right]dt + \sqrt{2\beta^{-1}}\,dW_t.
\end{equation}

\subsection*{Derivation Task 3}
Derive the reverse-time SDE associated with the Fokker--Planck equation and explain its connection to score matching.

\section{Variational and Entropic Perspectives}

Both reinforcement learning and generative modeling admit variational formulations.

In entropy-regularized RL, the objective becomes
\begin{equation}
J(\pi) = \mathbb{E}_\pi \left[ \sum_t r_t - \alpha \sum_t \log \pi(a_t|s_t) \right],
\end{equation}
leading to a modified HJB equation with a log-sum-exp structure.

Similarly, diffusion and score-based models can be interpreted as minimizing free-energy or Kullback--Leibler functionals over probability paths.

\subsection*{Derivation Task 4}
Show that entropy-regularized reinforcement learning leads to a soft HJB equation and compare it to the variational objective of diffusion models.

\section{Control vs Inference: A PDE Comparison}

A central comparison explored in this project is:
\begin{center}
\begin{tabular}{l l}
\textbf{Reinforcement Learning} & \textbf{Generative Learning} \\
\hline
Optimal control & Probabilistic inference \\
HJB equation & Fokker--Planck equation \\
Backward PDE & Forward--backward PDE \\
Policy optimization & Density evolution \\
\end{tabular}
\end{center}

Students will analyze how:
\begin{itemize}
  \item Policies correspond to optimal drift fields,
  \item Value functions resemble logarithmic transforms of densities,
  \item Control and sampling differ mathematically but share PDE structure.
\end{itemize}

\subsection*{Derivation Task 5}
Demonstrate the formal correspondence between a logarithmic transformation of the value function and a density-based formulation.

\section{Computational Experiments}

The computational component consists of:
\begin{itemize}
  \item Solving a low-dimensional HJB equation numerically,
  \item Implementing a reinforcement learning agent approximating the same solution,
  \item Training a diffusion or score-based model on a related stochastic system.
\end{itemize}

Results are compared in terms of convergence, stability, and approximation quality.

\section*{Expected Outcomes}

By completing this project, students will:
\begin{itemize}
  \item Understand reinforcement learning and generative models as PDE problems,
  \item Connect stochastic control, inference, and variational principles,
  \item Analyze neural networks as numerical solvers,
  \item Gain tools relevant to scientific machine learning, control, and physics-informed AI.
\end{itemize}

\end{document}

