TITLE: Advanced machine learning and data analysis for the physical sciences
AUTHOR: Morten Hjorth-Jensen {copyright, 1999-present|CC BY-NC} at Department of Physics and Center for Computing in Science Education, University of Oslo, Norway
DATE: April 24, 2025


!split
===== Plans for the week April 21-25, 2025  =====

!bblock  Deep generative models
o Variational Autoencoders (VAE), mathematics, basic mathematics
o Writing our own codes for VAEs
#o "Video of lecture":"https://youtu.be/rw-NBN293o4"
#o "Whiteboard notes":"https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/HandwrittenNotes/2024/NotesApril16.pdf"
!eblock

!split
===== Readings =====
!bblock 
o Add VAE material
!eblock

#todo: add about Langevin sampling, see https://www.lyndonduong.com/sgmcmc/
# code for VAEs applied to MNIST and CIFAR perhaps

!split
===== Theory of Variational Autoencoders =====


!split
===== Mathematics of Variational Autoencoders =====

We have defined earlier a probability (marginal) distribution with hidden variables $\bm{h}$ and parameters $\bm{\Theta}$ as
!bt
\[
p(\bm{x};\bm{\Theta}) = \int d\bm{h}p(\bm{x},\bm{h};\bm{\Theta}),
\]
!et
for continuous variables $\bm{h}$ and
!bt
\[
p(\bm{x};\bm{\Theta}) = \sum_{\bm{h}}p(\bm{x},\bm{h};\bm{\Theta}),
\]
!et
for discrete stochastic events $\bm{h}$. The variables $\bm{h}$ are normally called the _latent variables_ in the theory of autoencoders. We will also call then for that here.

!split
===== Using the conditional probability =====

Using the the definition of the conditional probabilities $p(\bm{x}\vert\bm{h};\bm{\Theta})$, $p(\bm{h}\vert\bm{x};\bm{\Theta})$ and 
and the prior $p(\bm{h})$, we can rewrite the above equation as
!bt
\[
p(\bm{x};\bm{\Theta}) = \sum_{\bm{h}}p(\bm{x}\vert\bm{h};\bm{\Theta})p(\bm{h},
\]
!et

which allows us to make the dependence of $\bm{x}$ on $\bm{h}$
explicit by using the law of total probability. The intuition behind
this approach for finding the marginal probability for $\bm{x}$ is to
optimize the above equations with respect to the parameters
$\bm{\Theta}$.  This is done normally by maximizing the probability,
the so-called maximum-likelihood approach discussed earlier.

!split
===== VAEs versus autoencoders =====

This trained probability is assumed to be able to produce similar
samples as the input.  In VAEs it is then common to compare via for
example the mean-squared error or the cross-entropy the predicted
values with the input values.  Compared with autoencoders, we are now
producing a probability instead of a functions which mimicks the
input.

In VAEs, the choice of this output distribution is often Gaussian,
meaning that the conditional probability is
!bt
\[
p(\bm{x}\vert\bm{h};\bm{\Theta})=N(\bm{x}\vert f(\bm{h};\bm{\Theta}), \sigma^2\times \bm{I}),
\]
!et
with mean value given by the function $f(\bm{h};\bm{\Theta})$ and a
diagonal covariance matrix multiplied by a parameter $\sigma^2$ which
is treated as a hyperparameter.

!split
===== Gradient descent =====

By having a Gaussian distribution, we can use gradient descent (or any
other optimization technique) to increase $p(\bm{x};\bm{\Theta})$ by
making $f(\bm{h};\bm{\Theta})$ approach $\bm{x}$ for some $\bm{h}$,
gradually making the training data more likely under the generative
model. The important property is simply that the marginal probability
can be computed, and it is continuous in $\bm{\Theta}$..

!split
===== Are VAEs just modified autoencoders? =====

The mathematical basis of VAEs actually has relatively little to do
with classical autoencoders, for example the sparse autoencoders or
denoising autoencoders discussed earlier.

VAEs approximately maximize the probability equation discussed
above. They are called autoencoders only because the final training
objective that derives from this setup does have an encoder and a
decoder, and resembles a traditional autoencoder. Unlike sparse
autoencoders, there are generally no tuning parameters analogous to
the sparsity penalties. And unlike sparse and denoising autoencoders,
we can sample directly from $p(\bm{x})$ without performing Markov
Chain Monte Carlo.


!split
===== Training VAEs =====

To solve the integral or sum for $p(\bm{x})$, there are two problems
that VAEs must deal with: how to define the latent variables $\bm{h}$,
that is decide what information they represent, and how to deal with
the integral over $\bm{h}$.  VAEs give a definite answer to both.



!split
===== Kullback-Leibler relative entropy (notation to be updated) =====

When the goal of the training is to approximate a probability
distribution, as it is in generative modeling, another relevant
measure is the _Kullback-Leibler divergence_, also known as the
relative entropy or Shannon entropy. It is a non-symmetric measure of the
dissimilarity between two probability density functions $p$ and
$q$. If $p$ is the unkown probability which we approximate with $q$,
we can measure the difference by
!bt
\begin{align*}
	\text{KL}(p||q) = \int_{-\infty}^{\infty} p (\bm{x}) \log \frac{p(\bm{x})}{q(\bm{x})}  d\bm{x}.
\end{align*}
!et

!split
===== Kullback-Leibler divergence and RBMs =====

Thus, the Kullback-Leibler divergence between the distribution of the
training data $f(\bm{x})$ and the model marginal distribution $p(\bm{x};\bm{\Theta})$ from an RBM is

!bt
\begin{align*}
	\text{KL} (f(\bm{x})|| p(\bm{x}| \bm{\Theta})) =& \int_{-\infty}^{\infty}
	f (\bm{x}) \log \frac{f(\bm{x})}{p(\bm{x}; \bm{\Theta})} d\bm{x} \\
	=& \int_{-\infty}^{\infty} f(\bm{x}) \log f(\bm{x}) d\bm{x} - \int_{-\infty}^{\infty} f(\bm{x}) \log
	p(\bm{x};\bm{\Theta}) d\bm{x} \\
	%=& \mathbb{E}_{f(\bm{x})} (\log f(\bm{x})) - \mathbb{E}_{f(\bm{x})} (\log p(\bm{x}; \bm{\Theta}))
	=& \langle \log f(\bm{x}) \rangle_{f(\bm{x})} - \langle \log p(\bm{x};\bm{\Theta}) \rangle_{f(\bm{x})} \\
	=& \langle \log f(\bm{x}) \rangle_{data} + \langle E(\bm{x}) \rangle_{data} + \log Z.
\end{align*}
!et

!split
===== Maximizing log-likelihood =====

The first term is constant with respect to $\bm{\Theta}$ since
$f(\bm{x})$ is independent of $\bm{\Theta}$. Thus the Kullback-Leibler
divergence is minimal when the second term is minimal. The second term
is the log-likelihood cost function, hence minimizing the
Kullback-Leibler divergence is equivalent to maximizing the
log-likelihood. 


!split
===== Back to VAEs =====

We want to train the marginal probability with some latent variables $\bm{h}$
!bt
\[
p(\bm{x};\bm{\Theta}) = \int d\bm{h}p(\bm{x},\bm{h};\bm{\Theta}),
\]
!et
for the continuous version (see previous slides for the discrete variant).


!split
===== Using the KL divergence =====

In practice, for most $\bm{h}$, $p(\bm{x}\vert \bm{h}; \bm{\Theta})$
will be nearly zero, and hence contribute almost nothing to our
estimate of $p(\bm{x})$.

The key idea behind the variational autoencoder is to attempt to
sample values of $\bm{h}$ that are likely to have produced $\bm{x}$,
and compute $p(\bm{x})$ just from those.

This means that we need a new function $Q(\bm{h}|\bm{x})$ which can
take a value of $\bm{x}$ and give us a distribution over $\bm{h}$
values that are likely to produce $\bm{x}$.  Hopefully the space of
$\bm{h}$ values that are likely under $Q$ will be much smaller than
the space of all $\bm{h}$'s that are likely under the prior
$p(\bm{h})$.  This lets us, for example, compute $E_{\bm{h}\sim
Q}p(\bm{x}\vert \bm{h})$ relatively easily. Note that we drop
$\bm{\Theta}$ from here and for notational simplicity.


!split
===== Kullback-Leibler again =====

However, if $\bm{h}$ is sampled from an arbitrary distribution with
PDF $Q(\bm{h})$, which is not $\mathcal{N}(0,I)$, then how does that
help us optimize $p(\bm{x})$?

The first thing we need to do is relate
$E_{\bm{h}\sim Q}P(\bm{x}\vert \bm{h})$ and $p(\bm{x})$.  We will see where $Q$ comes from later.

The relationship between $E_{\bm{h}\sim Q}p(\bm{x}\vert \bm{h})$ and $p(\bm{x})$ is one of the cornerstones of variational Bayesian methods.
We begin with the definition of Kullback-Leibler divergence (KL divergence or $\mathcal{D}$) between $p(\bm{h}\vert \bm{x})$ and $Q(\bm{h})$, for some arbitrary $Q$ (which may or may not depend on $\bm{x}$):
!bt
\[
    \mathcal{D}\left[Q(\bm{h})\|p(\bm{h}|\bm{x})\right]=E_{\bm{h}\sim Q}\left[\log Q(\bm{h}) - \log p(\bm{h}|\bm{x}) \right].
\]
!et

!split
===== And applying Bayes rule =====

We can get both $p(\bm{x})$ and $p(\bm{x}\vert \bm{h})$ into this equation by applying Bayes rule to $p(\bm{h}|\bm{x})$
!bt
\[
    \mathcal{D}\left[Q(\bm{h})\|p(\bm{h}\vert \bm{x})\right]=E_{\bm{h}\sim Q}\left[\log Q(\bm{h}) - \log p(\bm{x}|\bm{h}) - \log p(\bm{h}) \right] + \log p(\bm{x}).
\]
!et

Here, $\log p(\bm{x})$ comes out of the expectation because it does not depend on $\bm{h}$.
Negating both sides, rearranging, and contracting part of $E_{\bm{h}\sim Q}$ into a KL-divergence terms yields:
!bt
\[
\log p(\bm{x}) - \mathcal{D}\left[Q(\bm{h})\|p(\bm{h}\vert \bm{x})\right]=E_{\bm{h}\sim Q}\left[\log p(\bm{x}\vert\bm{h})  \right] - \mathcal{D}\left[Q(\bm{h})\|P(\bm{h})\right].
\]
!et

!split
===== Rearranging =====

Using Bayes rule we obtain
!bt
\[
E_{\bm{h}\sim Q}\left[\log p(y_i|\bm{h},x_i)\right]=E_{\bm{h}\sim Q}\left[\log p(\bm{h}|y_i,x_i) - \log p(\bm{h}|x_i) + \log p(y_i|x_i) \right]
\]
!et

Rearranging the terms and subtracting $E_{\bm{h}\sim Q}\log Q(\bm{h})$ from both sides gives
!bt
\[
\begin{array}{c}
\log P(y_i|x_i) - E_{\bm{h}\sim Q}\left[\log Q(\bm{h})-\log p(\bm{h}|x_i,y_i)\right]=\hspace{10em}\\
\hspace{10em}E_{\bm{h}\sim Q}\left[\log p(y_i|\bm{h},x_i)+\log p(\bm{h}|x_i)-\log Q(\bm{h})\right]
\end{array}
\]
!et
Note that $\bm{x}$ is fixed, and $Q$ can be \textit{any} distribution, not
just a distribution which does a good job mapping $\bm{x}$ to the $\bm{h}$'s
that can produce $X$.


!split
===== Inferring the probability =====

Since we are interested in inferring $p(\bm{x})$, it makes sense to
construct a $Q$ which \textit{does} depend on $\bm{x}$, and in particular,
one which makes $\mathcal{D}\left[Q(\bm{h})\|p(\bm{h}|\bm{x})\right]$ small
!bt
\[
\log p(\bm{x}) - \mathcal{D}\left[Q(\bm{h}|\bm{x})\|p(\bm{h}|\bm{x})\right]=E_{\bm{h}\sim Q}\left[\log p(\bm{x}|\bm{h})  \right] - \mathcal{D}\left[Q(\bm{h}|\bm{x})\|p(\bm{h})\right].
\]
!et

Hence, during training, it makes sense to choose a $Q$ which will make
$E_{\bm{h}\sim Q}[\log Q(\bm{h})-$ $\log p(\bm{h}|x_i,y_i)]$ (a
$\mathcal{D}$-divergence) small, such that the right hand side is a
close approximation to $\log p(y_i|y_i)$.

!split
===== Central equation of VAEs =====

This equation serves as the core of the variational autoencoder, and
it is worth spending some time thinking about what it means.

o The left hand side has the quantity we want to maximize, namely $\log p(\bm{x})$ plus an error term.
o The right hand side is something we can optimize via stochastic gradient descent given the right choice of $Q$.


!split
===== Setting up SGD =====
So how can we perform stochastic gradient descent?

First we need to be a bit more specific about the form that $Q(\bm{h}|\bm{x})$
will take.  The usual choice is to say that
$Q(\bm{h}|\bm{x})=\mathcal{N}(\bm{h}|\mu(\bm{x};\vartheta),\Sigma(;\vartheta))$, where
$\mu$ and $\Sigma$ are arbitrary deterministic functions with
parameters $\vartheta$ that can be learned from data (we will omit
$\vartheta$ in later equations).  In practice, $\mu$ and $\Sigma$ are
again implemented via neural networks, and $\Sigma$ is constrained to
be a diagonal matrix.


!split
===== More on the SGD =====

The name variational ``autoencoder'' comes from
the fact that $\mu$ and $\Sigma$ are ``encoding'' $\bm{x}$ into the latent
space $\bm{h}$.  The advantages of this choice are computational, as they
make it clear how to compute the right hand side.  The last
term---$\mathcal{D}\left[Q(\bm{h}|\bm{x})\|p(\bm{h})\right]$---is now a KL-divergence
between two multivariate Gaussian distributions, which can be computed
in closed form as:
!bt
\[
\begin{array}{c}
 \mathcal{D}[\mathcal{N}(\mu_0,\Sigma_0) \| \mathcal{N}(\mu_1,\Sigma_1)] = \hspace{20em}\\
  \hspace{5em}\frac{ 1 }{ 2 } \left( \mathrm{tr} \left( \Sigma_1^{-1} \Sigma_0 \right) + \left( \mu_1 - \mu_0\right)^\top \Sigma_1^{-1} ( \mu_1 - \mu_0 ) - k + \log \left( \frac{ \det \Sigma_1 }{ \det \Sigma_0  } \right)  \right)
\end{array}
\]
!et
where $k$ is the dimensionality of the distribution.

!split
===== Simplification =====
In our case, this simplifies to:
!bt
\[
\begin{array}{c}
 \mathcal{D}[\mathcal{N}(\mu(X),\Sigma(X)) \| \mathcal{N}(0,I)] = \hspace{20em}\\
\hspace{6em}\frac{ 1 }{ 2 } \left( \mathrm{tr} \left( \Sigma(X) \right) + \left( \mu(X)\right)^\top ( \mu(X) ) - k - \log\det\left(  \Sigma(X)  \right)  \right).
\end{array}
\]
!et


!split
===== Terms to compute =====

The first term on the right hand side is a bit more tricky.
We could use sampling to estimate $E_{z\sim Q}\left[\log P(X|z)  \right]$, but getting a good estimate would require passing many samples of $z$ through $f$, which would be expensive.
Hence, as is standard in stochastic gradient descent, we take one sample of $z$ and treat $\log P(X|z)$ for that $z$ as an approximation of $E_{z\sim Q}\left[\log P(X|z)  \right]$.
After all, we are already doing stochastic gradient descent over different values of $X$ sampled from a dataset $D$.
The full equation we want to optimize is:

!bt
\[
\begin{array}{c}
    E_{X\sim D}\left[\log P(X) - \mathcal{D}\left[Q(z|X)\|P(z|X)\right]\right]=\hspace{16em}\\
\hspace{10em}E_{X\sim D}\left[E_{z\sim Q}\left[\log P(X|z)  \right] - \mathcal{D}\left[Q(z|X)\|P(z)\right]\right].
\end{array}
\]
!et


!split
===== Computing the gradients =====

If we take the gradient of this equation, the gradient symbol can be moved into the expectations.
Therefore, we can sample a single value of $X$ and a single value of $z$ from the distribution $Q(z|X)$, and compute the gradient of:
!bt
\begin{equation}
 \log P(X|z)-\mathcal{D}\left[Q(z|X)\|P(z)\right].
\end{equation}
!et

We can then average the gradient of this function over arbitrarily many samples of $X$ and $z$, and the result converges to the gradient.

There is, however, a significant problem
$E_{z\sim Q}\left[\log P(X|z)  \right]$ depends not just on the parameters of $P$, but also on the parameters of $Q$.

In order to make VAEs work, it is essential to drive $Q$ to produce codes for $X$ that $P$ can reliably decode.  
!bt
\[
 E_{X\sim D}\left[E_{\epsilon\sim\mathcal{N}(0,I)}[\log P(X|z=\mu(X)+\Sigma^{1/2}(X)*\epsilon)]-\mathcal{D}\left[Q(z|X)\|P(z)\right]\right].
\]
!et



