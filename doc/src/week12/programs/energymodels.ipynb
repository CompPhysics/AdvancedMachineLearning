{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 8: Deep Energy-Based Generative Models\n",
    "\n",
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)\n",
    "\n",
    "**Filled notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial8/Deep_Energy_Models.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial8/Deep_Energy_Models.ipynb)  \n",
    "**Pre-trained models:** \n",
    "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/tutorial8)\n",
    "[![GoogleDrive](https://img.shields.io/static/v1.svg?logo=google-drive&logoColor=yellow&label=GDrive&message=Download&color=yellow)](https://drive.google.com/drive/folders/11ZI7x2sfCNtaZUNpe4v08YXWN870spXs?usp=sharing)  \n",
    "**Recordings:** \n",
    "[![YouTube - Part 1](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%201&color=red)](https://youtu.be/E6PDwquBBQc)\n",
    "[![YouTube - Part 2](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%202&color=red)](https://youtu.be/QJ94zuSQoP4)    \n",
    "**Author:** Phillip Lippe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will look at energy-based deep learning models, and focus on their application as generative models. Energy models have been a popular tool before the huge deep learning hype around 2012 hit. However, in recent years, energy-based models have gained increasing attention because of improved training methods and tricks being proposed. Although they are still in a research stage, they have shown to outperform strong Generative Adversarial Networks (Lecture/Tutorial 10) in certain cases which have been the state of the art of generating images ([blog post](https://ajolicoeur.wordpress.com/the-new-contender-to-gans-score-matching-with-langevin-sampling/) about strong energy-based models, [blog post](https://medium.com/syncedreview/nvidia-open-sources-hyper-realistic-face-generator-stylegan-f346e1a73826) about the power of GANs). Hence, it is important to be aware of energy-based models, and as the theory can be abstract sometimes, we will show the idea of energy-based models with a lot of examples.\n",
    "\n",
    "First, let's import our standard libraries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/td/3yk470mj5p931p9dtkk0y6jw0000gn/T/ipykernel_35000/1193259737.py:13: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n",
      "/Users/mhjensen/miniforge3/envs/myenv/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/mhjensen/miniforge3/envs/myenv/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <2D1B8D5C-7891-3680-9CF9-F771AE880676> /Users/mhjensen/miniforge3/envs/myenv/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in:     <ADC0A61A-5B83-3A02-975F-EE5DFF441305> /Users/mhjensen/miniforge3/envs/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial8\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have pre-trained models that we download below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial8/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"MNIST.ckpt\", \"tensorboards/events.out.tfevents.MNIST\"]\n",
    "\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if \"/\" in file_name:\n",
    "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Models\n",
    "\n",
    "In the first part of this tutorial, we will review the theory of the energy-based models (the same theory has been discussed in Lecture 8). While most of the previous models had the goal of classification or regression, energy-based models are motivated from a different perspective: density estimation. Given a dataset with a lot of elements, we want to estimate the probability distribution over the whole data space. As an example, if we model images from CIFAR10, our goal would be to have a probability distribution over all possible images of size $32\\times32\\times3$ where those images have a high likelihood that look realistic and are one of the 10 CIFAR classes. Simple methods like interpolation between images don't work because images are extremely high-dimensional (especially for large HD images). Hence, we turn to deep learning methods that have performed well on complex data.\n",
    "\n",
    "However, how do we predict a probability distribution $p(\\mathbf{x})$ over so many dimensions using a simple neural network? The problem is that we cannot just predict a score between 0 and 1, because a probability distribution over data needs to fulfill two properties:\n",
    "\n",
    "1. The probability distribution needs to assign any possible value of $\\mathbf{x}$ a non-negative value: $p(\\mathbf{x}) \\geq 0$.\n",
    "2. The probability density must sum/integrate to 1 over **all** possible inputs: $\\int_{\\mathbf{x}} p(\\mathbf{x}) d\\mathbf{x} = 1$. \n",
    "\n",
    "Luckily, there are actually many approaches for this, and one of them are energy-based models. The fundamental idea of energy-based models is that you can turn any function that predicts values larger than zero into a probability distribution by dividing by its volume. Imagine we have a neural network, which has as output a single neuron, like in regression. We can call this network $E_{\\theta}(\\mathbf{x})$, where $\\theta$ are our parameters of the network, and $\\mathbf{x}$ the input data (e.g. an image). The output of $E_{\\theta}$ is a scalar value between $-\\infty$ and $\\infty$. Now, we can use basic probability theory to *normalize* the scores of all possible inputs:\n",
    "\n",
    "$$\n",
    "q_{\\theta}(\\mathbf{x}) = \\frac{\\exp\\left(-E_{\\theta}(\\mathbf{x})\\right)}{Z_{\\theta}} \\hspace{5mm}\\text{where}\\hspace{5mm}\n",
    "Z_{\\theta} = \\begin{cases}\n",
    "    \\int_{\\mathbf{x}}\\exp\\left(-E_{\\theta}(\\mathbf{x})\\right) d\\mathbf{x} & \\text{if }x\\text{ is continuous}\\\\\n",
    "    \\sum_{\\mathbf{x}}\\exp\\left(-E_{\\theta}(\\mathbf{x})\\right) & \\text{if }x\\text{ is discrete}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The $\\exp$-function ensures that we assign a probability greater than zero to any possible input. We use a negative sign in front of $E$ because we call $E_{\\theta}$ to be the energy function: data points with high likelihood have a low energy, while data points with low likelihood have a high energy. $Z_{\\theta}$ is our normalization terms that ensures that the density integrates/sums to 1. We can show this by integrating over $q_{\\theta}(\\mathbf{x})$:\n",
    "\n",
    "$$\n",
    "\\int_{\\mathbf{x}}q_{\\theta}(\\mathbf{x})d\\mathbf{x} = \n",
    "\\int_{\\mathbf{x}}\\frac{\\exp\\left(-E_{\\theta}(\\mathbf{x})\\right)}{\\int_{\\mathbf{\\tilde{x}}}\\exp\\left(-E_{\\theta}(\\mathbf{\\tilde{x}})\\right) d\\mathbf{\\tilde{x}}}d\\mathbf{x} =\n",
    "\\frac{\\int_{\\mathbf{x}}\\exp\\left(-E_{\\theta}(\\mathbf{x})\\right)d\\mathbf{x}}{\\int_{\\mathbf{\\tilde{x}}}\\exp\\left(-E_{\\theta}(\\mathbf{\\tilde{x}})\\right) d\\mathbf{\\tilde{x}}} = 1\n",
    "$$\n",
    "\n",
    "Note that we call the probability distribution $q_{\\theta}(\\mathbf{x})$ because this is the learned distribution by the model, and is trained to be as close as possible to the *true*, unknown distribution $p(\\mathbf{x})$. \n",
    "\n",
    "The main benefit of this formulation of the probability distribution is its great flexibility as we can choose $E_{\\theta}$ in whatever way we like, without any constraints. Nevertheless, when looking at the equation above, we can see a fundamental issue: How do we calculate $Z_{\\theta}$? There is no chance that we can calculate $Z_{\\theta}$ analytically for high-dimensional input and/or larger neural networks, but the task requires us to know $Z_{\\theta}$. Although we can't determine the exact likelihood of a point, there exist methods with which we can train energy-based models. Thus, we will look next at \"Contrastive Divergence\" for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Divergence\n",
    "\n",
    "When we train a model on generative modeling, it is usually done by maximum likelihood estimation. In other words, we try to maximize the likelihood of the examples in the training set. As the exact likelihood of a point cannot be determined due to the unknown normalization constant $Z_{\\theta}$, we need to train energy-based models slightly different. We cannot just maximize the un-normalized probability $\\exp(-E_{\\theta}(\\mathbf{x}_{\\text{train}}))$ because there is no guarantee that $Z_{\\theta}$ stays constant, or that $\\mathbf{x}_{\\text{train}}$ is becoming more likely than the others. However, if we base our training on comparing the likelihood of points, we can create a stable objective. Namely, we can re-write our maximum likelihood objective where we maximize the probability of $\\mathbf{x}_{\\text{train}}$ compared to a randomly sampled data point of our model:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\nabla_{\\theta}\\mathcal{L}_{\\text{MLE}}(\\mathbf{\\theta};p) & = -\\mathbb{E}_{p(\\mathbf{x})}\\left[\\nabla_{\\theta}\\log q_{\\theta}(\\mathbf{x})\\right]\\\\[5pt]\n",
    "    & = \\mathbb{E}_{p(\\mathbf{x})}\\left[\\nabla_{\\theta}E_{\\theta}(\\mathbf{x})\\right] - \\mathbb{E}_{q_{\\theta}(\\mathbf{x})}\\left[\\nabla_{\\theta}E_{\\theta}(\\mathbf{x})\\right]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Note that the loss is still an objective we want to minimize. Thus, we try to minimize the energy for data points from the dataset, while maximizing the energy for randomly sampled data points from our model (how we sample will be explained below). Although this objective sounds intuitive, how is it actually derived from our original distribution $q_{\\theta}(\\mathbf{x})$? The trick is that we approximate $Z_{\\theta}$ by a single Monte-Carlo sample. This gives us the exact same objective as written above.\n",
    "\n",
    "Visually, we can look at the objective as follows (figure credit - [Stefano Ermon and Aditya Grover](https://deepgenerativemodels.github.io/assets/slides/cs236_lecture11.pdf)):\n",
    "\n",
    "<center width=\"100%\"><img src=\"contrastive_divergence.svg\" width=\"700px\"></center>\n",
    "\n",
    "$f_{\\theta}$ represents $\\exp(-E_{\\theta}(\\mathbf{x}))$ in our case. The point on the right, called \"correct answer\", represents a data point from the dataset (i.e. $x_{\\text{train}}$), and the left point, \"wrong answer\", a sample from our model (i.e. $x_{\\text{sample}}$). Thus, we try to \"pull up\" the probability of the data points in the dataset, while \"pushing down\" randomly sampled points. The two forces for pulling and pushing are in balance iff $q_{\\theta}(\\mathbf{x})=p(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from Energy-Based Models\n",
    "\n",
    "For sampling from an energy-based model, we can apply a Markov Chain Monte Carlo using Langevin Dynamics. The idea of the algorithm is to start from a random point, and slowly move towards the direction of higher probability using the gradients of $E_{\\theta}$. Nevertheless, this is not enough to fully capture the probability distribution. We need to add noise $\\omega$ at each gradient step to the current sample. Under certain conditions such as that we perform the gradient steps an infinite amount of times, we would be able to create an exact sample from our modeled distribution. However, as this is not practically possible, we usually limit the chain to $K$ steps ($K$ a hyperparameter that needs to be finetuned). Overall, the sampling procedure can be summarized in the following algorithm:\n",
    "\n",
    "<center width=\"100%\" style=\"padding:15px\"><img src=\"sampling.svg\" width=\"750px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of Energy-based models beyond generation\n",
    "\n",
    "Modeling the probability distribution for sampling new data is not the only application of energy-based models. Any application which requires us to compare two elements is much simpler to learn because we just need to go for the higher energy. A couple of examples are shown below (figure credit - [Stefano Ermon and Aditya Grover](https://deepgenerativemodels.github.io/assets/slides/cs236_lecture11.pdf)). A classification setup like object recognition or sequence labeling can be considered as an energy-based task as we just need to find the $Y$ input that minimizes the output $E(X, Y)$ (hence maximizes probability). Similarly, a popular application of energy-based models is denoising of images. Given an image $X$ with a lot of noise, we try to minimize the energy by finding the true input image $Y$. \n",
    "\n",
    "<center width=\"100%\"><img src=\"energy_models_application.svg\" width=\"600px\"></center>\n",
    "\n",
    "Nonetheless, we will focus on generative modeling here as in the next couple of lectures, we will discuss more generative deep learning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image generation\n",
    "\n",
    "As an example for energy-based models, we will train a model on image generation. Specifically, we will look at how we can generate MNIST digits with a very simple CNN model. However, it should be noted that energy models are not easy to train and often diverge if the hyperparameters are not well tuned. We will rely on training tricks proposed in the paper [Implicit Generation and Generalization in Energy-Based Models](https://arxiv.org/abs/1903.08689) by Yilun Du and Igor Mordatch ([blog](https://openai.com/blog/energy-based-models/)). The important part of this notebook is however to see how the theory above can actually be used in a model.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "First, we can load the MNIST dataset below. Note that we need to normalize the images between -1 and 1 instead of mean 0 and std 1 because during sampling, we have to limit the input space. Scaling between -1 and 1 makes it easier to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model\n",
    "\n",
    "First, we implement our CNN model. The MNIST images are of size 28x28, hence we only need a small model. As an example, we will apply several convolutions with stride 2 that downscale the images. If you are interested, you can also use a deeper model such as a small ResNet, but for simplicity, we will stick with the tiny network.\n",
    "\n",
    "It is a good practice to use a smooth activation function like Swish instead of ReLU in the energy model. This is because we will rely on the gradients we get back with respect to the input image, which should not be sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_features=32, out_dim=1, **kwargs):\n",
    "        super().__init__()\n",
    "        # We increase the hidden dimension over layers. Here pre-calculated for simplicity.\n",
    "        c_hid1 = hidden_features//2\n",
    "        c_hid2 = hidden_features\n",
    "        c_hid3 = hidden_features*2\n",
    "        \n",
    "        # Series of convolutions and Swish activation functions\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "                nn.Conv2d(1, c_hid1, kernel_size=5, stride=2, padding=4), # [16x16] - Larger padding to get 32x32 image\n",
    "                Swish(),\n",
    "                nn.Conv2d(c_hid1, c_hid2, kernel_size=3, stride=2, padding=1), #  [8x8]\n",
    "                Swish(),\n",
    "                nn.Conv2d(c_hid2, c_hid3, kernel_size=3, stride=2, padding=1), # [4x4]\n",
    "                Swish(),\n",
    "                nn.Conv2d(c_hid3, c_hid3, kernel_size=3, stride=2, padding=1), # [2x2]\n",
    "                Swish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(c_hid3*4, c_hid3),\n",
    "                Swish(),\n",
    "                nn.Linear(c_hid3, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x).squeeze(dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rest of the notebook, the output of the model will actually not represent $E_{\\theta}(\\mathbf{x})$, but $-E_{\\theta}(\\mathbf{x})$. This is a standard implementation practice for energy-based models, as some people also write the energy probability density as $q_{\\theta}(\\mathbf{x}) = \\frac{\\exp\\left(f_{\\theta}(\\mathbf{x})\\right)}{Z_{\\theta}}$. In that case, the model would actually represent $f_{\\theta}(\\mathbf{x})$. In the training loss etc., we need to be careful to not switch up the signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling buffer\n",
    "\n",
    "In the next part, we look at the training with sampled elements. To use the contrastive divergence objective, we need to generate samples during training. Previous work has shown that due to the high dimensionality of images, we need a lot of iterations inside the MCMC sampling to obtain reasonable samples. However, there is a training trick that significantly reduces the sampling cost: using a sampling buffer. The idea is that we store the samples of the last couple of batches in a buffer, and re-use those as the starting point of the MCMC algorithm for the next batches. This reduces the sampling cost because the model requires a significantly lower number of steps to converge to reasonable samples. However, to not solely rely on previous samples and allow novel samples as well, we re-initialize 5% of our samples from scratch (random noise between -1 and 1). \n",
    "\n",
    "Below, we implement the sampling buffer. The function `sample_new_exmps` returns a new batch of \"fake\" images. We refer to those as fake images because they have been generated, but are not actually part of the dataset. As mentioned before, we use initialize 5% randomly, and 95% are randomly picked from our buffer. On this initial batch, we perform MCMC for 60 iterations to improve the image quality and come closer to samples from $q_{\\theta}(\\mathbf{x})$. In the function `generate_samples`, we implemented the MCMC for images. Note that the hyperparameters of `step_size`, `steps`, the noise standard deviation $\\sigma$ are specifically set for MNIST, and need to be finetuned for a different dataset if you want to use such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "\n",
    "    def __init__(self, model, img_shape, sample_size, max_len=8192):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model - Neural network to use for modeling E_theta\n",
    "            img_shape - Shape of the images to model\n",
    "            sample_size - Batch size of the samples\n",
    "            max_len - Maximum number of data points to keep in the buffer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.img_shape = img_shape\n",
    "        self.sample_size = sample_size\n",
    "        self.max_len = max_len\n",
    "        self.examples = [(torch.rand((1,)+img_shape)*2-1) for _ in range(self.sample_size)]\n",
    "\n",
    "    def sample_new_exmps(self, steps=60, step_size=10):\n",
    "        \"\"\"\n",
    "        Function for getting a new batch of \"fake\" images.\n",
    "        Inputs:\n",
    "            steps - Number of iterations in the MCMC algorithm\n",
    "            step_size - Learning rate nu in the algorithm above\n",
    "        \"\"\"\n",
    "        # Choose 95% of the batch from the buffer, 5% generate from scratch\n",
    "        n_new = np.random.binomial(self.sample_size, 0.05)\n",
    "        rand_imgs = torch.rand((n_new,) + self.img_shape) * 2 - 1\n",
    "        old_imgs = torch.cat(random.choices(self.examples, k=self.sample_size-n_new), dim=0)\n",
    "        inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0).detach().to(device)\n",
    "\n",
    "        # Perform MCMC sampling\n",
    "        inp_imgs = Sampler.generate_samples(self.model, inp_imgs, steps=steps, step_size=step_size)\n",
    "\n",
    "        # Add new images to the buffer and remove old ones if needed\n",
    "        self.examples = list(inp_imgs.to(torch.device(\"cpu\")).chunk(self.sample_size, dim=0)) + self.examples\n",
    "        self.examples = self.examples[:self.max_len]\n",
    "        return inp_imgs\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_samples(model, inp_imgs, steps=60, step_size=10, return_img_per_step=False):\n",
    "        \"\"\"\n",
    "        Function for sampling images for a given model. \n",
    "        Inputs:\n",
    "            model - Neural network to use for modeling E_theta\n",
    "            inp_imgs - Images to start from for sampling. If you want to generate new images, enter noise between -1 and 1.\n",
    "            steps - Number of iterations in the MCMC algorithm.\n",
    "            step_size - Learning rate nu in the algorithm above\n",
    "            return_img_per_step - If True, we return the sample at every iteration of the MCMC\n",
    "        \"\"\"\n",
    "        # Before MCMC: set model parameters to \"required_grad=False\"\n",
    "        # because we are only interested in the gradients of the input. \n",
    "        is_training = model.training\n",
    "        model.eval()\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "        inp_imgs.requires_grad = True\n",
    "        \n",
    "        # Enable gradient calculation if not already the case\n",
    "        had_gradients_enabled = torch.is_grad_enabled()\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "        # We use a buffer tensor in which we generate noise each loop iteration.\n",
    "        # More efficient than creating a new tensor every iteration.\n",
    "        noise = torch.randn(inp_imgs.shape, device=inp_imgs.device)\n",
    "        \n",
    "        # List for storing generations at each step (for later analysis)\n",
    "        imgs_per_step = []\n",
    "        \n",
    "        # Loop over K (steps)\n",
    "        for _ in range(steps):\n",
    "            # Part 1: Add noise to the input.\n",
    "            noise.normal_(0, 0.005)\n",
    "            inp_imgs.data.add_(noise.data)\n",
    "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "            \n",
    "            # Part 2: calculate gradients for the current input.\n",
    "            out_imgs = -model(inp_imgs)\n",
    "            out_imgs.sum().backward()\n",
    "            inp_imgs.grad.data.clamp_(-0.03, 0.03) # For stabilizing and preventing too high gradients\n",
    "\n",
    "            # Apply gradients to our current samples\n",
    "            inp_imgs.data.add_(-step_size * inp_imgs.grad.data)\n",
    "            inp_imgs.grad.detach_()\n",
    "            inp_imgs.grad.zero_()\n",
    "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "            \n",
    "            if return_img_per_step:\n",
    "                imgs_per_step.append(inp_imgs.clone().detach())\n",
    "        \n",
    "        # Reactivate gradients for parameters for training\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "        model.train(is_training)\n",
    "        \n",
    "        # Reset gradient calculation to setting before this function\n",
    "        torch.set_grad_enabled(had_gradients_enabled)\n",
    "\n",
    "        if return_img_per_step:\n",
    "            return torch.stack(imgs_per_step, dim=0)\n",
    "        else:\n",
    "            return inp_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of the buffer becomes a bit clearer in the following algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training algorithm\n",
    "\n",
    "With the sampling buffer being ready, we can complete our training algorithm. Below is shown a summary of the full training algorithm of an energy model on image modeling: \n",
    "\n",
    "<center width=\"100%\" style=\"padding: 15px\"><img src=\"training_algorithm.svg\" width=\"700px\"></center>\n",
    "\n",
    "The first few statements in each training iteration concern the sampling of the real and fake data, as we have seen above with the sample buffer. Next, we calculate the contrastive divergence objective using our energy model $E_{\\theta}$. However, one additional training trick we need is to add a regularization loss on the output of $E_{\\theta}$. As the output of the network is not constrained and adding a large bias or not to the output doesn't change the contrastive divergence loss, we need to ensure somehow else that the output values are in a reasonable range. Without the regularization loss, the output values will fluctuate in a very large range. With this, we ensure that the values for the real data are around 0, and the fake data likely slightly lower (for noise or outliers the score can be still significantly lower). As the regularization loss is less important than the Contrastive Divergence, we have a weight factor $\\alpha$ which is usually quite some smaller than 1. Finally, we perform an update step with an optimizer on the combined loss and add the new samples to the buffer.\n",
    "\n",
    "Below, we put this training dynamic into a PyTorch Lightning module. Remember that, since we model $f_{\\theta}(x)=-E_{\\theta}(x)$, we need to be careful with switching all important signs, e.g. in the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepEnergyModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, img_shape, batch_size, alpha=0.1, lr=1e-4, beta1=0.0, **CNN_args):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.cnn = CNNModel(**CNN_args)\n",
    "        self.sampler = Sampler(self.cnn, img_shape=img_shape, sample_size=batch_size)\n",
    "        self.example_input_array = torch.zeros(1, *img_shape)\n",
    " \n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)\n",
    "        return z\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Energy models can have issues with momentum as the loss surfaces changes with its parameters. \n",
    "        # Hence, we set it to 0 by default. \n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr, betas=(self.hparams.beta1, 0.999))\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.97) # Exponential decay over epochs\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # We add minimal noise to the original images to prevent the model from focusing on purely \"clean\" inputs\n",
    "        real_imgs, _ = batch\n",
    "        small_noise = torch.randn_like(real_imgs) * 0.005\n",
    "        real_imgs.add_(small_noise).clamp_(min=-1.0, max=1.0)\n",
    "        \n",
    "        # Obtain samples\n",
    "        fake_imgs = self.sampler.sample_new_exmps(steps=60, step_size=10)\n",
    "        \n",
    "        # Predict energy score for all images\n",
    "        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "        real_out, fake_out = self.cnn(inp_imgs).chunk(2, dim=0)\n",
    "        \n",
    "        # Calculate losses\n",
    "        reg_loss = self.hparams.alpha * (real_out ** 2 + fake_out ** 2).mean()\n",
    "        cdiv_loss = fake_out.mean() - real_out.mean()\n",
    "        loss = reg_loss + cdiv_loss\n",
    "        \n",
    "        # Logging\n",
    "        self.log('loss', loss)\n",
    "        self.log('loss_regularization', reg_loss)\n",
    "        self.log('loss_contrastive_divergence', cdiv_loss)\n",
    "        self.log('metrics_avg_real', real_out.mean())\n",
    "        self.log('metrics_avg_fake', fake_out.mean())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # For validating, we calculate the contrastive divergence between purely random images and unseen examples\n",
    "        # Note that the validation/test step of energy-based models depends on what we are interested in the model\n",
    "        real_imgs, _ = batch\n",
    "        fake_imgs = torch.rand_like(real_imgs) * 2 - 1\n",
    "        \n",
    "        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "        real_out, fake_out = self.cnn(inp_imgs).chunk(2, dim=0)\n",
    "        \n",
    "        cdiv = fake_out.mean() - real_out.mean()\n",
    "        self.log('val_contrastive_divergence', cdiv)\n",
    "        self.log('val_fake_out', fake_out.mean())\n",
    "        self.log('val_real_out', real_out.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not implement a test step because energy-based, generative models are usually not evaluated on a test set. The validation step however is used to get an idea of the difference between ennergy/likelihood of random images to unseen examples of the dataset. Alternative test steps would be to generate new images and evaluate how realistic they are based on FID or Inception score, or try to denoise images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "\n",
    "To track the performance of our model during training, we will make extensive use of PyTorch Lightning's callback framework. Remember that callbacks can be used for running small functions at any point of the training, for instance after finishing an epoch. Here, we will use three different callbacks we define ourselves.\n",
    "\n",
    "The first callback, called `GenerateCallback`, is used for adding image generations to the model during training. After every $N$ epochs (usually $N=5$ to reduce output to TensorBoard), we take a small batch of random images and perform many MCMC iterations until the model's generation converges. Compared to the training that used 60 iterations, we use 256 here because (1) we only have to do it once compared to the training that has to do it every iteration, and (2) we do not start from a buffer here, but from scratch. It is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCallback(pl.Callback):\n",
    "\n",
    "    def __init__(self, batch_size=8, vis_steps=8, num_steps=256, every_n_epochs=5):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size         # Number of images to generate\n",
    "        self.vis_steps = vis_steps           # Number of steps within generation to visualize\n",
    "        self.num_steps = num_steps           # Number of steps to take during generation\n",
    "        self.every_n_epochs = every_n_epochs # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        # Skip for all other epochs\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            # Generate images\n",
    "            imgs_per_step = self.generate_imgs(pl_module)\n",
    "            # Plot and add to tensorboard\n",
    "            for i in range(imgs_per_step.shape[1]):\n",
    "                step_size = self.num_steps // self.vis_steps\n",
    "                imgs_to_plot = imgs_per_step[step_size-1::step_size,i]\n",
    "                grid = torchvision.utils.make_grid(imgs_to_plot, nrow=imgs_to_plot.shape[0], normalize=True, range=(-1,1))\n",
    "                trainer.logger.experiment.add_image(f\"generation_{i}\", grid, global_step=trainer.current_epoch)\n",
    "                \n",
    "    def generate_imgs(self, pl_module):\n",
    "        pl_module.eval()\n",
    "        start_imgs = torch.rand((self.batch_size,) + pl_module.hparams[\"img_shape\"]).to(pl_module.device)\n",
    "        start_imgs = start_imgs * 2 - 1\n",
    "        torch.set_grad_enabled(True)  # Tracking gradients for sampling necessary\n",
    "        imgs_per_step = Sampler.generate_samples(pl_module.cnn, start_imgs, steps=self.num_steps, step_size=10, return_img_per_step=True)\n",
    "        torch.set_grad_enabled(False)\n",
    "        pl_module.train()\n",
    "        return imgs_per_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second callback is called `SamplerCallback`, and simply adds a randomly picked subset of images in the sampling buffer to the TensorBoard. This helps to understand what images are currently shown to the model as \"fake\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplerCallback(pl.Callback):\n",
    "    \n",
    "    def __init__(self, num_imgs=32, every_n_epochs=5):\n",
    "        super().__init__()\n",
    "        self.num_imgs = num_imgs             # Number of images to plot\n",
    "        self.every_n_epochs = every_n_epochs # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "        \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            exmp_imgs = torch.cat(random.choices(pl_module.sampler.examples, k=self.num_imgs), dim=0)\n",
    "            grid = torchvision.utils.make_grid(exmp_imgs, nrow=4, normalize=True, range=(-1,1))\n",
    "            trainer.logger.experiment.add_image(\"sampler\", grid, global_step=trainer.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our last callback is `OutlierCallback`. This callback evaluates the model by recording the (negative) energy assigned to random noise. While our training loss is almost constant across iterations, this score is likely showing the progress of the model to detect \"outliers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierCallback(pl.Callback):\n",
    "    \n",
    "    def __init__(self, batch_size=1024):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        with torch.no_grad():\n",
    "            pl_module.eval()\n",
    "            rand_imgs = torch.rand((self.batch_size,) + pl_module.hparams[\"img_shape\"]).to(pl_module.device)\n",
    "            rand_imgs = rand_imgs * 2 - 1.0\n",
    "            rand_out = pl_module.cnn(rand_imgs).mean()\n",
    "            pl_module.train()\n",
    "        \n",
    "        trainer.logger.experiment.add_scalar(\"rand_out\", rand_out, global_step=trainer.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model\n",
    "\n",
    "Finally, we can add everything together to create our final training function. The function is very similar to any other PyTorch Lightning training function we have seen so far. However, there is the small difference of that we do not test the model on a test set because we will analyse the model afterward by checking its prediction and ability to perform outlier detection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(**kwargs):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"MNIST\"),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=60,\n",
    "                         gradient_clip_val=0.1,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor='val_contrastive_divergence'),\n",
    "                                    GenerateCallback(every_n_epochs=5),\n",
    "                                    SamplerCallback(every_n_epochs=5),\n",
    "                                    OutlierCallback(),\n",
    "                                    LearningRateMonitor(\"epoch\")\n",
    "                                   ])\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"MNIST.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = DeepEnergyModel.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model = DeepEnergyModel(**kwargs)\n",
    "        trainer.fit(model, train_loader, test_loader)\n",
    "        model = DeepEnergyModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    # No testing as we are more interested in other properties\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(img_shape=(1,28,28), \n",
    "                    batch_size=train_loader.batch_size,\n",
    "                    lr=1e-4,\n",
    "                    beta1=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "In the last part of the notebook, we will try to take the trained energy-based generative model, and analyse its properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "\n",
    "The first thing we can look at is the TensorBoard generate during training. This can help us to understand the training dynamic even better, and shows potential issues. Let's load the TensorBoard below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH!\n",
    "%tensorboard --logdir ../saved_models/tutorial8/tensorboards/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center width=\"100%\"><img src=\"tensorboard_screenshot.png\" width=\"1000px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the contrastive divergence as well as the regularization converge quickly to 0. However, the training continues although the loss is always close to zero. This is because our \"training\" data changes with the model by sampling. The progress of training can be best measured by looking at the samples across iterations, and the score for random images that decreases constantly over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Generation\n",
    "\n",
    "Another way of evaluating generative models is by sampling a few generated images. Generative models need to be good at generating realistic images as this truely shows that they have modeled the true data distribution. Thus, let's sample a few images of the model below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "pl.seed_everything(43)\n",
    "callback = GenerateCallback(batch_size=4, vis_steps=8, num_steps=256)\n",
    "imgs_per_step = callback.generate_imgs(model)\n",
    "imgs_per_step = imgs_per_step.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The characteristic of sampling with energy-based models is that they require the iterative MCMC algorithm. To gain an insight in how the images change over iterations, we plot a few intermediate samples in the MCMC as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(imgs_per_step.shape[1]):\n",
    "    step_size = callback.num_steps // callback.vis_steps\n",
    "    imgs_to_plot = imgs_per_step[step_size-1::step_size,i]\n",
    "    imgs_to_plot = torch.cat([imgs_per_step[0:1,i],imgs_to_plot], dim=0)\n",
    "    grid = torchvision.utils.make_grid(imgs_to_plot, nrow=imgs_to_plot.shape[0], normalize=True, range=(-1,1), pad_value=0.5, padding=2)\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(grid)\n",
    "    plt.xlabel(\"Generation iteration\")\n",
    "    plt.xticks([(imgs_per_step.shape[-1]+2)*(0.5+j) for j in range(callback.vis_steps+1)], \n",
    "               labels=[1] + list(range(step_size,imgs_per_step.shape[0]+1,step_size)))\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that although starting from noise in the very first step, the sampling algorithm obtains reasonable shapes after only 32 steps. Over the next 200 steps, the shapes become clearer and changed towards realistic digits. The specific samples can differ when you run the code on Colab, hence the following description is specific to the plots shown on the website. The first row shows an 8, where we remove unnecessary white parts over iterations. The transformation across iterations can be seen at best for the second sample, which creates a digit of 2. While the first sample after 32 iterations looks a bit like a digit, but not really, the sample is transformed more and more to a typical image of the digit 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-distribution detection\n",
    "\n",
    "A very common and strong application of energy-based models is out-of-distribution detection (sometimes referred to as \"anomaly\" detection). As more and more deep learning models are applied in production and applications, a crucial aspect of these models is to know what the models don't know. Deep learning models are usually overconfident, meaning that they classify even random images sometimes with 100% probability. Clearly, this is not something that we want to see in applications. Energy-based models can help with this problem because they are trained to detect images that do not fit the training dataset distribution. Thus, in those applications, you could train an energy-based model along with the classifier, and only output predictions if the energy-based models assign a (unnormalized) probability higher than $\\delta$ to the image. You can actually combine classifiers and energy-based objectives in a single model, as proposed in this [paper](https://arxiv.org/abs/1912.03263).\n",
    "\n",
    "In this part of the analysis, we want to test the out-of-distribution capability of our energy-based model. Remember that a lower output of the model denotes a low probability. Thus, we hope to see low scores if we enter random noise to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    rand_imgs = torch.rand((128,) + model.hparams.img_shape).to(model.device)\n",
    "    rand_imgs = rand_imgs * 2 - 1.0\n",
    "    rand_out = model.cnn(rand_imgs).mean()\n",
    "    print(f\"Average score for random images: {rand_out.item():4.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we hoped, the model assigns very low probability to those noisy images. As another reference, let's look at predictions for a batch of images from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_imgs,_ = next(iter(train_loader))\n",
    "    train_imgs = train_imgs.to(model.device)\n",
    "    train_out = model.cnn(train_imgs).mean()\n",
    "    print(f\"Average score for training images: {train_out.item():4.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are close to 0 because of the regularization objective that was added to the training. So clearly, the model can distinguish between noise and real digits. However, what happens if we change the training images a little, and see which ones gets a very low score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compare_images(img1, img2):\n",
    "    imgs = torch.stack([img1, img2], dim=0).to(model.device)\n",
    "    score1, score2 = model.cnn(imgs).cpu().chunk(2, dim=0)\n",
    "    grid = torchvision.utils.make_grid([img1.cpu(), img2.cpu()], nrow=2, normalize=True, range=(-1,1), pad_value=0.5, padding=2)\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(grid)\n",
    "    plt.xticks([(img1.shape[2]+2)*(0.5+j) for j in range(2)],\n",
    "               labels=[\"Original image\", \"Transformed image\"])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    print(f\"Score original image: {score1.item():4.2f}\")\n",
    "    print(f\"Score transformed image: {score2.item():4.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a random test image for this. Feel free to change it to experiment with the model yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs, _ = next(iter(test_loader))\n",
    "exmp_img = test_imgs[0].to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first transformation is to add some random noise to the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_noisy = exmp_img + torch.randn_like(exmp_img) * 0.3\n",
    "img_noisy.clamp_(min=-1.0, max=1.0)\n",
    "compare_images(exmp_img, img_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the score considerably drops. Hence, the model can detect random Gaussian noise on the image. This is also to expect as initially, the \"fake\" samples are pure noise images.\n",
    "\n",
    "Next, we flip an image and check how this influences the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_flipped = exmp_img.flip(dims=(1,2))\n",
    "compare_images(exmp_img, img_flipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the digit can only be read in this way, for example, the 7, then we can see that the score drops. However, the score only drops slightly. This is likely because of the small size of our model. Keep in mind that generative modeling is a much harder task than classification, as we do not only need to distinguish between classes but learn **all** details/characteristics of the digits. With a deeper model, this could eventually be captured better (but at the cost of greater training instability).\n",
    "\n",
    "Finally, we check what happens if we reduce the digit significantly in size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tiny = torch.zeros_like(exmp_img)-1\n",
    "img_tiny[:,exmp_img.shape[1]//2:,exmp_img.shape[2]//2:] = exmp_img[:,::2,::2]\n",
    "compare_images(exmp_img, img_tiny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score again drops but not by a large margin, although digits in the MNIST dataset usually are much larger. \n",
    "\n",
    "Overall, we can conclude that our model is good for detecting Gaussian noise and smaller transformations to existing digits. Nonetheless, to obtain a very good out-of-distribution model, we would need to train deeper models and for more iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instability\n",
    "\n",
    "Finally, we should discuss the possible instabilities of energy-based models, in particular for the example of image generation that we have implemented in this notebook. In the process of hyperparameter search for this notebook, there have been several models that diverged. Divergence in energy-based models means that the models assign a high probability to examples of the training set which is a good thing. However, at the same time, the sampling algorithm fails and only generates noise images that obtain minimal probability scores. This happens because the model has created many local maxima in which the generated noise images fall. The energy surface over which we calculate the gradients to reach data points with high probability has \"diverged\" and is not useful for our MCMC sampling.\n",
    "\n",
    "Besides finding the optimal hyperparameters, a common trick in energy-based models is to reload stable checkpoints. If we detect that the model is diverging, we stop the training, load the model from one epoch ago where it did not diverge yet. Afterward, we continue training and hope that with a different seed the model is not diverging again. Nevertheless, this should be considered as the \"last hope\" for stabilizing the models, and careful hyperparameter tuning is the better way to do so. Sensitive hyperparameters include `step_size`, `steps` and the noise standard deviation in the sampler, and the learning rate and feature dimensionality in the CNN model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have discussed energy-based models for generative modeling. The concept relies on the idea that any strictly positive function can be turned into a probability distribution by normalizing over the whole dataset. As this is not reasonable to calculate for high dimensional data like images, we train the model using contrastive divergence and sampling via MCMC. While the idea allows us to turn any neural network into an energy-based model, we have seen that there are multiple training tricks needed to stabilize the training. Furthermore, the training time of these models is relatively long as, during every training iteration, we need to sample new \"fake\" images, even with a sampling buffer. In the next lectures and assignment, we will see different generative models (e.g. VAE, GAN, NF) that allow us to do generative modeling more stably, but with the cost of more parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=⭐&message=Star%20Our%20Repository&color=yellow)](https://github.com/phlippe/uvadlc_notebooks/)  If you found this tutorial helpful, consider ⭐-ing our repository.    \n",
    "[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=❔&message=Ask%20Questions&color=9cf)](https://github.com/phlippe/uvadlc_notebooks/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
