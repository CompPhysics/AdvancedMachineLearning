\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{rnn}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \section{Recurrent neural network (RNN) from
scratch}\label{recurrent-neural-network-rnn-from-scratch}

    \textbf{Author:} Brage Andreas Trefjord.

Developed as part of a summer internship at the University of Oslo in
2024.

    \section{Sources and resources}\label{sources-and-resources}

    The contents of this notebook is largely based on the book Deep Learning
by Goodfellow et al., which you can buy in norwegian book stores, or
read online for free at \href{https://www.deeplearningbook.org/}{the
book's website}. Chapter 10 on sequence modeling is especially relevant.

All the diagrams drawn in this notebook are created by me using draw.io,
so feel free to use them in your projects. (Ask the professor or group
teachers for how to properly cite this notebook).

Other books you might find useful are
\href{https://www.oreilly.com/library/view/machine-learning-with/9781801819312/}{Machine
Learning with PyTorch and Scikit-Learn} by Raschka et al., and
\href{https://learning.oreilly.com/library/view/deep-learning-from/9781492041405/}{Deep
Learning from Scratch} by Seth Weidman. Both books can be found at
https://www.oreilly.com/, where students get free access through
https://www.oreilly.com/library-access/ (choose ``institution not
listed'' and use your UiO mail).

In addition, I would like to recommend
\href{https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi}{this
video playlist} by Grant Sanderson (3Blue1Brown), and
\href{https://www.youtube.com/playlist?list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1}{this
video playlist} by Josh Starmer (Statquest).

    \section{Recurrent neural networks
(RNNs)}\label{recurrent-neural-networks-rnns}

    \subsection{What is an RNN?}\label{what-is-an-rnn}

    A recurrent neural network (RNN), as opposed to a regular fully
connected neural network (FCNN), has layers that are connected to
themselves. This difference might be clearer by first looking at an
FCNN.

    

    In an FCNN there are no connections between nodes in a single layer. For
instance, \(h_1^1\) is not connected to \(h_2^1\). In addition, the
input and output are always of a fixed length.

    In an RNN, however, this is no longer the case. Nodes in the hidden
layers are connected to themselves, represented by the curved lines in
the figure below.

    

    Thus the output \(\vec{h}\) from the hidden layer is fed back into the
hidden layer. This recurrence makes RNNs useful when working with
sequential data, as we can have input of variable length. This is more
clear if we unfold the recurrent part of the network.

    

    Note that the nodes don't represent the same thing in the FCNN and RNN
diagrams above. In the FCNN, each node gives a \emph{scalar} value,
representing a certain feature of the layer, while the nodes in the RNN
represent \emph{vectors} containing all features at that layer and time
step. Thus the connections between the nodes in the RNN are in fact
dense connections as seen in th FCNN.

    

    \subsection{Why RNNs?}\label{why-rnns}

    Recurrent neural networks work very well when working with
\emph{sequential data}, i.e., data where the order matters. In a regular
fully connected network, the order of input doesn't really matter.
Consider a simplified version of the
\href{https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html}{Wisconsin
breast cancer dataset} with only two features, the \emph{mean radius}
and the \emph{mean area}. It doesn't matter to a neural network whether
\emph{mean radius} or \emph{mean area} comes first in the input, since
the order does not affect the interpretation of the data.

    Another great thing about RNNs is that they can handle variable input
and output. Consider again the simplified breast cancer dataset. If you
have trained a regular FCNN on the dataset with the two features, it
makes no sense to suddenly add a third feature. The network would not
know what to do with it, and would reject such inputs with three
features (or any other number of features that isn't two, for that
matter).

    It is, of course, not a problem that the regular FCNN can't handle
sequential data of variable input and output when working with datasets
such as the Wisconsin breast cancer dataset. Consider instead some other
problem, such as language translation. Let's say you want to train a
neural network that can translate norwegian sentences into english. Here
are a couple of sentences from the Norwegian bokmål - English data from
\href{https://www.manythings.org/anki/}{Anki}:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Norwegian & English \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Slutt å rope & Stop yelling \\
Jeg har vondt i hodet & My head hurts \\
Skremte jeg deg? & Did I scare you? \\
\end{longtable}

    Note that the order of the words is important for the translation. ``Did
I scare you?'' and ``I did scare you'' means very different things
(ignoring the question mark here), and while ``My head hurts'' is a
meaningful sentence, ``Head my hurts'' makes no sense. Also, the input
and output have variable length. The first input contains three words,
and outputs two words, the second input contains five words and outputs
three, and the last input takes three words and outputs four. A regular
neural network simply doesn't work for such cases, but a
\emph{recurrent} neural network is tailored specifically for such cases.

    \begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Note:} One might think that the words in the example above are
the \emph{features} of the input/output, but this is not really the
case. The feature length is, in our case, still the same for all inputs,
but we introduce instead another \emph{dimension} of the data compared
to the FCNN case. We will call the new dimension the \emph{sequence
dimension}, and the different sequence elements will be named \emph{time
steps} due to the sequential nature of the data. Each time step will
still contain some fixed number of features.

In the case of language modeling, such as the translation example above,
the sentences will be made up of tokens, which could be words, parts of
words, punctuation, etc., and each token is represented by a vector
where each element corresponds to some feature of the token. Language
processing is only one of the things RNNs can be used for, and, although
a very fascinating subject, it is something I know little about. If you
want to learn a bit more about language processing, check out
\href{https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi}{this
video series} by Grant Sanderson (3Blue1Brown). He starts talking about
language processing in the fifth video, but if you haven't watched the
first four already I would highly recommend them as they give a clear
and visual understanding of how regular neural networks work.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

    \subsection{Limitations and improvements of
RNNs}\label{limitations-and-improvements-of-rnns}

    One of the biggest limitations of RNNs is the problem of
exploding/vanishing gradients. We want to backpropagate through the RNN
to tune the weights, but if the input data is a very long sequence, we
have to backpropagate through very many nodes. If the weight connecting
time steps is very small, the backpropagation through many time steps
causes the gradient to decrease very rapidly (they \emph{vanish}). If
the weight is very large, gradients increase very rapidly (they
\emph{explode}). Thus, a simple RNN like the one we will create in this
notebook will perform poorly on datasets of long sequences.

There are ways to make RNNs perform better with long sequences, the
perhaps most prominent being \emph{gated RNNs} such as the \emph{long
short-term memory} (LSTM) and the \emph{gated recurrent unit} (GRU). In
this notebook we will only develop a simple RNN, which will have its
limitations on what data we can look at and how good our results will
be, but feel free to look up LSTMs, GRUs, and other methods of improving
the RNN.

    \section{The mathematics of RNNs}\label{the-mathematics-of-rnns}

    Now that we know what RNNs are, and why they are useful, let's get into
some of the math that builds up the network. We will start by looking at
the architecture of the RNN and go through the notation I use in this
notebook. After that, we will derive the equations needed for forward-
and backpropagation through the network.

    \subsection{The RNN architecture}\label{the-rnn-architecture}

    Consider some sequential input \(X\) with \(n\) features. Note that
\(X\) here is an array with two axes, since it contains \(n\) features
at each time step in the sequence. We will denote the input at a
specific time step \(t\) as \[\vec{X}^{(t)} = \begin{pmatrix}
X^{(t)}_1 \\ \vdots \\ X^{(t)}_n
\end{pmatrix},\] which is then an \(n\)-dimensional vector.

    Next, consider an RNN with \(L\) hidden layers, and an output layer with
\(m\) features. We will denote the output of the \(l\)'th hidden layer
at time step \(t\) as \[\vec{h}_l^{(t)} = \begin{pmatrix}
h_{l, 1}^{(t)} \\ \vdots \\ h_{l, n_l}^{(t)}
\end{pmatrix},\] with \(n_l\) being the number of features in the
\(l\)'th hidden layer. The output of the RNN at time step \(t\) is
denoted \[\hat{\vec{y}}^{(t)} = \begin{pmatrix}
\hat{y}_1 \\ \vdots \\ \hat{y}_m
\end{pmatrix},\] where the hat is there to distinguish the RNN output
\(\hat{\vec{y}}^{(t)}\) from the target value, which is denoted
\(\vec{y}^{(t)}\). The RNN will then look like this.

    

    \subsection{Forward propagation}\label{forward-propagation}

    In order to propagate forward through the network we need some weights
and biases to connect the nodes. To simplify the notation going forward,
we will consider the input layer to be the \emph{zeroth layer}, and the
output layer to be the \emph{\(L+1\)'th layer}. We need each node to
propagate to the node at the next layer (keeping the time step
constant), and the next time step (keeping the layer constant), except
for the input and output layers which do not connect to each other (as
illustrated in the diagram above).

    Let \(W^{l,l+1}\) be the weight matrix and \(\vec{b}^{l,l+1}\) the bias
vector, both connecting nodes at the \(l\)'th layer to the \(l+1\)'th
layer, keeping the time step constant. Next, let \(W^{ll}\) be the
weight matrix and \(\vec{b}^{ll}\) the bias vector, both connecting
nodes at subsequent time steps in the same layer. Also, let \(\sigma_l\)
be the activation function in the \(l\)'th layer. Lastly, define the
weighted sum \(\vec{z}_l^{(t)}\) at layer \(l\) and time step \(t\) such
that the output of the node is the activation of that weighted sum, that
is, such that \(\vec{h}_l^{(t)} = \sigma_l (\vec{z}_l^{(t)})\).

    Using these definitions the output from the first hidden layer at the
first time step is then
\[ \vec{h}_1^{(1)} = \sigma_1 \left( \vec{z}_1^{(1)} \right), \] with
\[ \vec{z}_1^{(1)} = W^{01} \vec{X}^{(1)} + \vec{b}^{01}.\]

    At later time steps we will also need to consider the contribution from
the previous time step. Hence for \(t \geq 2\) we will define
\[\left( \vec{z}_1^{(t)} \right)_\text{layer} = W^{01} X^{(t)} + \vec{b}^{01}\]
\[\left( \vec{z}_1^{(t)} \right)_\text{time} = W^{11} \vec{h}_1^{(t-1)} + \vec{b}^{11},\]
such that \(\left( \vec{z}_1^{(t)} \right)_\text{layer}\) is the
contribution from the previous layer, and
\(\left( \vec{z}_1^{(t)} \right)_\text{time}\) is the contribution from
the previous time step. We then have
\[\vec{z}_1^{(t)} = \left( \vec{z}_1^{(t)} \right)_\text{layer} + \left( \vec{z}_1^{(t)} \right)_\text{time},\]
and \[\vec{h}_1^{(t)} = \sigma_1 \left( \vec{z}_1^{(t)} \right).\]

    The expression is exactly the same for any hidden node, but for
\(l \geq 2\) we substitute \(\vec{X}^{(t)}\) with
\(\vec{h}_{l-1}^{(t)}\). Thus for the \(l\)'th layer and \(t\)'th time
step we have
\[ \left( \vec{z}_l^{(t)} \right)_{layer} = W^{l-1,l} \vec{h}_{l-1}^{(t)} + \vec{b}^{l-1,l} \]
and
\[ \left( \vec{z}_l^{(t)} \right)_{time} = W^{ll} \vec{h}_{l}^{(t-1)} + \vec{b}^{ll}, \]
that combine to give
\[ \vec{z}_l^{(t)} = \left( \vec{z}_l^{(t)} \right)_{layer} + \left( \vec{z}_l^{(t)} \right)_{time}, \]
which in turn results in
\[ \vec{h}_l^{(t)} = \sigma_l \left( \vec{z}_l^{(t)} \right). \] This is
also valid at the first time step by setting
\(\left( \vec{z}_l^{(1)} \right)_\text{time} = 0\).

    The expression for the output layer is exactly the same as above, but
with \(\left( \vec{z}_l^{(t)} \right)_\text{time} = 0\). Thus we have
\[ \vec{z}_{L+1}^{(t)} = \left( \vec{z}_{L+1}^{(t)} \right)_\text{layer} = W^{L,L+1} \vec{h}_L^{(t)} + \vec{b}^{L,L+1} \]
and
\[ \hat{\vec{y}}^{(t)} = \sigma_{L+1} \left( \vec{z}_{L+1}^{(t)} \right) \]

    The equations given for the forward propagation can seem a bit messy, so
it is nice to have a more visual aid of what is going on. Here is a
diagram of the complete RNN including the weights and biases relating
the different nodes.

    

    And here is a weights and biases connected to a single arbitrary node.
The green arrows represent input to the node, and the red arrows
represent the output from the node.

    

    And here is the connections resulting in \(\vec{h}_l^{(t)}\) in more
detail.

    

    \subsection{Backpropagation through time
(BPTT)}\label{backpropagation-through-time-bptt}

    Backpropagation in an RNN works by comparing the output of the network
to some target output (just as in the regular neural network), and
propagating backwards through both the layers and the \emph{time
sequence}. It is therefore commonly referred to as \emph{backpropagation
through time} (BPTT). We will now derive the necessary equations to
perform BPTT.

    We assume that we have propagated forward through the network, and have
produced some output \(\hat{\vec{y}}^{(t)}\). We want to compare this
with some target output value \(\vec{y}^{(t)}\), and will do so through
a cost function \(C \left(\hat{\vec{y}}, \vec{y} \right)\). We will
denote the cost at a specific time step \(t\) by
\(C^{(t)} = C^{(t)} \left(\hat{\vec{y}}^{(t)}, \vec{y}^{(t)} \right)\),
and the overall cost of the network as \(C\).

    From the cost function at each time step, we want to compute the
gradient with respect to each weight and bias, that is, we want to
compute \[
\frac{\partial C}{\partial W^{l_1 l_2}} \; \text{ and } \; \frac{\partial C}{\partial \vec{b}^{l_1 l_2}}
\]

    We will do this one layer at a time, starting at the output layer, and
propagating backwards through time in each layer. We assume that we know
the gradient of the cost function with respect to the output
\(\frac{\partial C^{(t)}}{\partial \hat{\vec{y}}^{(t)}}\), and start by
finding the gradient with respect to the output weights and biases
\(W^{L,L+1}\) and \(\vec{b}^{L,L+1}\).

    \subsubsection{Backpropagation through the output
layer}\label{backpropagation-through-the-output-layer}

    First, we want to find the gradient with respect to
\(\vec{z}_{L+1}^{(t)}\). The derivative of \(C\) with respect to some
element \(z_{L+1, i}^{(t)}\) of the weighted sum is given by

\begin{align*}
\frac{\partial C}{\partial z_{L+1,i}^{(t)}} &= \frac{\partial C^{(t)}}{\partial z_{L+1,i}^{(t)}}
\\[4ex]
&= \sum_{j=1}^m \frac{\partial C^{(t)}}{\partial \hat{y}_j^{(t)}}  \frac{\partial \hat{y}_j^{(t)}}{\partial z_{L+1,i}^{(t)}}
\\[4ex]
&= \sum_{j=1}^m \frac{\partial C^{(t)}}{\partial \hat{y}_j^{(t)}}  \sigma_{L+1}^\prime \left( z_{L+1,i}^{(t)} \right) \delta_{ij}
\\[4ex]
&= \frac{\partial C^{(t)}}{\partial \hat{y}_i^{(t)}}  \sigma_{L+1}^\prime \left( z_{L+1,i}^{(t)} \right)
\end{align*}

where \(\delta_{ij}\) is the Kronecker delta
\(\delta_{ij} = \begin{cases}
0, & i \neq j\\
1, & i = j
\end{cases}\), and \(\sigma_{L+1}^\prime\) denotes the derivative of the
activation function, which we will assume to be known. we can write this
expression more compactly in vector form as \[
\frac{\partial C}{\partial \vec{z}_{L+1}^{(t)}} = \frac{\partial C^{(t)}}{\partial \hat{\vec{y}}^{(t)}} \odot \sigma_{L+1}^\prime \left( \vec{z}_{L+1}^{(t)} \right),
\] where \(\odot\) denotes the \emph{Hadamard product}, an elementwise
multiplication of two vectors/matrices of same size.

    \begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Note:} Sometimes the derivatives are real numbers like
\(\frac{\partial C^{(t)}}{\partial z_{L+1,i}^{(t)}}\), sometimes they
are vectors such as
\(\frac{\partial C^{(t)}}{\partial \vec{z}_{L+1}^{(t)}}\), and sometimes
they are matrices. I have not included any explicit notation to explain
when they are what, but will assume that this is understood implicitly.
A general rule would be to look at whether the expression contains
indices like \(i,j,k,\ldots\) or not.

\textbf{Another note:} There are a lot of indices to keep track of, so
to make the notation simpler to follow I will try to follow these rules
consistently: - \(l\) = layer index (with \(L\) being the final hidden
layer). If I need several layer indices I will use \(l_1,l_2,\ldots\). -
\((t)\) = time step index. - \(i,j,k\) = vector/matrix elements. - \(n\)
= number of input features (length of \(\vec{X}\)). - \(m\) = number of
output features (length of \(\hat{\vec{y}}\)). - \(n_1,n_2,\ldots\) =
number of features in hidden layer number \(1,2,\ldots\).

\textbf{Third note:} I will not always write the upper bound of
summations explicitly, but will assume that this is understood
implicitly. For instance, \(\sum_j W^{l-1,l}_{ij} h_{l-1,j}\) should be
understood to mean \(\sum_{j=1}^{n_{l-1}} W^{l-1,l}_{ij} h_{l-1,j}\),
such that it sums over all elements of \(\vec{h}_{l-1}\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

    The derivative with respect to the weighted sum will be used a lot
during backpropagation, so we will give it its own notation
\[ \vec{\delta}_{L+1}^{(t)} \equiv \frac{\partial C^{(t)}}{\partial \vec{z}_{L+1}^{(t)}} = \frac{\partial C^{(t)}}{\partial \hat{\vec{y}}^{(t)}} \odot \sigma_{L+1}^\prime \left( \vec{z}_{L+1}^{(t)} \right).\]
\(\delta_{L+1}^{(t)}\) has one index downstairs (denoting layer), and
one index upstairs in parentheses (denoting time step), so don't mix it
up with the Kronecker delta \(\delta_{ij}\), which I will consistently
write with two indices downstairs.

    From the delta we can find the cost gradient with respect to the output
bias. Note that the same weights and biases occur several times in the
RNN, so we have to sum over each contribution. The cost gradients with
respect to the weights and biases in layer \(l\) are denoted
\(\frac{\partial C}{\partial W^{l-1,l}}\),
\(\frac{\partial C}{\partial W^{ll}}\),
\(\frac{\partial C}{\partial \vec{b}^{l-1,l}}\) and
\(\frac{\partial C}{\partial \vec{b}^{ll}}\), and we will denote the
contribution at time step \(t\) as
\(\left(\frac{\partial C}{\partial W^{l-1,l}} \right)^{(t)}\),
\(\left( \frac{\partial C}{\partial W^{ll}} \right)^{(t)}\),
\(\left( \frac{\partial C}{\partial \vec{b}^{l-1,l}} \right)^{(t)}\) and
\(\left( \frac{\partial C}{\partial \vec{b}^{ll}} \right)^{(t)}\) such
that
\(\frac{\partial C}{\partial W^{l-1,l}} = \sum_t \left( \frac{\partial C}{\partial W^{l-1,l}}\right)^{(t)}\)
and so on. Using this notation, the gradient with respect to the output
bias becomes

\begin{align*}
\left( \frac{\partial C}{\partial b^{L,L+1}_i} \right)^{(t)} &= \sum_{j=1}^m \frac{\partial C}{\partial z_{L+1,j}^{(t)}} \frac{\partial z_{L+1,j}^{(t)}}{\partial b^{L,L+1}_i}
\\[4ex]
&= \sum_{j=1}^m \frac{\partial C}{\partial z_{L+1,j}^{(t)}} \frac{\partial}{\partial b^{L,L+1}_i} \left( \sum_k W^{L,L+1}_{jk} h_{L,k}^{(t)} + b^{L,L+1}_j \right)
\\[4ex]
&= \sum_{j=1}^m \frac{\partial C}{\partial z_{L+1,j}^{(t)}} \delta_{ij}
\\[4ex]
&= \frac{\partial C}{\partial z_{L+1,i}^{(t)}}
\\[4ex]
&= \delta_{L+1,i}^{(t)}.
\end{align*}

Thus on vector form we have
\[ \left( \frac{\partial C}{\partial \vec{b}^{L,L+1}} \right)^{(t)} = \vec{\delta}_{L+1}^{(t)},\]
and finally
\[\frac{\partial C}{\partial \vec{b}^{L,L+1}} = \sum_t \left( \frac{\partial C}{\partial \vec{b}^{L,L+1}} \right)^{(t)}\]

    We can also compute the gradient with respect to the output weights

\begin{align*}
\left( \frac{\partial C}{W^{L,L+1}_{ij}} \right)^{(t)} &= \sum_{k_1=1}^m \frac{\partial C}{\partial z_{L+1,k_1}^{(t)}} \frac{\partial z_{L+1,k_1}^{(t)}}{\partial W^{L,L+1}_{ij}}
\\[4ex]
&= \sum_{k_1=1}^m \delta_{L+1,k_1}^{(t)} \frac{\partial}{\partial W^{L,L+1}_{ij}}
\left( \sum_{k_2} W^{L,L+1}_{k_1 k_2} h_{L,k_2}^{(t)} + b^{L,L+1}_{k_1} \right)
\\[4ex]
&= \sum_{k_1=1}^m \delta_{L+1,k_1}^{(t)} \sum_{k_2} h_{L,k_2}^{(t)} \delta_{i k_1} \delta_{j k_2}
\\[4ex]
&= \delta_{L+1,i}^{(t)} h_{L,j}^{(t)}
\\[4ex]
&= \left[ \vec{\delta}_{L+1}^{(t)} \left(\vec{h}_{L}^{(t)}\right)^T \right]_{ij}.
\end{align*}

Thus on vector form we have

\[ \left( \frac{\partial C}{W^{L,L+1}} \right)^{(t)} = \vec{\delta}_{L+1}^{(t)} \left(\vec{h}_{L}^{(t)}\right)^T, \]
and
\[\frac{\partial C}{W^{L,L+1}} = \sum_t \left( \frac{\partial C}{W^{L,L+1}} \right)^{(t)}.\]

    Note that we here have an outer product between two vectors, which
results in a matrix:

\[
\vec{\delta}_{L+1}^{(t)} \left(\vec{h}_{L}^{(t)}\right)^T
=
\begin{pmatrix}
\delta_{L+1,1}^{(t)} \\ \vdots \\ \delta_{L+1,m}^{(t)}
\end{pmatrix}
\begin{pmatrix}
h_{L,1}^{(t)} & \cdots & h_{L,n_L}^{(t)}
\end{pmatrix}
=
\begin{pmatrix}
\delta_{L+1,1}^{(t)} h_{L,1}^{(t)} & \cdots & \delta_{L+1,1}^{(t)} h_{L,n_L}^{(t)}
\\
\vdots & \ddots & \vdots
\\
\delta_{L+1,m}^{(t)} h_{L,1}^{(t)} & \cdots & \delta_{L+1,m}^{(t)} h_{L,n_L}^{(t)}
\end{pmatrix}
\]

    Lastly, we need to compute the gradient with respect to the output from
the previous layer \(\frac{\partial C}{\partial \vec{h}_L^{(t)}}\), in
order to continue backpropagating through previous layers. We find this
in much the same way as we found the other gradients above.

\begin{align*}
\frac{\partial C}{\partial h_{L,i}^{(t)}} &= \sum_j \frac{\partial C}{z_{L+1,j}^{(t)}} \frac{\partial z_{L+1,j}^{(t)}}{\partial h_{L,i}^{(t)}}
\\[4ex]
&= \sum_j \delta_{L+1,j}^{(t)} \frac{\partial}{\partial h_{L,i}^{(t)}} \left( \sum_k W^{L,L+1}_{jk} h_{L,k}^{(t)} + b_j^{L,L+1} \right)
\\[4ex]
&= \sum_j \delta_{L+1,j}^{(t)} \sum_k W^{L,L+1}_{jk} \delta_{ik}
\\[4ex]
&= \sum_j \delta_{L+1,j}^{(t)} W^{L,L+1}_{ji}
\\[4ex]
&= \sum_j \left[ \left( W^{L,L+1} \right)^T \right]_{ij} \delta_{L+1,j}^{(t)}
\\[4ex]
&= \left[ \left(W^{L,L+1} \right)^T \vec{\delta}_{L+1}^{(t)} \right]_i
\end{align*}

And thus on vector form we have

\[\frac{\partial C}{\partial \vec{h}_L^{(t)}} = \left( W^{L,L+1} \right)^T \vec{\delta}_{L+1}^{(t)}\]

    Here is a diagram showing the backpropagation through the output layer.

    

    \subsubsection{Backpropagation through arbitrary
node}\label{backpropagation-through-arbitrary-node}

    Consider some arbitrary node in the RNN with output \(\vec{h}_l^{(t)}\).
Assume you know the total gradient of the cost with respect to this
output from the two suceeding nodes \[
\frac{\partial C}{\partial \vec{h}_l^{(t)}} = \left( \frac{\partial C}{\partial \vec{h}_l^{(t)}} \right)_\text{layer} + \left( \frac{\partial C}{\partial \vec{h}_l^{(t)}} \right)_\text{time}.
\] We now want to compute the gradients with respect to the weights and
biases connecting the two previous nodes to this node, so that we can
update these weights and biases when training the network, as well as
the gradient with respect to the two previous nodes, so that we can
continue backpropagation through the other nodes. The situation is
illustrated in the diagram below. The blue arrows show the input
gradient from the succeeding nodes, and the red arrows show the
gradients we want to compute.

    

    The necessary gradients are derived in the same way as for the output
layer, so I will simply state the results here. We get the following set
of equations for backpropagating through a general node in the RNN.
\begin{align}
\delta_l^{(t)} &= \frac{\partial C}{\partial \vec{h}_l^{(t)}} \odot \sigma_l^\prime \left(\vec{z}_l^{(t)} \right)
\\[4ex]
\left( \frac{\partial C}{\partial \vec{b}^{l-1,l}} \right)^{(t)} = \left( \frac{\partial C}{\partial \vec{b}^{ll}} \right)^{(t)} &= \delta_l^{(t)}
\\[4ex]
\left( \frac{\partial C}{\partial W^{l-1,l}} \right)^{(t)} &= \delta_l^{(t)} \left( \vec{h}_{l-1}^{(t)} \right)^T
\\[4ex]
\left( \frac{\partial C}{\partial W^{ll}} \right)^{(t)} &= \delta_l^{(t)} \left( \vec{h}_l^{(t-1)} \right)^T
\\[4ex]
\frac{\partial C}{\partial \vec{h}_{l-1}^{(t)}} &= \left[ \left( W^{l-1,l} \right)^{(t)} \right]^T \delta_l^{(t)}
\\[4ex]
\frac{\partial C}{\partial \vec{h}_{l}^{(t-1)}} &= \left[ \left( W^{ll} \right)^{(t-1)} \right]^T \delta_l^{(t)},
\end{align}

    and \begin{align}
\frac{\partial C}{\partial \vec{b}^{l-1,l}} &= \sum_t \left( \frac{\partial C}{\partial \vec{b}^{l-1,l}} \right)^{(t)}
\\[4ex]
\frac{\partial C}{\partial \vec{b}^{ll}} &= \sum_t \left( \frac{\partial C}{\partial \vec{b}^{ll}} \right)^{(t)}
\\[4ex]
\frac{\partial C}{\partial W^{l-1,l}} &= \sum_t \left( \frac{\partial C}{\partial W^{l-1,l}} \right)^{(t)}
\\[4ex]
\frac{\partial C}{\partial W^{ll}} &= \sum_t \left( \frac{\partial C}{\partial W^{ll}} \right)^{(t)}.
\end{align}

    With this method we can start with the nodes in the output layer, and
propagate backwards. The necessary input to one node is the output from
backpropagating through the previous node. Thus we can use the equations
above recursively, layer by layer, to backpropagate through the entire
network.

    

    \section{The RNN code}\label{the-rnn-code}

    Now that we have the mathematical framework, we can develop the code for
the RNN.

    \subsection{Functions}\label{functions}

    Before we start building a recurrent neural network, we need to define
some functions. We need activation functions, cost functions and a way
to differentiate these. We also need gradient descent schedulers to
update our weights and biases when backpropagating. These functions are
defined in this section.

    \subsubsection{Activation functions}\label{activation-functions}

    We want to be able to choose which activation function to use in
different layers of the RNN. Here we define some activation functions
that can be used by the network. If you have developed a regular fully
connected neural network in FYS-STK4155 these functions will probably
look very familiar, as they are pretty much copied from those lecture
notes. The main difference is that I have used JAX instead of autograd
for automatic differentiation. Due to the way JAX vectorizes, I have not
gotten gradients with jax to work for softmax, but have included
grad\_softmax() as its own function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{jax}\PY{n+nn}{.}\PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{jnp}

\PY{k}{def} \PY{n+nf}{identity}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{X}


\PY{k}{def} \PY{n+nf}{sigmoid}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
    \PY{k}{try}\PY{p}{:}
        \PY{k}{return} \PY{l+m+mf}{1.0} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{jnp}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{X}\PY{p}{)}\PY{p}{)}
    \PY{k}{except} \PY{n+ne}{FloatingPointError}\PY{p}{:}
        \PY{k}{return} \PY{n}{jnp}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{X} \PY{o}{\PYZgt{}} \PY{n}{jnp}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,} \PY{n}{jnp}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,} \PY{n}{jnp}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{softmax}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
    \PY{n}{X} \PY{o}{=} \PY{n}{X} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    \PY{n}{delta} \PY{o}{=} \PY{l+m+mf}{10e\PYZhy{}10}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{X}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)} \PY{o}{+} \PY{n}{delta}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{grad\PYZus{}softmax}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
    \PY{n}{f} \PY{o}{=} \PY{n}{softmax}\PY{p}{(}\PY{n}{X}\PY{p}{)}
    \PY{k}{return} \PY{n}{f} \PY{o}{\PYZhy{}} \PY{n}{f}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}


\PY{k}{def} \PY{n+nf}{RELU}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{jnp}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{X} \PY{o}{\PYZgt{}} \PY{n}{jnp}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{jnp}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{LRELU}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
    \PY{n}{delta} \PY{o}{=} \PY{l+m+mf}{10e\PYZhy{}4}
    \PY{k}{return} \PY{n}{jnp}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{X} \PY{o}{\PYZgt{}} \PY{n}{jnp}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{delta} \PY{o}{*} \PY{n}{X}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{tanh}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{jnp}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{Cost functions}\label{cost-functions}

    Next, we need to implement the cost functions. We include three cost
functions here. The ordinary least squares (OLS) is used for regression
problems, and the logistic regression and cross-entropy are used for
classification problems.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{CostOLS}\PY{p}{(}\PY{n}{target}\PY{p}{)}\PY{p}{:}

    \PY{k}{def} \PY{n+nf}{func}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{p}{(}\PY{l+m+mf}{1.0} \PY{o}{/} \PY{n}{target}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{*} \PY{n}{jnp}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{target} \PY{o}{\PYZhy{}} \PY{n}{X}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}

    \PY{k}{return} \PY{n}{func}


\PY{k}{def} \PY{n+nf}{CostLogReg}\PY{p}{(}\PY{n}{target}\PY{p}{)}\PY{p}{:}

    \PY{k}{def} \PY{n+nf}{func}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}

        \PY{k}{return} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{l+m+mf}{1.0} \PY{o}{/} \PY{n}{target}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{*} \PY{n}{jnp}\PY{o}{.}\PY{n}{sum}\PY{p}{(}
            \PY{p}{(}\PY{n}{target} \PY{o}{*} \PY{n}{jnp}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{X} \PY{o}{+} \PY{l+m+mf}{10e\PYZhy{}10}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{target}\PY{p}{)} \PY{o}{*} \PY{n}{jnp}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{X} \PY{o}{+} \PY{l+m+mf}{10e\PYZhy{}10}\PY{p}{)}\PY{p}{)}
        \PY{p}{)}

    \PY{k}{return} \PY{n}{func}


\PY{k}{def} \PY{n+nf}{CostCrossEntropy}\PY{p}{(}\PY{n}{target}\PY{p}{)}\PY{p}{:}

    \PY{k}{def} \PY{n+nf}{func}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{l+m+mf}{1.0} \PY{o}{/} \PY{n}{target}\PY{o}{.}\PY{n}{size}\PY{p}{)} \PY{o}{*} \PY{n}{jnp}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{target} \PY{o}{*} \PY{n}{jnp}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{X} \PY{o}{+} \PY{l+m+mf}{10e\PYZhy{}10}\PY{p}{)}\PY{p}{)}

    \PY{k}{return} \PY{n}{func}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{Automatic
differentiation}\label{automatic-differentiation}

    As mentioned above, we use JAX for automatic differentiation, which is
done with the function \emph{grad} in the JAX library. For \emph{grad}
to work on a function, it cannot use regular numpy, but must use
\emph{jax.numpy}, which is why we imported and used this when defining
the activation and cost functions. JAX's numpy is only used for these
functions, while we stick with regular numpy for everything else.

The RELU and leaky RELU activation functions are not continuously
differentiable, so we will handle these explicitly in our code.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{jax} \PY{k+kn}{import} \PY{n}{grad}

\PY{k}{def} \PY{n+nf}{derivate}\PY{p}{(}\PY{n}{func}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{n}{func}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}name\PYZus{}\PYZus{}} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RELU}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}

        \PY{k}{def} \PY{n+nf}{func}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
            \PY{k}{return} \PY{n}{jnp}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{X} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}

        \PY{k}{return} \PY{n}{func}

    \PY{k}{elif} \PY{n}{func}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}name\PYZus{}\PYZus{}} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LRELU}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}

        \PY{k}{def} \PY{n+nf}{func}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
            \PY{n}{delta} \PY{o}{=} \PY{l+m+mf}{10e\PYZhy{}4}
            \PY{k}{return} \PY{n}{jnp}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{X} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{delta}\PY{p}{)}

        \PY{k}{return} \PY{n}{func}

    \PY{k}{else}\PY{p}{:}
        \PY{k}{return} \PY{n}{grad}\PY{p}{(}\PY{n}{func}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Note that the \emph{grad} function is not, in itself, vectorized. This
means that if we send in an array to a function that has been
differentiated, JAX will treat this as a function with an array as
input, not as a function treating each element individually. This is
better understood with an example.

    Consider the function \(f(x)=x^2\), with derivative \(f^\prime (x)=2x\).
With JAX we get this with

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{f}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{x}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}

\PY{n}{df} \PY{o}{=} \PY{n}{grad}\PY{p}{(}\PY{n}{f}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    This works if we input a scalar value

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{x} \PY{o}{=} \PY{l+m+mf}{2.0}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
4.0
    \end{Verbatim}

    But if we try to input an array of values we get an error message.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}

\PY{k}{try}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
\PY{k}{except} \PY{n+ne}{TypeError} \PY{k}{as} \PY{n}{msg}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error message: }\PY{l+s+si}{\PYZob{}}\PY{n}{msg}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Error message: Gradient only defined for scalar-output functions. Output had
shape: (10,).
    \end{Verbatim}

    This is because JAX does not treat each element in \(x\) individually.
To get around this we can use the \emph{vmap} function in the JAX
library, which vectorizes the function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{jax} \PY{k+kn}{import} \PY{n}{vmap}
\PY{n}{df} \PY{o}{=} \PY{n}{vmap}\PY{p}{(}\PY{n}{grad}\PY{p}{(}\PY{n}{f}\PY{p}{)}\PY{p}{)}
\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[0.        0.6666667 1.3333334 2.        2.6666667 3.3333333 4.
 4.6666665 5.3333335 6.       ]
    \end{Verbatim}

    Note that \emph{vmap} only vectorizes along one dimension. So if the
input array contains several axes, we have to apply \emph{vmap} for each
axis.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{df} \PY{o}{=} \PY{n}{vmap}\PY{p}{(}\PY{n}{vmap}\PY{p}{(}\PY{n}{vmap}\PY{p}{(}\PY{n}{grad}\PY{p}{(}\PY{n}{f}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{27}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[[0.         0.23076923 0.46153846]
  [0.6923077  0.9230769  1.1538461 ]
  [1.3846154  1.6153846  1.8461539 ]]

 [[2.0769231  2.3076923  2.5384614 ]
  [2.7692308  3.         3.2307692 ]
  [3.4615386  3.6923077  3.9230769 ]]

 [[4.1538463  4.3846154  4.6153846 ]
  [4.8461537  5.076923   5.3076925 ]
  [5.5384617  5.769231   6.        ]]]
    \end{Verbatim}

    This approach is tedious, yes, but as far as I know it is the only way
to make JAX differentiate elementwise. If you find a simpler
work-around, feel free to share with the professor or the group teachers
so these notes can be updated.

    \subsubsection{Schedulers}\label{schedulers}

    We also want to be able to choose which method we want to use for
gradient descent when training the RNN. This is done by using a
scheduler defined below. These schedulers are identical to the ones used
in FYS-STK4155.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{Scheduler}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Abstract class for Schedulers}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{eta}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{eta} \PY{o}{=} \PY{n}{eta}

    \PY{c+c1}{\PYZsh{} should be overwritten}
    \PY{k}{def} \PY{n+nf}{update\PYZus{}change}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{gradient}\PY{p}{)}\PY{p}{:}
        \PY{k}{raise} \PY{n+ne}{NotImplementedError}

    \PY{c+c1}{\PYZsh{} overwritten if needed}
    \PY{k}{def} \PY{n+nf}{reset}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{k}{pass}


\PY{k}{class} \PY{n+nc}{Constant}\PY{p}{(}\PY{n}{Scheduler}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{eta}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n}{eta}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{update\PYZus{}change}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{gradient}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{eta} \PY{o}{*} \PY{n}{gradient}

    \PY{k}{def} \PY{n+nf}{reset}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{k}{pass}


\PY{k}{class} \PY{n+nc}{Momentum}\PY{p}{(}\PY{n}{Scheduler}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{eta}\PY{p}{:} \PY{n+nb}{float}\PY{p}{,} \PY{n}{momentum}\PY{p}{:} \PY{n+nb}{float}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n}{eta}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{momentum} \PY{o}{=} \PY{n}{momentum}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{change} \PY{o}{=} \PY{l+m+mi}{0}

    \PY{k}{def} \PY{n+nf}{update\PYZus{}change}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{gradient}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{change} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{momentum} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{change} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{eta} \PY{o}{*} \PY{n}{gradient}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{change}

    \PY{k}{def} \PY{n+nf}{reset}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{k}{pass}


\PY{k}{class} \PY{n+nc}{Adagrad}\PY{p}{(}\PY{n}{Scheduler}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{eta}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n}{eta}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}t} \PY{o}{=} \PY{k+kc}{None}

    \PY{k}{def} \PY{n+nf}{update\PYZus{}change}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{gradient}\PY{p}{)}\PY{p}{:}
        \PY{n}{delta} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}8}  \PY{c+c1}{\PYZsh{} avoid division ny zero}

        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}t} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}t} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{gradient}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{gradient}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}t} \PY{o}{+}\PY{o}{=} \PY{n}{gradient} \PY{o}{@} \PY{n}{gradient}\PY{o}{.}\PY{n}{T}

        \PY{n}{G\PYZus{}t\PYZus{}inverse} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{/} \PY{p}{(}
            \PY{n}{delta} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{diagonal}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}t}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}t}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{eta} \PY{o}{*} \PY{n}{gradient} \PY{o}{*} \PY{n}{G\PYZus{}t\PYZus{}inverse}

    \PY{k}{def} \PY{n+nf}{reset}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}t} \PY{o}{=} \PY{k+kc}{None}


\PY{k}{class} \PY{n+nc}{AdagradMomentum}\PY{p}{(}\PY{n}{Scheduler}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{eta}\PY{p}{,} \PY{n}{momentum}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n}{eta}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}t} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{momentum} \PY{o}{=} \PY{n}{momentum}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{change} \PY{o}{=} \PY{l+m+mi}{0}

    \PY{k}{def} \PY{n+nf}{update\PYZus{}change}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{gradient}\PY{p}{)}\PY{p}{:}
        \PY{n}{delta} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}8}  \PY{c+c1}{\PYZsh{} avoid division ny zero}

        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}t} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}t} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{gradient}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{gradient}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}t} \PY{o}{+}\PY{o}{=} \PY{n}{gradient} \PY{o}{@} \PY{n}{gradient}\PY{o}{.}\PY{n}{T}

        \PY{n}{G\PYZus{}t\PYZus{}inverse} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{/} \PY{p}{(}
            \PY{n}{delta} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{diagonal}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}t}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}t}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{change} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{change} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{momentum} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{eta} \PY{o}{*} \PY{n}{gradient} \PY{o}{*} \PY{n}{G\PYZus{}t\PYZus{}inverse}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{change}

    \PY{k}{def} \PY{n+nf}{reset}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}t} \PY{o}{=} \PY{k+kc}{None}


\PY{k}{class} \PY{n+nc}{RMS\PYZus{}prop}\PY{p}{(}\PY{n}{Scheduler}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{eta}\PY{p}{,} \PY{n}{rho}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n}{eta}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rho} \PY{o}{=} \PY{n}{rho}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{second} \PY{o}{=} \PY{l+m+mf}{0.0}


    \PY{k}{def} \PY{n+nf}{update\PYZus{}change}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{gradient}\PY{p}{)}\PY{p}{:}
        \PY{n}{delta} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}8}  \PY{c+c1}{\PYZsh{} avoid division ny zero}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{second} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rho} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{second} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rho}\PY{p}{)} \PY{o}{*} \PY{n}{gradient} \PY{o}{*} \PY{n}{gradient}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{eta} \PY{o}{*} \PY{n}{gradient} \PY{o}{/} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{second} \PY{o}{+} \PY{n}{delta}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{reset}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{second} \PY{o}{=} \PY{l+m+mf}{0.0}


\PY{k}{class} \PY{n+nc}{Adam}\PY{p}{(}\PY{n}{Scheduler}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{eta}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{rho2}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n}{eta}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rho} \PY{o}{=} \PY{n}{rho}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rho2} \PY{o}{=} \PY{n}{rho2}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{moment} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{second} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}epochs} \PY{o}{=} \PY{l+m+mi}{1}

    \PY{k}{def} \PY{n+nf}{update\PYZus{}change}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{gradient}\PY{p}{)}\PY{p}{:}
        \PY{n}{delta} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}8}  \PY{c+c1}{\PYZsh{} avoid division ny zero}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{moment} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rho} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{moment} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rho}\PY{p}{)} \PY{o}{*} \PY{n}{gradient}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{second} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rho2} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{second} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rho2}\PY{p}{)} \PY{o}{*} \PY{n}{gradient} \PY{o}{*} \PY{n}{gradient}

        \PY{n}{moment\PYZus{}corrected} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{moment} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rho}\PY{o}{*}\PY{o}{*}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}epochs}\PY{p}{)}
        \PY{n}{second\PYZus{}corrected} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{second} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rho2}\PY{o}{*}\PY{o}{*}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}epochs}\PY{p}{)}

        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{eta} \PY{o}{*} \PY{n}{moment\PYZus{}corrected} \PY{o}{/} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{second\PYZus{}corrected} \PY{o}{+} \PY{n}{delta}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{reset}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}epochs} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{moment} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{second} \PY{o}{=} \PY{l+m+mi}{0}
\end{Verbatim}
\end{tcolorbox}

    \subsection{The RNN}\label{the-rnn}

    We will now implement the code for the RNN. The network will be
object-oriented, consisting of the following classes:

\begin{itemize}
\tightlist
\item
  \emph{RNN}: The complete network, consisting of several \emph{Layer}
  objects.
\item
  \emph{Layer}: Abstract class containing information that is shared
  across the different types of layers. It is the parent class of the
  following:

  \begin{itemize}
  \tightlist
  \item
    \emph{InputLayer}: Layer containing the input to the network. Does
    not contain any weights and biases.
  \item
    \emph{RNNLayer}: The recurrent layer consisting of nodes in
    sequence.
  \item
    \emph{OutputLayer}: Recurrent layer for output. Similar to RNNLayer,
    but does not have any connections between the nodes, only to the
    nodes at the same time step in the previous layer.
  \item
    \emph{DenseLayer}: Fully connected layer, used to switch from a
    recurrent network to a regular fully connected network. Especially
    used for non-sequential output, for instance in classification where
    you want to classify the entire sequence with a single output.
  \end{itemize}
\item
  \emph{Node}: Contains information about a single node. This is where
  all the math of forward- and backpropagation takes place.
\end{itemize}

    In our code we want to be able to input batches of values, so that we
can feed forward (and backpropagate) many inputs at once. The input to
our network will therefore be an array with three axes:

\begin{itemize}
\tightlist
\item
  The input axis, separating the different inputs in the batch.
\item
  The sequence axis, separating the different time steps of the
  sequence.
\item
  The feature axis, separating the different features of the input (the
  vector elements).
\end{itemize}

This input is fed forward through the network, thus each layer has the
same three axes. Within the layers, we separate each time step into
their own node, thus the nodes only have two axes: the input and feature
axes.

    Since the RNN class is dependent on the Layer classes, and the Layer
classes are dependent on the Node class, we will build the network from
the down up, starting with the Node class.

    \subsubsection{The Node class}\label{the-node-class}

    The Node class takes care of the math discussed in the sections
\hyperref[forward-propagation]{Forward propagation} and
\hyperref[backpropagation-through-arbitrary-node]{Backpropagation through arbitrary node}.
I restate the relevant equations below.

    Forward propagation:

\begin{align*}
\left( \vec{z}_l^{(t)} \right)_{layer} &= W^{l-1,l} \vec{h}_{l-1}^{(t)} + \vec{b}^{l-1,l}
\\[4ex]
\left( \vec{z}_l^{(t)} \right)_{time} &= W^{ll} \vec{h}_{l}^{(t-1)} + \vec{b}^{ll}
\\[4ex]
\vec{z}_l^{(t)} &= \left( \vec{z}_l^{(t)} \right)_{layer} + \left( \vec{z}_l^{(t)} \right)_{time}
\\[4ex]
\vec{h}_l^{(t)} &= \sigma_l \left( \vec{z}_l^{(t)} \right)
\end{align*}

    Backpropagation:

\begin{align*}
\frac{\partial C}{\partial \vec{h}_l^{(t)}} &= \left( \frac{\partial C}{\partial \vec{h}_l^{(t)}} \right)_\text{layer} + \left( \frac{\partial C}{\partial \vec{h}_l^{(t)}} \right)_\text{time}
\\[4ex]
\delta_l^{(t)} &= \frac{\partial C}{\partial \vec{h}_l^{(t)}} \odot \sigma_l^\prime \left(\vec{z}_l^{(t)} \right)
\\[4ex]
\left( \frac{\partial C}{\partial \vec{b}^{l-1,l}} \right)^{(t)} = \left( \frac{\partial C}{\partial \vec{b}^{ll}} \right)^{(t)} &= \delta_l^{(t)}
\\[4ex]
\left( \frac{\partial C}{\partial W^{l-1,l}} \right)^{(t)} &= \delta_l^{(t)} \left( \vec{h}_{l-1}^{(t)} \right)^T
\\[4ex]
\left( \frac{\partial C}{\partial W^{ll}} \right)^{(t)} &= \delta_l^{(t)} \left( \vec{h}_l^{(t-1)} \right)^T
\\[4ex]
\frac{\partial C}{\partial \vec{h}_{l-1}^{(t)}} &= \left[ \left( W^{l-1,l} \right)^{(t)} \right]^T \delta_l^{(t)}
\\[4ex]
\frac{\partial C}{\partial \vec{h}_{l}^{(t-1)}} &= \left[ \left( W^{ll} \right)^{(t-1)} \right]^T \delta_l^{(t)}.
\end{align*}

    \begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Note:} The input and output from a node are two-dimensional
arrays, with different inputs along the first axis, and the features
along the second axis. The equations we have derived for computing
forward- and backpropagation assume that we are working with column
vectors, but when performing matrix multiplication with these
two-dimensional arrays we are in practice working with row vectors. This
has to be taken into account in our code.

Consider, for instance, multiplying some weight matrix \(W\) with a
vector \(\vec{h}\). The way we have derived the equations this will look
like

\[
W\vec{h} = \begin{pmatrix}
W_{11} & \cdots & W_{1n}\\
\vdots & \ddots & \vdots\\
W_{m1} & \cdots & W_{mn}
\end{pmatrix} \begin{pmatrix}
h_1 \\ \vdots \\ h_n
\end{pmatrix}.
\]

In the code, however, this will look like

\[
\vec{h} W = \begin{pmatrix}
h_{11} & \cdots & h_{1n}\\
\vdots & \ddots & \vdots\\
h_{N1} & \cdots & h_{Nn}
\end{pmatrix} \begin{pmatrix}
W_{11} & \cdots & W_{1m}\\
\vdots & \ddots & \vdots\\
W_{n1} & \cdots & W_{nm}
\end{pmatrix},
\]

where \(N\) is the number of inputs. Note that \(W\) and \(\vec{h}\)
have switched places in the matrix multiplication, and that \(W\) is
essentially a transpose of the corresponding \(W\) in the previous case
(since we are multiplying the columns with the \(\vec{h}\)-vectors,
instead of the rows).

This is not a big difference, but can be taken into account by reversing
the order of some of the matrix multiplications. The full set of changes
that must be done to account for this are listed here:

\begin{align*}
W \vec{h} \to \vec{h} W \\
\vec{\delta} \vec{h}^T \to \vec{h}^T \vec{\delta}\\
W^T \vec{\delta} \to \vec{\delta} W^T,
\end{align*}

and are taken care of in the code below.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{collections}\PY{n+nn}{.}\PY{n+nn}{abc} \PY{k+kn}{import} \PY{n}{Callable} \PY{c+c1}{\PYZsh{} Used for type hints of functions}

\PY{k}{class} \PY{n+nc}{Node}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Single node in the RNN. Computes forward propagation through a single node, stores the output of as a vector (1D array) of length \PYZsq{}n\PYZus{}features\PYZsq{}. This class also computes the backpropagation through the node, and stores all the relevant gradients.}

\PY{l+s+sd}{    Attributes}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    n\PYZus{}features (int)}
\PY{l+s+sd}{        Number of features in this node (the length of the vector).}
\PY{l+s+sd}{    act\PYZus{}func (Callable)}
\PY{l+s+sd}{        The activation function of this node.}
\PY{l+s+sd}{    \PYZob{}b/W\PYZcb{}\PYZus{}\PYZob{}layer/time\PYZcb{} (ndarray)}
\PY{l+s+sd}{        Bias(b) or weight matrix (W) for computing the contribution to this node from the node in the previous layer/time step.}
\PY{l+s+sd}{    h\PYZus{}\PYZob{}layer/time/output\PYZcb{} (ndarray)}
\PY{l+s+sd}{        The output of the node in the previous layer/time step, or the output of this node.}
\PY{l+s+sd}{    z\PYZus{}output (ndarray)}
\PY{l+s+sd}{        The weighted sum output of this node (the output before activation).}
\PY{l+s+sd}{    grad\PYZus{}\PYZob{}b/W/h\PYZcb{}\PYZus{}\PYZob{}layer/time\PYZcb{} (ndarray)}
\PY{l+s+sd}{        The gradient of the cost function with respect to the appropriate variable.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{n\PYZus{}features}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,}
            \PY{n}{act\PYZus{}func}\PY{p}{:} \PY{n}{Callable}\PY{p}{[}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]} \PY{o}{=} \PY{n}{identity}\PY{p}{,}
            \PY{n}{W\PYZus{}layer}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray} \PY{o}{=} \PY{k+kc}{None}\PY{p}{,}
            \PY{n}{b\PYZus{}layer}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray} \PY{o}{=} \PY{k+kc}{None}\PY{p}{,}
            \PY{n}{W\PYZus{}time}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray} \PY{o}{=} \PY{k+kc}{None}\PY{p}{,}
            \PY{n}{b\PYZus{}time}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray} \PY{o}{=} \PY{k+kc}{None}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Constructor for Node objects.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        n\PYZus{}features (int)}
\PY{l+s+sd}{            Number of features in this node (the length of the vector).}
\PY{l+s+sd}{        act\PYZus{}func (Callable)}
\PY{l+s+sd}{            The activation function of this node.}
\PY{l+s+sd}{        \PYZob{}b/W\PYZcb{}\PYZus{}\PYZob{}layer/time\PYZcb{} (ndarray)}
\PY{l+s+sd}{            Bias(b) or weight matrix (W) for computing the contribution to this node from the node in the previous layer/time step.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features} \PY{o}{=} \PY{n}{n\PYZus{}features}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{act\PYZus{}func} \PY{o}{=} \PY{n}{act\PYZus{}func}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer} \PY{o}{=} \PY{n}{W\PYZus{}layer}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer} \PY{o}{=} \PY{n}{b\PYZus{}layer}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}time} \PY{o}{=} \PY{n}{W\PYZus{}time}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}time} \PY{o}{=} \PY{n}{b\PYZus{}time}

        \PY{c+c1}{\PYZsh{}\PYZsh{} Values from feed\PYZus{}forward()}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{h\PYZus{}layer} \PY{o}{=} \PY{k+kc}{None} \PY{c+c1}{\PYZsh{} h from previous layer}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{h\PYZus{}time} \PY{o}{=} \PY{k+kc}{None} \PY{c+c1}{\PYZsh{} h from previous time step}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{z\PYZus{}output} \PY{o}{=} \PY{k+kc}{None} \PY{c+c1}{\PYZsh{} z for this node}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{h\PYZus{}output} \PY{o}{=} \PY{k+kc}{None} \PY{c+c1}{\PYZsh{} h from this node}

        \PY{c+c1}{\PYZsh{}\PYZsh{} Values from backpropagate()}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}b\PYZus{}layer} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}b\PYZus{}time} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}W\PYZus{}layer} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}W\PYZus{}time} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}h\PYZus{}layer} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}h\PYZus{}time} \PY{o}{=} \PY{k+kc}{None}
    
    \PY{k}{def} \PY{n+nf}{set\PYZus{}Wb}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{W\PYZus{}layer}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{,}
            \PY{n}{b\PYZus{}layer}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{,}
            \PY{n}{W\PYZus{}time}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray} \PY{o}{=} \PY{k+kc}{None}\PY{p}{,}
            \PY{n}{b\PYZus{}time}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray} \PY{o}{=} \PY{k+kc}{None}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Sets the weights and biases to specific values. Used by the layer classes to ensure all nodes have correct weights and biases.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        \PYZob{}b/W\PYZcb{}\PYZus{}\PYZob{}layer/time\PYZcb{} (ndarray)}
\PY{l+s+sd}{            Bias(b) or weight matrix (W) for computing the contribution to this node from the node in the previous layer/time step.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer} \PY{o}{=} \PY{n}{W\PYZus{}layer}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer} \PY{o}{=} \PY{n}{b\PYZus{}layer}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}time} \PY{o}{=} \PY{n}{W\PYZus{}time}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}time} \PY{o}{=} \PY{n}{b\PYZus{}time}

    \PY{k}{def} \PY{n+nf}{feed\PYZus{}forward}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{h\PYZus{}layer}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{,}
            \PY{n}{h\PYZus{}time}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray} \PY{o}{=} \PY{k+kc}{None}
    \PY{p}{)} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Computes the output of this node from the output of the nodes at the previous layer and time step, using the equations for forward propagation.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        h\PYZus{}\PYZob{}layer/time/output\PYZcb{} (ndarray)}
\PY{l+s+sd}{            The output of the node in the previous layer/time step.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Returns}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        h\PYZus{}output (ndarray)}
\PY{l+s+sd}{            The output of this node.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{} Save h\PYZus{}layer and h\PYZus{}time for use in backpropagation}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{h\PYZus{}layer} \PY{o}{=} \PY{n}{h\PYZus{}layer}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{h\PYZus{}time} \PY{o}{=} \PY{n}{h\PYZus{}time}

        \PY{n}{num\PYZus{}inputs} \PY{o}{=} \PY{n}{h\PYZus{}layer}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}

        \PY{c+c1}{\PYZsh{}\PYZsh{} Compute weighted sum z for this node.}
        \PY{n}{z\PYZus{}layer} \PY{o}{=} \PY{n}{h\PYZus{}layer} \PY{o}{@} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer} \PY{c+c1}{\PYZsh{} Dimension example: (100,5)@(5,7) + (7) = (100,7)}

        \PY{k}{if} \PY{n}{h\PYZus{}time} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} This node is at the first time step, thus not receiving any input from previous time steps.}
            \PY{n}{z\PYZus{}time} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{num\PYZus{}inputs}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{p}{)}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{z\PYZus{}time} \PY{o}{=} \PY{n}{h\PYZus{}time} \PY{o}{@} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}time} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}time}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{z\PYZus{}output} \PY{o}{=} \PY{n}{z\PYZus{}layer} \PY{o}{+} \PY{n}{z\PYZus{}time} \PY{c+c1}{\PYZsh{} Save the weighted sum in the node}

        \PY{c+c1}{\PYZsh{}\PYZsh{} Compute activation of the node}
        \PY{n}{h\PYZus{}output} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{act\PYZus{}func}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{z\PYZus{}output}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{h\PYZus{}output} \PY{o}{=} \PY{n}{h\PYZus{}output} \PY{c+c1}{\PYZsh{} Save the output in the node}
        \PY{k}{return} \PY{n}{h\PYZus{}output} \PY{c+c1}{\PYZsh{} Return output}
        

    \PY{k}{def} \PY{n+nf}{backpropagate}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{dC\PYZus{}layer}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray} \PY{o}{=} \PY{k+kc}{None}\PY{p}{,}
            \PY{n}{dC\PYZus{}time}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray} \PY{o}{=} \PY{k+kc}{None}\PY{p}{,}
            \PY{n}{lmbd}\PY{p}{:} \PY{n+nb}{float} \PY{o}{=} \PY{l+m+mf}{0.01}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Performs backpropagation through this node. Computes the gradient of the cost function with respect to the weights and biases of this layer (and stores these gradients so the weights and biases can be updated in the layer object this node belongs to), and the gradient with respect to the output of the nodes in the previous layer and time step (and stores these as well for further backpropagation through the network).}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        dC\PYZus{}\PYZob{}layer/time\PYZcb{} (ndarray)}
\PY{l+s+sd}{            Contribution of cost gradient w.r.t. this node from node at next layer/time.}
\PY{l+s+sd}{        lmbd (float)}
\PY{l+s+sd}{            Regularization parameter for finding the cost gradient with respect to the weights.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{n\PYZus{}batches} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{h\PYZus{}output}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{} Total gradient is the sum of the gradient from \PYZdq{}next\PYZdq{} layer and time}
        \PY{k}{if} \PY{n}{dC\PYZus{}time} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} If this is the last node in the layer, the gradient is just the gradient from the next layer}
            \PY{n}{dC} \PY{o}{=} \PY{n}{dC\PYZus{}layer}
        \PY{k}{elif} \PY{n}{dC\PYZus{}layer} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} If the next layer has no node at this time step (because it is a SingleOutputLayer), use only dC\PYZus{}time}
            \PY{n}{dC} \PY{o}{=} \PY{n}{dC\PYZus{}time}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{dC} \PY{o}{=} \PY{n}{dC\PYZus{}layer} \PY{o}{+} \PY{n}{dC\PYZus{}time}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{} delta (gradient of cost w.r.t. z)}
        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{act\PYZus{}func}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}name\PYZus{}\PYZus{}} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{softmax}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}
            \PY{n}{grad\PYZus{}act} \PY{o}{=} \PY{n}{grad\PYZus{}softmax}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{z\PYZus{}output}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{grad\PYZus{}act} \PY{o}{=} \PY{n}{vmap}\PY{p}{(}\PY{n}{vmap}\PY{p}{(}\PY{n}{derivate}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{act\PYZus{}func}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{z\PYZus{}output}\PY{p}{)} \PY{c+c1}{\PYZsh{} vmap is necessary for jax to vectorize gradient properly}
        
        \PY{n}{delta} \PY{o}{=} \PY{n}{grad\PYZus{}act} \PY{o}{*} \PY{n}{dC} \PY{c+c1}{\PYZsh{} Hadamard product, i.e., elementwise multiplication}

        \PY{c+c1}{\PYZsh{}\PYZsh{} Gradients w.r.t. bias}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}b\PYZus{}layer} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{delta}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)} \PY{o}{/} \PY{n}{n\PYZus{}batches}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}b\PYZus{}time} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{delta}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)} \PY{o}{/} \PY{n}{n\PYZus{}batches}

        \PY{c+c1}{\PYZsh{}\PYZsh{} Gradients w.r.t. weights}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}W\PYZus{}layer} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{h\PYZus{}layer}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{n}{delta} \PY{o}{/} \PY{n}{n\PYZus{}batches}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}W\PYZus{}layer} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}W\PYZus{}layer} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer} \PY{o}{*} \PY{n}{lmbd} \PY{c+c1}{\PYZsh{} Regularization factor}
        
        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{h\PYZus{}time} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}W\PYZus{}time} \PY{o}{=} \PY{k+kc}{None}
        \PY{k}{else}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}W\PYZus{}time} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{h\PYZus{}time}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{n}{delta} \PY{o}{/} \PY{n}{n\PYZus{}batches}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}W\PYZus{}time} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}W\PYZus{}time} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}time} \PY{o}{*} \PY{n}{lmbd} \PY{c+c1}{\PYZsh{} Regularization factor}

        \PY{c+c1}{\PYZsh{}\PYZsh{} Gradients w.r.t. input from previous nodes}
        \PY{c+c1}{\PYZsh{} Need to not transpose delta in order for matrices to match up correctly, since we have batches along rows, and features along columns}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}h\PYZus{}layer} \PY{o}{=} \PY{n}{delta} \PY{o}{@} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer}\PY{o}{.}\PY{n}{T}
        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{h\PYZus{}time} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}h\PYZus{}time} \PY{o}{=} \PY{k+kc}{None}
        \PY{k}{else}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{grad\PYZus{}h\PYZus{}time} \PY{o}{=} \PY{n}{delta} \PY{o}{@} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}time}\PY{o}{.}\PY{n}{T}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{The Layer class}\label{the-layer-class}

    The RNN consists of several layers of various types. The Layer class is
an abstract class keeping track of attributes and methods that are
common for the different types of layers. We define this class for
better organization of the different layers, but it should never be used
by itself, for instance by creating a Layer object. You will notice that
most of the Layer methods are only declared, not implemented. These
methods will vary between layer types, and are implemented in the
Layer's child classes, although the Layer class gives a nice overview of
which methods to expect from its children.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{\PYZus{}\PYZus{}future\PYZus{}\PYZus{}} \PY{k+kn}{import} \PY{n}{annotations} \PY{c+c1}{\PYZsh{} Necessary to create typing hint of Layer within the class Layer}

\PY{k}{class} \PY{n+nc}{Layer}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Abstract class for layers. The attributes given here are the attributes that are common to all layers, but they will also contain other attributes in addition to these.}

\PY{l+s+sd}{    Attributes}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    n\PYZus{}features (int)}
\PY{l+s+sd}{        Number of features in the nodes of this layer (the length of the vectors).}
\PY{l+s+sd}{    seed (int)}
\PY{l+s+sd}{        Seed for random number generating with numpy.}
\PY{l+s+sd}{    nodes (list)}
\PY{l+s+sd}{        List containing all the nodes of the layer in sequence, such that nodes[0] is the node at the first time step, nodes[1] at the second time step, and so on.}
\PY{l+s+sd}{    n\PYZus{}nodes (int)}
\PY{l+s+sd}{        Number of nodes in the layer. Is updated when adding or removing nodes.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{n\PYZus{}features}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,}
            \PY{n}{seed}\PY{p}{:} \PY{n+nb}{int} \PY{o}{=} \PY{l+m+mi}{100}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Constructor to be called from child classes.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        n\PYZus{}features (int)}
\PY{l+s+sd}{            Number of features in the nodes of this layer (the length of the vectors).}
\PY{l+s+sd}{        seed (int)}
\PY{l+s+sd}{            Seed for random number generating with numpy.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features} \PY{o}{=} \PY{n}{n\PYZus{}features}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed} \PY{o}{=} \PY{n}{seed}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes} \PY{o}{=} \PY{l+m+mi}{0}
    
    \PY{k}{def} \PY{n+nf}{reset\PYZus{}weights}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{k}{pass}
    
    \PY{k}{def} \PY{n+nf}{reset\PYZus{}schedulers}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{k}{pass}
    
    \PY{k}{def} \PY{n+nf}{update\PYZus{}weights\PYZus{}all\PYZus{}nodes}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{k}{raise} \PY{n+ne}{NotImplementedError}
    
    \PY{k}{def} \PY{n+nf}{add\PYZus{}node}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{k}{raise} \PY{n+ne}{NotImplementedError}
    
    \PY{k}{def} \PY{n+nf}{remove\PYZus{}nodes}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Remove all the nodes created for this layer.}

\PY{l+s+sd}{        NOTE}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        The weights and biases of the nodes are still stored in the layer, so we can easily}
\PY{l+s+sd}{        create new nodes. Removing the nodes is used to allow the sequence length to vary with}
\PY{l+s+sd}{        each call of feed\PYZus{}forward().}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes} \PY{o}{=} \PY{l+m+mi}{0}

    \PY{k}{def} \PY{n+nf}{feed\PYZus{}forward}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{prev\PYZus{}layer}\PY{p}{:} \PY{n}{Layer}
    \PY{p}{)}\PY{p}{:}
        \PY{k}{raise} \PY{n+ne}{NotImplementedError}
    
    \PY{k}{def} \PY{n+nf}{backpropagate}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{next\PYZus{}layer}\PY{p}{:} \PY{n}{Layer}\PY{p}{,}
            \PY{n}{lmbd}\PY{p}{:} \PY{n+nb}{float}
    \PY{p}{)}\PY{p}{:}
        \PY{k}{raise} \PY{n+ne}{NotImplementedError}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{The InputLayer class}\label{the-inputlayer-class}

    When we want to feed forward through an RNN, we will send a
3-dimensional array of shape (batch size, sequence length, number of
features) as input. We then want to feed forward through the first
hidden layer by computing the output of each node separately. Each node
should then get information from the node at the previous time step, and
the node at the previous layer. The way we implement forward propagation
in this notebook, we need the previous layer to contain one node for
each time step, and feed forward through these nodes. To make sure this
also works for the first hidden layer, we include the InputLayer class,
which takes the 3-dimensional input array to the network, and stores the
value at each time step as 2-dimensional arrays in separate nodes. For
all subsequent layers, we can then send in the previous layer as input
to the feed\_forward() method, work with each node separately.

Note that feed\_forward() will take a layer class as input for all other
layers, but since this is the first layer (with no preceding layer), it
will instead take in the 3-dimensional input array.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{InputLayer}\PY{p}{(}\PY{n}{Layer}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    The input layer of the RNN, used for storing the input to the RNN in nodes for easy forward propagation through subsequent layers.}
\PY{l+s+sd}{    This class does not contain any weights or biases, nor does it implement backpropagation as there are no parameters to update.}
\PY{l+s+sd}{    InputLayer is the only class that can function as the first layer in the RNN, and it should only be used for this purpose.}

\PY{l+s+sd}{    Attributes}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    n\PYZus{}features (int)}
\PY{l+s+sd}{        Number of features in the nodes of this layer (the length of the vectors).}
\PY{l+s+sd}{    seed (int)}
\PY{l+s+sd}{        Seed for random number generating with numpy.}
\PY{l+s+sd}{    nodes (list)}
\PY{l+s+sd}{        List containing all the nodes of the layer in sequence, such that nodes[0] is the node at the first time step, nodes[1] at the second time step, and so on.}
\PY{l+s+sd}{    n\PYZus{}nodes (int)}
\PY{l+s+sd}{        Number of nodes in the layer. Is updated when adding or removing nodes.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{n\PYZus{}features}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,}
            \PY{n}{seed}\PY{p}{:} \PY{n+nb}{int} \PY{o}{=} \PY{l+m+mi}{100}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Constructor for InputLayer objects.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        n\PYZus{}features (int)}
\PY{l+s+sd}{            Number of features in the nodes of this layer (the length of the vectors).}
\PY{l+s+sd}{        seed (int)}
\PY{l+s+sd}{            Seed for random number generating with numpy.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n}{n\PYZus{}features}\PY{p}{,} \PY{n}{seed}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{add\PYZus{}node}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Add a node. Weights and biases are set to None, and activation to identity by default,}
\PY{l+s+sd}{        as none of these are relevant for the input layer.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{new\PYZus{}node} \PY{o}{=} \PY{n}{Node}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{p}{)} \PY{c+c1}{\PYZsh{} Activation and weights are not used for the input layer}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{new\PYZus{}node}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}

    \PY{k}{def} \PY{n+nf}{feed\PYZus{}forward}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{X}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Feed forward through the layer. As this is the input layer, this amounts to adding nodes and setting the output of each node to the value of the input at the corresponding time step. Also checks that the number of features of the input is the same as the layer expects it to be.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        X (ndarray)}
\PY{l+s+sd}{            Input to the RNN, and thus to this layer. X has shape (batch size, sequence length, number of features)}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        NOTE}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        Unlike the other layers, this layer takes a numpy array as input instead of a Layer, since it is the first layer.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{X\PYZus{}shape} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}
        \PY{n}{sequence\PYZus{}length} \PY{o}{=} \PY{n}{X\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
        
        \PY{k}{if} \PY{o+ow}{not} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features} \PY{o}{==} \PY{n}{X\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Input must have the same number of features as defined by the layer}
            \PY{k}{raise} \PY{n+ne}{ValueError}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Expected the number of features in the input layer to be }\PY{l+s+si}{\PYZob{}}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, got }\PY{l+s+si}{\PYZob{}}\PY{n}{X\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Add a node to the layer for each time step, and set output}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{remove\PYZus{}nodes}\PY{p}{(}\PY{p}{)}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{sequence\PYZus{}length}\PY{p}{)}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{add\PYZus{}node}\PY{p}{(}\PY{p}{)}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{h\PYZus{}output} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{,}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{The RNNLayer class}\label{the-rnnlayer-class}

    The RNNLayer class represents the recurrent layers in the RNN. It takes
the value at each time step in the preceding layer and computes the
output from each node in its own layer, propagating forward through each
node one by one, taking into account both the connections between the
layers and the time steps. The math behind forward- and backpropagation
are already taken care of by the Node class, so the RNN class is
actually quite simple. In forward propagation, it simply goes through
each node, inserts the output from the nodes at the previous layer and
time step, and lets the nodes themselves do the rest, including storing
the results. In backpropagation it goes through each node in reversed
order, inserts the gradient of the cost function with respect to their
output, lets the nodes calculate the gradients with respect to weights
and biases, and then uses these to update the weights and biases of the
layer.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{copy} \PY{k+kn}{import} \PY{n}{copy}
 
\PY{k}{class} \PY{n+nc}{RNNLayer}\PY{p}{(}\PY{n}{Layer}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    The recurrent layer of the RNN. Computes forward\PYZhy{} and backpropagation through a recurrent layer by feeding forward, or backpropagating, through each}
\PY{l+s+sd}{    node of the layer one at a time in sequence.}

\PY{l+s+sd}{    Attributes}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    n\PYZus{}features (int)}
\PY{l+s+sd}{        Number of features in the nodes of this layer (the length of the vectors).}
\PY{l+s+sd}{    n\PYZus{}features\PYZus{}prev (int)}
\PY{l+s+sd}{        Number of features in the nodes of the preceding layer (the length of the vectors).}
\PY{l+s+sd}{    seed (int)}
\PY{l+s+sd}{        Seed for random number generating with numpy.}
\PY{l+s+sd}{    nodes (list)}
\PY{l+s+sd}{        List containing all the nodes of the layer in sequence, such that nodes[0] is the node at the first time step, nodes[1] at the second time step, and so on.}
\PY{l+s+sd}{    n\PYZus{}nodes (int)}
\PY{l+s+sd}{        Number of nodes in the layer. Is updated when adding or removing nodes.}
\PY{l+s+sd}{    act\PYZus{}func (Callable)}
\PY{l+s+sd}{        The activation function of this layer.}
\PY{l+s+sd}{    \PYZob{}b/W\PYZcb{}\PYZus{}\PYZob{}layer/time\PYZcb{} (ndarray)}
\PY{l+s+sd}{        Bias (b) or weight matrix (W) for computing the forward propagation from nodes at the previous layer to this one (layer), }
\PY{l+s+sd}{        or between time steps within this layer (time).}
\PY{l+s+sd}{    \PYZob{}b/W\PYZcb{}\PYZus{}\PYZob{}layer/time\PYZcb{}\PYZus{}size (tuple)}
\PY{l+s+sd}{        The shape of b\PYZus{}layer/W\PYZus{}layer/b\PYZus{}time/W\PYZus{}time.}
\PY{l+s+sd}{    scheduler\PYZus{}\PYZob{}b/W\PYZcb{}\PYZus{}\PYZob{}layer/time\PYZcb{} (Scheduler)}
\PY{l+s+sd}{        The scheduler for updating b\PYZus{}layer/W\PYZus{}layer/b\PYZus{}time/W\PYZus{}time with gradient descent when backpropagating through this layer.}
\PY{l+s+sd}{    is\PYZus{}dense (bool)}
\PY{l+s+sd}{        Tells if this is a DenseLayer or not. It is set to False for this layer, as it is not the DenseLayer.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{n\PYZus{}features}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,}
            \PY{n}{n\PYZus{}features\PYZus{}prev}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,}
            \PY{n}{act\PYZus{}func}\PY{p}{:} \PY{n}{Callable}\PY{p}{[}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}\PY{p}{,}
            \PY{n}{scheduler}\PY{p}{:} \PY{n}{Scheduler}\PY{p}{,}
            \PY{n}{seed}\PY{p}{:} \PY{n+nb}{int} \PY{o}{=} \PY{l+m+mi}{100}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Constructor for RNNLayer objects.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        n\PYZus{}features (int)}
\PY{l+s+sd}{            Number of features in the nodes of this layer (the length of the vectors).}
\PY{l+s+sd}{        n\PYZus{}features\PYZus{}prev (int)}
\PY{l+s+sd}{            Number of features in the nodes of the preceding layer (the length of the vectors).}
\PY{l+s+sd}{        act\PYZus{}func (Callable)}
\PY{l+s+sd}{            The activation function of this layer.}
\PY{l+s+sd}{        scheduler (Scheduler)}
\PY{l+s+sd}{            The scheduler to use for updating weights and biases with gradient descent when backpropagating through this layer.}
\PY{l+s+sd}{        seed (int)}
\PY{l+s+sd}{            Seed for random number generating with numpy.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n}{n\PYZus{}features}\PY{p}{,} \PY{n}{seed}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features\PYZus{}prev} \PY{o}{=} \PY{n}{n\PYZus{}features\PYZus{}prev}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{act\PYZus{}func} \PY{o}{=} \PY{n}{act\PYZus{}func}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}time} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}time} \PY{o}{=} \PY{k+kc}{None}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features\PYZus{}prev}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}time\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}time\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}W\PYZus{}layer} \PY{o}{=} \PY{n}{copy}\PY{p}{(}\PY{n}{scheduler}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}W\PYZus{}time} \PY{o}{=} \PY{n}{copy}\PY{p}{(}\PY{n}{scheduler}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}b\PYZus{}layer} \PY{o}{=} \PY{n}{copy}\PY{p}{(}\PY{n}{scheduler}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}b\PYZus{}time} \PY{o}{=} \PY{n}{copy}\PY{p}{(}\PY{n}{scheduler}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{is\PYZus{}dense} \PY{o}{=} \PY{k+kc}{False}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reset\PYZus{}weights}\PY{p}{(}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{reset\PYZus{}weights}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Reset weights and biases to random values from a normal distribution.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer\PYZus{}size}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer\PYZus{}size}\PY{p}{)} \PY{o}{*} \PY{l+m+mf}{0.01}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}time} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}time\PYZus{}size}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}time} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}time\PYZus{}size}\PY{p}{)} \PY{o}{*} \PY{l+m+mf}{0.01}
    
    \PY{k}{def} \PY{n+nf}{reset\PYZus{}schedulers}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Reset the schedulers of the layer.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}W\PYZus{}layer}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}b\PYZus{}layer}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}W\PYZus{}time}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}b\PYZus{}time}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{update\PYZus{}weights\PYZus{}all\PYZus{}nodes}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Update the weights and biases in all nodes of the layer.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{new\PYZus{}W\PYZus{}layer} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer}
        \PY{n}{new\PYZus{}W\PYZus{}time} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}time}
        \PY{n}{new\PYZus{}b\PYZus{}layer} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer}
        \PY{n}{new\PYZus{}b\PYZus{}time} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}time}
        \PY{k}{for} \PY{n}{node} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{p}{:}
            \PY{n}{node}\PY{o}{.}\PY{n}{set\PYZus{}Wb}\PY{p}{(}\PY{n}{new\PYZus{}W\PYZus{}layer}\PY{p}{,} \PY{n}{new\PYZus{}b\PYZus{}layer}\PY{p}{,} \PY{n}{new\PYZus{}W\PYZus{}time}\PY{p}{,} \PY{n}{new\PYZus{}b\PYZus{}time}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{add\PYZus{}node}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Add a node with the weights and biases specified by the layer.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{new\PYZus{}node} \PY{o}{=} \PY{n}{Node}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{act\PYZus{}func}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}time}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}time}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{new\PYZus{}node}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}

    \PY{k}{def} \PY{n+nf}{feed\PYZus{}forward}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{prev\PYZus{}layer}\PY{p}{:} \PY{n}{Layer}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Feed forward through this layer one node at a time. The results are stored in the nodes.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        prev\PYZus{}layer (Layer)}
\PY{l+s+sd}{            The preceding layer of the RNN.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{remove\PYZus{}nodes}\PY{p}{(}\PY{p}{)}
        \PY{n}{n\PYZus{}nodes\PYZus{}prev} \PY{o}{=} \PY{n}{prev\PYZus{}layer}\PY{o}{.}\PY{n}{n\PYZus{}nodes}

        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}nodes\PYZus{}prev}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Get output of node from previous layer}
            \PY{n}{prev\PYZus{}layer\PYZus{}node} \PY{o}{=} \PY{n}{prev\PYZus{}layer}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{n}{i}\PY{p}{]}
            \PY{n}{h\PYZus{}layer} \PY{o}{=} \PY{n}{prev\PYZus{}layer\PYZus{}node}\PY{o}{.}\PY{n}{h\PYZus{}output}
            
            \PY{c+c1}{\PYZsh{} Get output of node from previous time step}
            \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} No previous node if this is the first time step}
                \PY{n}{h\PYZus{}time} \PY{o}{=} \PY{k+kc}{None}
            \PY{k}{else}\PY{p}{:}
                \PY{n}{prev\PYZus{}time\PYZus{}node} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
                \PY{n}{h\PYZus{}time} \PY{o}{=} \PY{n}{prev\PYZus{}time\PYZus{}node}\PY{o}{.}\PY{n}{h\PYZus{}output}
            
            \PY{c+c1}{\PYZsh{} Create and compute new node at this time step}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{add\PYZus{}node}\PY{p}{(}\PY{p}{)}
            \PY{n}{new\PYZus{}node} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{n}{i}\PY{p}{]}
            \PY{n}{new\PYZus{}node}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{h\PYZus{}layer}\PY{p}{,} \PY{n}{h\PYZus{}time}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{backpropagate}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{next\PYZus{}layer}\PY{p}{:} \PY{n}{Layer}\PY{p}{,}
            \PY{n}{lmbd}\PY{p}{:} \PY{n+nb}{float} \PY{o}{=} \PY{l+m+mf}{0.01}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Backpropagate through the layer one node at a time. The results are stored in the nodes.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        next\PYZus{}layer (Layer)}
\PY{l+s+sd}{            The subsequent layer of the RNN.}
\PY{l+s+sd}{        lmbd (float)}
\PY{l+s+sd}{            Regularization parameter for finding the cost gradient with respect to the weights.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{} Check if the next layer is a DenseLayer}
        \PY{n}{next\PYZus{}is\PYZus{}dense} \PY{o}{=} \PY{n}{next\PYZus{}layer}\PY{o}{.}\PY{n}{is\PYZus{}dense}

        \PY{c+c1}{\PYZsh{}\PYZsh{} Go through all nodes, starting with the last}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{}\PYZsh{} Gradient from node at next layer}
            \PY{k}{if} \PY{n}{next\PYZus{}is\PYZus{}dense}\PY{p}{:}
                \PY{c+c1}{\PYZsh{}\PYZsh{} If next layer is DenseLayer, consider only grad\PYZus{}h\PYZus{}layer in the last node in this layer}
                \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                    \PY{n}{node\PYZus{}layer} \PY{o}{=} \PY{n}{next\PYZus{}layer}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                    \PY{n}{dC\PYZus{}layer} \PY{o}{=} \PY{n}{node\PYZus{}layer}\PY{o}{.}\PY{n}{grad\PYZus{}h\PYZus{}layer}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{dC\PYZus{}layer} \PY{o}{=} \PY{k+kc}{None}
            \PY{k}{else}\PY{p}{:}
                \PY{c+c1}{\PYZsh{}\PYZsh{} If next layer is not DenseLayer, get gradient from all nodes}
                \PY{n}{node\PYZus{}layer} \PY{o}{=} \PY{n}{next\PYZus{}layer}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                \PY{n}{dC\PYZus{}layer} \PY{o}{=} \PY{n}{node\PYZus{}layer}\PY{o}{.}\PY{n}{grad\PYZus{}h\PYZus{}layer}

            \PY{c+c1}{\PYZsh{}\PYZsh{} Gradient from node at next time step (unless this is the last node)}
            \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                \PY{n}{dC\PYZus{}time} \PY{o}{=} \PY{k+kc}{None}
            \PY{k}{else}\PY{p}{:}
                \PY{n}{node\PYZus{}time} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}
                \PY{n}{dC\PYZus{}time} \PY{o}{=} \PY{n}{node\PYZus{}time}\PY{o}{.}\PY{n}{grad\PYZus{}h\PYZus{}time}
            
            \PY{c+c1}{\PYZsh{}\PYZsh{} Backpropagate through this node. Results are stored in the nodes}
            \PY{n}{node} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{n}{i}\PY{p}{]}
            \PY{n}{node}\PY{o}{.}\PY{n}{backpropagate}\PY{p}{(}\PY{n}{dC\PYZus{}layer}\PY{p}{,} \PY{n}{dC\PYZus{}time}\PY{p}{,} \PY{n}{lmbd}\PY{p}{)}

            \PY{c+c1}{\PYZsh{}\PYZsh{} Update weights and biases}
            \PY{n}{grad\PYZus{}W\PYZus{}layer} \PY{o}{=} \PY{n}{node}\PY{o}{.}\PY{n}{grad\PYZus{}W\PYZus{}layer} \PY{o}{/} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes}
            \PY{n}{grad\PYZus{}W\PYZus{}time} \PY{o}{=} \PY{n}{node}\PY{o}{.}\PY{n}{grad\PYZus{}W\PYZus{}time}
            \PY{n}{grad\PYZus{}b\PYZus{}layer} \PY{o}{=} \PY{n}{node}\PY{o}{.}\PY{n}{grad\PYZus{}b\PYZus{}layer} \PY{o}{/} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes}
            \PY{n}{grad\PYZus{}b\PYZus{}time} \PY{o}{=} \PY{n}{node}\PY{o}{.}\PY{n}{grad\PYZus{}b\PYZus{}time}

            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}W\PYZus{}layer}\PY{o}{.}\PY{n}{update\PYZus{}change}\PY{p}{(}\PY{n}{grad\PYZus{}W\PYZus{}layer}\PY{p}{)}
            \PY{k}{if} \PY{n}{grad\PYZus{}W\PYZus{}time} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                \PY{n}{grad\PYZus{}W\PYZus{}time} \PY{o}{=} \PY{n}{grad\PYZus{}W\PYZus{}time} \PY{o}{/} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}time} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}W\PYZus{}time}\PY{o}{.}\PY{n}{update\PYZus{}change}\PY{p}{(}\PY{n}{grad\PYZus{}W\PYZus{}time}\PY{p}{)}
            
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}b\PYZus{}layer}\PY{o}{.}\PY{n}{update\PYZus{}change}\PY{p}{(}\PY{n}{grad\PYZus{}b\PYZus{}layer}\PY{p}{)}
            \PY{k}{if} \PY{n}{grad\PYZus{}b\PYZus{}time} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                \PY{n}{grad\PYZus{}b\PYZus{}time} \PY{o}{=} \PY{n}{grad\PYZus{}b\PYZus{}time} \PY{o}{/} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}time} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}b\PYZus{}time}\PY{o}{.}\PY{n}{update\PYZus{}change}\PY{p}{(}\PY{n}{grad\PYZus{}b\PYZus{}time}\PY{p}{)}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{update\PYZus{}weights\PYZus{}all\PYZus{}nodes}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{The OutputLayer class}\label{the-outputlayer-class}

    The OutputLayer class gives the output of the RNN, with a value at each
time step. It is very similar to the RNNLayer class, as they essentially
do the same thing, but it only connects nodes at the previous layer to
the corresponding output nodes, without any connections between ndoes at
different time steps. Note also that, in the same way as
\emph{feed\_forward()} in the InputLayer took a numpy array as input
instead of a Layer object, the OutputLayer takes a numpy array as input
to the \emph{backpropagation()} method, since there is no subsequent
layer to this one.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{OutputLayer}\PY{p}{(}\PY{n}{Layer}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    The output layer of the RNN, computing an output for each time step of the RNN. Very similar to RNNLayer, but has no connections between the nodes, only from the previous.}

\PY{l+s+sd}{    Attributes}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    n\PYZus{}features (int)}
\PY{l+s+sd}{        Number of features in the nodes of this layer (the length of the vectors).}
\PY{l+s+sd}{    n\PYZus{}features\PYZus{}prev (int)}
\PY{l+s+sd}{        Number of features in the nodes of the preceding layer (the length of the vectors).}
\PY{l+s+sd}{    seed (int)}
\PY{l+s+sd}{        Seed for random number generating with numpy.}
\PY{l+s+sd}{    nodes (list)}
\PY{l+s+sd}{        List containing all the nodes of the layer in sequence, such that nodes[0] is the node at the first time step, nodes[1] at the second time step, and so on.}
\PY{l+s+sd}{    n\PYZus{}nodes (int)}
\PY{l+s+sd}{        Number of nodes in the layer. Is updated when adding or removing nodes.}
\PY{l+s+sd}{    act\PYZus{}func (Callable)}
\PY{l+s+sd}{        The activation function of this layer.}
\PY{l+s+sd}{    \PYZob{}b/W\PYZcb{}\PYZus{}layer (ndarray)}
\PY{l+s+sd}{        Bias (b) or weight matrix (W) for computing the forward propagation from nodes at the previous layer to this one (layer).}
\PY{l+s+sd}{    \PYZob{}b/W\PYZcb{}\PYZus{}layer\PYZus{}size (tuple)}
\PY{l+s+sd}{        The shape of b\PYZus{}layer/W\PYZus{}layer.}
\PY{l+s+sd}{    scheduler\PYZus{}\PYZob{}b/W\PYZcb{}\PYZus{}layer (Scheduler)}
\PY{l+s+sd}{        The scheduler for updating b\PYZus{}layer/W\PYZus{}layer with gradient descent when backpropagating through this layer.}
\PY{l+s+sd}{    is\PYZus{}dense (bool)}
\PY{l+s+sd}{        Tells if this is a DenseLayer or not. It is set to False for this layer, as this is not a DenseLayer.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{n\PYZus{}features}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,}
            \PY{n}{n\PYZus{}features\PYZus{}prev}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,}
            \PY{n}{act\PYZus{}func}\PY{p}{:} \PY{n}{Callable}\PY{p}{[}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}\PY{p}{,}
            \PY{n}{scheduler}\PY{p}{:} \PY{n}{Scheduler}\PY{p}{,}
            \PY{n}{seed}\PY{p}{:} \PY{n+nb}{int} \PY{o}{=} \PY{l+m+mi}{100}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Constructor for OutputLayer objects.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        n\PYZus{}features (int)}
\PY{l+s+sd}{            Number of features in the nodes of this layer (the length of the vectors).}
\PY{l+s+sd}{        n\PYZus{}features\PYZus{}prev (int)}
\PY{l+s+sd}{            Number of features in the nodes of the preceding layer (the length of the vectors).}
\PY{l+s+sd}{        act\PYZus{}func (Callable)}
\PY{l+s+sd}{            The activation function of this layer.}
\PY{l+s+sd}{        scheduler (Scheduler)}
\PY{l+s+sd}{            The scheduler to use for updating weights and biases with gradient descent when backpropagating through this layer.}
\PY{l+s+sd}{        seed (int)}
\PY{l+s+sd}{            Seed for random number generating with numpy.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n}{n\PYZus{}features}\PY{p}{,} \PY{n}{seed}\PY{p}{)}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features\PYZus{}prev} \PY{o}{=} \PY{n}{n\PYZus{}features\PYZus{}prev}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{act\PYZus{}func} \PY{o}{=} \PY{n}{act\PYZus{}func}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer} \PY{o}{=} \PY{k+kc}{None}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features\PYZus{}prev}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}W\PYZus{}layer} \PY{o}{=} \PY{n}{copy}\PY{p}{(}\PY{n}{scheduler}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}b\PYZus{}layer} \PY{o}{=} \PY{n}{copy}\PY{p}{(}\PY{n}{scheduler}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{is\PYZus{}dense} \PY{o}{=} \PY{k+kc}{False}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reset\PYZus{}weights}\PY{p}{(}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{reset\PYZus{}weights}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Reset weights and biases to random values from a normal distribution.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer\PYZus{}size}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer\PYZus{}size}\PY{p}{)} \PY{o}{*} \PY{l+m+mf}{0.01}
    
    \PY{k}{def} \PY{n+nf}{reset\PYZus{}schedulers}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Reset the schedulers of the layer.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}W\PYZus{}layer}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}b\PYZus{}layer}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{update\PYZus{}weights\PYZus{}all\PYZus{}nodes}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Update the weights and biases in all nodes of the layer.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{new\PYZus{}W\PYZus{}layer} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer}
        \PY{n}{new\PYZus{}b\PYZus{}layer} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer}
        \PY{k}{for} \PY{n}{node} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{p}{:}
            \PY{n}{node}\PY{o}{.}\PY{n}{set\PYZus{}Wb}\PY{p}{(}\PY{n}{W\PYZus{}layer}\PY{o}{=}\PY{n}{new\PYZus{}W\PYZus{}layer}\PY{p}{,} \PY{n}{b\PYZus{}layer}\PY{o}{=}\PY{n}{new\PYZus{}b\PYZus{}layer}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{add\PYZus{}node}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Add a node with the weights and biases specified by the layer.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{new\PYZus{}node} \PY{o}{=} \PY{n}{Node}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{act\PYZus{}func}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{new\PYZus{}node}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}

    \PY{k}{def} \PY{n+nf}{feed\PYZus{}forward}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{prev\PYZus{}layer}\PY{p}{:} \PY{n}{Layer}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Feed forward through this layer one node at a time. The results are stored in the nodes.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        prev\PYZus{}layer (Layer)}
\PY{l+s+sd}{            The preceding layer of the RNN.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{remove\PYZus{}nodes}\PY{p}{(}\PY{p}{)}
        \PY{n}{n\PYZus{}nodes\PYZus{}prev} \PY{o}{=} \PY{n}{prev\PYZus{}layer}\PY{o}{.}\PY{n}{n\PYZus{}nodes}

        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}nodes\PYZus{}prev}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Get output of node from previous layer}
            \PY{n}{prev\PYZus{}layer\PYZus{}node} \PY{o}{=} \PY{n}{prev\PYZus{}layer}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{n}{i}\PY{p}{]}
            \PY{n}{h\PYZus{}layer} \PY{o}{=} \PY{n}{prev\PYZus{}layer\PYZus{}node}\PY{o}{.}\PY{n}{h\PYZus{}output}
            
            \PY{c+c1}{\PYZsh{} Create and compute new node at this time step}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{add\PYZus{}node}\PY{p}{(}\PY{p}{)}
            \PY{n}{new\PYZus{}node} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{n}{i}\PY{p}{]}

            \PY{c+c1}{\PYZsh{} No info transfer between time steps in output layer}
            \PY{n}{new\PYZus{}node}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{h\PYZus{}layer}\PY{p}{,} \PY{k+kc}{None}\PY{p}{)}
            
    
    \PY{k}{def} \PY{n+nf}{backpropagate}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{dC}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{,}
            \PY{n}{lmbd}\PY{p}{:} \PY{n+nb}{float} \PY{o}{=} \PY{l+m+mf}{0.01}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Backpropagate through the layer one node at a time. The results are stored in the nodes.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        dC (ndarray)}
\PY{l+s+sd}{            Gradient of the cost function with respect to the output of the RNN. It has shape (batch size, sequence length, number of features).}
\PY{l+s+sd}{        lmbd (float)}
\PY{l+s+sd}{            Regularization parameter for finding the cost gradient with respect to the weights.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        NOTE}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        Unlike the other layers, this layer takes a numpy array as input instead of a Layer, since it is the last layer.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{} Go through all nodes}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes}\PY{p}{)}\PY{p}{:}            
            \PY{c+c1}{\PYZsh{}\PYZsh{} Backpropagate through this node. Results are stored in the nodes}
            \PY{n}{node} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{n}{i}\PY{p}{]}
            \PY{n}{dC\PYZus{}layer} \PY{o}{=} \PY{n}{dC}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{,}\PY{p}{:}\PY{p}{]} \PY{c+c1}{\PYZsh{} Treat dC as coming from a subsequent layer}
            \PY{n}{node}\PY{o}{.}\PY{n}{backpropagate}\PY{p}{(}\PY{n}{dC\PYZus{}layer}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{n}{lmbd}\PY{p}{)} \PY{c+c1}{\PYZsh{} No time gradient in the output layer}

            \PY{c+c1}{\PYZsh{}\PYZsh{} Update weights and biases (no time gradient in output layer)}
            \PY{n}{grad\PYZus{}W\PYZus{}layer} \PY{o}{=} \PY{n}{node}\PY{o}{.}\PY{n}{grad\PYZus{}W\PYZus{}layer}
            \PY{n}{grad\PYZus{}b\PYZus{}layer} \PY{o}{=} \PY{n}{node}\PY{o}{.}\PY{n}{grad\PYZus{}b\PYZus{}layer}

            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}W\PYZus{}layer}\PY{o}{.}\PY{n}{update\PYZus{}change}\PY{p}{(}\PY{n}{grad\PYZus{}W\PYZus{}layer}\PY{p}{)}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}b\PYZus{}layer}\PY{o}{.}\PY{n}{update\PYZus{}change}\PY{p}{(}\PY{n}{grad\PYZus{}b\PYZus{}layer}\PY{p}{)}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{update\PYZus{}weights\PYZus{}all\PYZus{}nodes}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{The DenseLayer class}\label{the-denselayer-class}

    So far we have created the layers of a recurrent neural network with
sequential input and sequential output, i.e., a network looking
something like the following diagram.

    

    This works well for situations where we want to predict some value at
each input, such as solving differential equations. Other times,
however, we want a single output for the entire input sequence, such as
for sentiment analysis, where we want to determine if a sentence has a
positive, negative or neutral tone. For such cases we still want to use
an RNN with sequential input, but we don't want a sequential output. One
way to accomplish this is to use a dense (fully connected) layer at the
end of the network. We then take the last recurrent layer in the
network, and add a connection from the last node to the dense layer.
This would look like the following diagram.

    

    Note here that we use a single node for the dense layer. This is because
we have defined the nodes in this code to contain a single time step,
but be vectors that represent the different features. Thus the
connection from one node to another is really a dense connection between
two ``layers'' (in the regular fully connected neural network sense). To
convince yourself that the dense layer represented by a single node in
the diagram is truly a dense \emph{layer}, remember the following
picture.

    

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{DenseLayer}\PY{p}{(}\PY{n}{Layer}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    A dense layer used to go from a recurrent to a fully connected neural network. Contains a single node to feed forward and backpropagate through.}

\PY{l+s+sd}{    Attributes}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    n\PYZus{}features (int)}
\PY{l+s+sd}{        Number of features in the node of this layer (the length of the vector).}
\PY{l+s+sd}{    n\PYZus{}features\PYZus{}prev (int)}
\PY{l+s+sd}{        Number of features in the nodes of the preceding layer (the length of the vectors).}
\PY{l+s+sd}{    seed (int)}
\PY{l+s+sd}{        Seed for random number generating with numpy.}
\PY{l+s+sd}{    nodes (list)}
\PY{l+s+sd}{        List containing all the nodes of the layer in sequence, such that nodes[0] is the node at the first time step, nodes[1] at the second time step, and so on.}
\PY{l+s+sd}{    n\PYZus{}nodes (int)}
\PY{l+s+sd}{        Number of nodes in the layer. Is updated when adding or removing nodes.}
\PY{l+s+sd}{    act\PYZus{}func (Callable)}
\PY{l+s+sd}{        The activation function of this layer.}
\PY{l+s+sd}{    \PYZob{}b/W\PYZcb{}\PYZus{}layer (ndarray)}
\PY{l+s+sd}{        Bias (b) or weight matrix (W) for computing the forward propagation from nodes at the previous layer to this one (layer).}
\PY{l+s+sd}{    \PYZob{}b/W\PYZcb{}\PYZus{}layer\PYZus{}size (tuple)}
\PY{l+s+sd}{        The shape of b\PYZus{}layer/W\PYZus{}layer.}
\PY{l+s+sd}{    scheduler\PYZus{}\PYZob{}b/W\PYZcb{}\PYZus{}layer (Scheduler)}
\PY{l+s+sd}{        The scheduler for updating b\PYZus{}layer/W\PYZus{}layer with gradient descent when backpropagating through this layer.}
\PY{l+s+sd}{    is\PYZus{}dense (bool)}
\PY{l+s+sd}{        Tells if this is a DenseLayer or not. It is set to True for this layer, as this is a DenseLayer.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{n\PYZus{}features}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,}
            \PY{n}{n\PYZus{}features\PYZus{}prev}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,}
            \PY{n}{act\PYZus{}func}\PY{p}{:} \PY{n}{Callable}\PY{p}{[}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}\PY{p}{,}
            \PY{n}{scheduler}\PY{p}{:} \PY{n}{Scheduler}\PY{p}{,}
            \PY{n}{seed}\PY{p}{:} \PY{n+nb}{int} \PY{o}{=} \PY{l+m+mi}{100}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Constructor for OutputLayer objects.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        n\PYZus{}features (int)}
\PY{l+s+sd}{            Number of features in the node of this layer (the length of the vector).}
\PY{l+s+sd}{        n\PYZus{}features\PYZus{}prev (int)}
\PY{l+s+sd}{            Number of features in the nodes of the preceding layer (the length of the vectors).}
\PY{l+s+sd}{        act\PYZus{}func (Callable)}
\PY{l+s+sd}{            The activation function of this layer.}
\PY{l+s+sd}{        scheduler (Scheduler)}
\PY{l+s+sd}{            The scheduler to use for updating weights and biases with gradient descent when backpropagating through this layer.}
\PY{l+s+sd}{        seed (int)}
\PY{l+s+sd}{            Seed for random number generating with numpy.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n}{n\PYZus{}features}\PY{p}{,} \PY{n}{seed}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features\PYZus{}prev} \PY{o}{=} \PY{n}{n\PYZus{}features\PYZus{}prev}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{act\PYZus{}func} \PY{o}{=} \PY{n}{act\PYZus{}func}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes} \PY{o}{=} \PY{l+m+mi}{0}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer} \PY{o}{=} \PY{k+kc}{None}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features\PYZus{}prev}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}W\PYZus{}layer} \PY{o}{=} \PY{n}{copy}\PY{p}{(}\PY{n}{scheduler}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}b\PYZus{}layer} \PY{o}{=} \PY{n}{copy}\PY{p}{(}\PY{n}{scheduler}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{is\PYZus{}dense} \PY{o}{=} \PY{k+kc}{True}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reset\PYZus{}weights}\PY{p}{(}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{reset\PYZus{}weights}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Reset weights and biases to random values from a normal distribution.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer\PYZus{}size}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer\PYZus{}size}\PY{p}{)} \PY{o}{*} \PY{l+m+mf}{0.01}
    
    \PY{k}{def} \PY{n+nf}{reset\PYZus{}schedulers}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Reset the schedulers of the layer.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}W\PYZus{}layer}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}b\PYZus{}layer}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{update\PYZus{}weights\PYZus{}all\PYZus{}nodes}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Update the weights and biases in the node.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}Wb}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{add\PYZus{}node}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Add a node with the weights and biases specified by the layer.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{new\PYZus{}node} \PY{o}{=} \PY{n}{Node}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{act\PYZus{}func}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{new\PYZus{}node}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}nodes} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}

    \PY{k}{def} \PY{n+nf}{feed\PYZus{}forward}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{prev\PYZus{}layer}\PY{p}{:} \PY{n}{Layer}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Feed forward through this layer. The result is stored in the node.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        prev\PYZus{}layer (Layer)}
\PY{l+s+sd}{            The preceding layer of the RNN.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{} Get output from last node of previous layer}
        \PY{n}{prev\PYZus{}node} \PY{o}{=} \PY{n}{prev\PYZus{}layer}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{h\PYZus{}layer} \PY{o}{=} \PY{n}{prev\PYZus{}node}\PY{o}{.}\PY{n}{h\PYZus{}output}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{remove\PYZus{}nodes}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{add\PYZus{}node}\PY{p}{(}\PY{p}{)}
        \PY{n}{new\PYZus{}node} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{output} \PY{o}{=} \PY{n}{new\PYZus{}node}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{h\PYZus{}layer}\PY{p}{)}

        \PY{k}{return} \PY{n}{output}
            
    
    \PY{k}{def} \PY{n+nf}{backpropagate}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{next\PYZus{}layer\PYZus{}or\PYZus{}dC}\PY{p}{:} \PY{n}{Layer} \PY{o}{|} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{,}
            \PY{n}{lmbd}\PY{p}{:} \PY{n+nb}{float} \PY{o}{=} \PY{l+m+mf}{0.01}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Backpropagate through the layer. The results are stored in the node.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        next\PYZus{}layer\PYZus{}or\PYZus{}dC (Layer | ndarray)}
\PY{l+s+sd}{            This variable should be one of the two, depending on the type of input, whether it is a Layer object or a numpy array:}
\PY{l+s+sd}{            \PYZhy{} Layer: The subsequent layer of the network. Used if this is not the last layer in the network.}
\PY{l+s+sd}{            \PYZhy{} Numpy array: The gradient of the cost function with respect to the output of the network.}
\PY{l+s+sd}{              It has shape (batch size, number of features). Used if this is the last layer (output) of the network.}
\PY{l+s+sd}{        lmbd (float)}
\PY{l+s+sd}{            Regularization parameter for finding the cost gradient with respect to the weights.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{} Get dC from next\PYZus{}layer\PYZus{}or\PYZus{}dC.}
        \PY{k}{if} \PY{n+nb}{isinstance}\PY{p}{(}\PY{n}{next\PYZus{}layer\PYZus{}or\PYZus{}dC}\PY{p}{,} \PY{n}{Layer}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} If next\PYZus{}layer\PYZus{}or\PYZus{}dC is a Layer, extract dC from last node.}
            \PY{n}{dC} \PY{o}{=} \PY{n}{next\PYZus{}layer\PYZus{}or\PYZus{}dC}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{grad\PYZus{}h\PYZus{}layer}
        \PY{k}{else}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} If next\PYZus{}layer\PYZus{}or\PYZus{}dC is not a layer, it is the cost gradient.}
            \PY{n}{dC} \PY{o}{=} \PY{n}{next\PYZus{}layer\PYZus{}or\PYZus{}dC}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{} Backpropagate through the node}
        \PY{n}{node} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{node}\PY{o}{.}\PY{n}{backpropagate}\PY{p}{(}\PY{n}{dC\PYZus{}layer}\PY{o}{=}\PY{n}{dC}\PY{p}{,} \PY{n}{dC\PYZus{}time}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{lmbd}\PY{o}{=}\PY{n}{lmbd}\PY{p}{)}

        \PY{c+c1}{\PYZsh{}\PYZsh{} Update weights and biases}
        \PY{n}{grad\PYZus{}W\PYZus{}layer} \PY{o}{=} \PY{n}{node}\PY{o}{.}\PY{n}{grad\PYZus{}W\PYZus{}layer}
        \PY{n}{grad\PYZus{}b\PYZus{}layer} \PY{o}{=} \PY{n}{node}\PY{o}{.}\PY{n}{grad\PYZus{}b\PYZus{}layer}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W\PYZus{}layer} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}W\PYZus{}layer}\PY{o}{.}\PY{n}{update\PYZus{}change}\PY{p}{(}\PY{n}{grad\PYZus{}W\PYZus{}layer}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}layer} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler\PYZus{}b\PYZus{}layer}\PY{o}{.}\PY{n}{update\PYZus{}change}\PY{p}{(}\PY{n}{grad\PYZus{}b\PYZus{}layer}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{update\PYZus{}weights\PYZus{}all\PYZus{}nodes}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{The RNN class}\label{the-rnn-class}

    Now that we have created all the different layer classes we need for the
RNN, we can create the network itself. The RNN class is used to organize
the layers in order to train and use the RNN.

Note that the class has two options for predicting results from data.
The method \emph{feed\_forward()} propagates the input forward and
returns the output in the same way as we have looked at so far. The
\emph{predict()} method starts by running \emph{feed\_forward()}, then
uses argmax to set one element in the output to \(1\) and the rest to
\(0\). In other words, \emph{feed\_forward()} is used for regression
problems, while \emph{predict()} is used for classification.

In addition to this, we also have a method named \emph{extrapolate()},
which can be used after having run \emph{feed\_forward()}. This method
takes the output at the last time step, and uses it as input for new
time steps, extending the sequence. This is shown in the following
diagram.

    

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k+kn}{import} \PY{n}{resample}

\PY{k}{class} \PY{n+nc}{RNN}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    The recurrent neural network. Builds the network by adding layers, trains it on data and predicts output of new data.}

\PY{l+s+sd}{    Attributes}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    layers (list)}
\PY{l+s+sd}{        List containing all the layers of the RNN in sequence, such that layers[0] is the first layer, nodes[1] is the second layer, and so on.}
\PY{l+s+sd}{    n\PYZus{}layers (int)}
\PY{l+s+sd}{        Number of layers in the RNN. Is updated when adding layers.}
\PY{l+s+sd}{    cost\PYZus{}func (Callable)}
\PY{l+s+sd}{        Function which takes in the target array and returns a new function. The new function takes in the output from the network,}
\PY{l+s+sd}{        and returns the cost of that output when compared to the target.}
\PY{l+s+sd}{    scheduler (Scheduler)}
\PY{l+s+sd}{        The scheduler to use for updating weights and biases with gradient descent when backpropagating through the network.}
\PY{l+s+sd}{    seed (int)}
\PY{l+s+sd}{        Seed for random number generating with numpy.}
\PY{l+s+sd}{    single\PYZus{}output (bool)}
\PY{l+s+sd}{        False if the network has a sequential output (using OutputLayer), and True if the network has a single output (using DenseLayer).}
\PY{l+s+sd}{    n\PYZus{}features\PYZus{}output (int)}
\PY{l+s+sd}{        Number of features in the output layer.}
\PY{l+s+sd}{    output (ndarray)}
\PY{l+s+sd}{        The output from the network after computing forward propagation.}
\PY{l+s+sd}{    predicted (ndarray)}
\PY{l+s+sd}{        The prediction from the network on classification problems.}
\PY{l+s+sd}{    output\PYZus{}extra (ndarray)}
\PY{l+s+sd}{        Output from extrapolating the time sequence beyond the length of the input.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{cost\PYZus{}func}\PY{p}{:} \PY{n}{Callable}\PY{p}{[}
                \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} Takes in the target output (array)}
                \PY{n}{Callable}\PY{p}{[}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]} \PY{c+c1}{\PYZsh{} Returns a function (the cost function)}
            \PY{p}{]}\PY{p}{,}
            \PY{n}{scheduler}\PY{p}{:} \PY{n}{Scheduler}\PY{p}{,}
            \PY{n}{seed}\PY{p}{:} \PY{n+nb}{int} \PY{o}{=} \PY{l+m+mi}{100}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Constructor for RNN objects.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        cost\PYZus{}func (Callable)}
\PY{l+s+sd}{            Function which takes in the target array and returns a new function. The new function takes in the output from the network,}
\PY{l+s+sd}{            and returns the cost of that output when compared to the target.}
\PY{l+s+sd}{        scheduler (Scheduler)}
\PY{l+s+sd}{            The scheduler to use for updating weights and biases with gradient descent when backpropagating through the network.}
\PY{l+s+sd}{        seed (int)}
\PY{l+s+sd}{            Seed for random number generating with numpy.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers} \PY{o}{=} \PY{p}{[}\PY{p}{]} \PY{c+c1}{\PYZsh{} List of layers}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}layers} \PY{o}{=} \PY{l+m+mi}{0}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{cost\PYZus{}func} \PY{o}{=} \PY{n}{cost\PYZus{}func}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler} \PY{o}{=} \PY{n}{scheduler}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed} \PY{o}{=} \PY{n}{seed}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{single\PYZus{}output} \PY{o}{=} \PY{k+kc}{None} \PY{c+c1}{\PYZsh{} Boolean. Will update this when adding OutputLayer or DenseLayer output}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features\PYZus{}output} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predicted} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}extra} \PY{o}{=} \PY{k+kc}{None}
    
    \PY{k}{def} \PY{n+nf}{reset\PYZus{}weights}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Reset weights and biases in all layers to random values from a normal distribution.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k}{for} \PY{n}{layer} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{:}
            \PY{n}{layer}\PY{o}{.}\PY{n}{reset\PYZus{}weights}\PY{p}{(}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{reset\PYZus{}schedulers}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Reset the schedulers of the RNN.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k}{for} \PY{n}{layer} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{:}
            \PY{n}{layer}\PY{o}{.}\PY{n}{reset\PYZus{}schedulers}\PY{p}{(}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{feed\PYZus{}forward}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{X}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Feed forward through the RNN one layer at a time.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        X (ndarray)}
\PY{l+s+sd}{            Input to the RNN, with shape (batch size, sequence length, number of features)}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Returns}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        output (ndarray)}
\PY{l+s+sd}{            Output from the RNN. If the network produces sequential output, it has the same shape as the input X. If the network produces a single output, it has the shape (batch size, number of features), that is, the same shape as for sequential output, but without the sequence axis.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{X\PYZus{}shape} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}

        \PY{c+c1}{\PYZsh{}\PYZsh{} Initialize output}
        \PY{n}{n\PYZus{}batches} \PY{o}{=} \PY{n}{X\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{sequence\PYZus{}length} \PY{o}{=} \PY{n}{X\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{n\PYZus{}features\PYZus{}output} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features\PYZus{}output}
        
        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{single\PYZus{}output}\PY{p}{:}
            \PY{n}{output\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{n}{n\PYZus{}batches}\PY{p}{,} \PY{n}{n\PYZus{}features\PYZus{}output}\PY{p}{)}
            \PY{n}{output} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{output\PYZus{}shape}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{output\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{n}{n\PYZus{}batches}\PY{p}{,} \PY{n}{sequence\PYZus{}length}\PY{p}{,} \PY{n}{n\PYZus{}features\PYZus{}output}\PY{p}{)}
            \PY{n}{output} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{output\PYZus{}shape}\PY{p}{)}
            
        \PY{c+c1}{\PYZsh{}\PYZsh{} Feed forward through all layers}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{X}\PY{p}{)}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}layers}\PY{p}{)}\PY{p}{:}
            \PY{n}{layer} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{n}{i}\PY{p}{]}
            \PY{n}{prev\PYZus{}layer} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
            \PY{n}{layer}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{prev\PYZus{}layer}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{} Get output from last layer}
        \PY{n}{output\PYZus{}layer} \PY{o}{=} \PY{n}{layer}
        \PY{k}{if} \PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{is\PYZus{}dense}\PY{p}{:}
            \PY{n}{node} \PY{o}{=} \PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{output} \PY{o}{=} \PY{n}{node}\PY{o}{.}\PY{n}{h\PYZus{}output}
        \PY{k}{else}\PY{p}{:}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{n\PYZus{}nodes}\PY{p}{)}\PY{p}{:}
                \PY{n}{node} \PY{o}{=} \PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                \PY{n}{output}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{,}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{node}\PY{o}{.}\PY{n}{h\PYZus{}output}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{} Store and return output}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output} \PY{o}{=} \PY{n}{output}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output}
    
    \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{X}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Feed forward through the RNN, and use argmax to classify the results.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        X (ndarray)}
\PY{l+s+sd}{            Input to the RNN, with shape (batch size, sequence length, number of features)}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Returns}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        predicted (ndarray)}
\PY{l+s+sd}{            Predicted classification output. If the network produces sequential output, it has the same shape as the input X. If the network produces a single output, it has the shape (batch size, number of features), that is, the same shape as for sequential output, but without the sequence axis.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{} Initialize predicted array}
        \PY{n}{output} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{X}\PY{p}{)}
        \PY{n}{predicted} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{output}\PY{p}{)}

        \PY{c+c1}{\PYZsh{}\PYZsh{} Find maximum values at the feature axis, and set predicted to 1 at those indices}
        \PY{n}{ind\PYZus{}batch} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{predicted}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{single\PYZus{}output}\PY{p}{:}
            \PY{n}{ind\PYZus{}max} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{output}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Set predicted to 1 at the maximum values}
            \PY{n}{predicted}\PY{p}{[}\PY{n}{ind\PYZus{}batch}\PY{p}{,} \PY{n}{ind\PYZus{}max}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{ind\PYZus{}seq} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{output}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
            \PY{n}{ind\PYZus{}max} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{output}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
            \PY{n}{ind\PYZus{}seq}\PY{p}{,} \PY{n}{ind\PYZus{}batch} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{ind\PYZus{}seq}\PY{p}{,} \PY{n}{ind\PYZus{}batch}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} Set predicted to 1 at the maximum values}
            \PY{n}{predicted}\PY{p}{[}\PY{n}{ind\PYZus{}batch}\PY{p}{,} \PY{n}{ind\PYZus{}seq}\PY{p}{,} \PY{n}{ind\PYZus{}max}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predicted} \PY{o}{=} \PY{n}{predicted}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predicted}


    \PY{k}{def} \PY{n+nf}{extrapolate}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{length}\PY{p}{:} \PY{n+nb}{int}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Extrapolate data by continuing the sequence from the output of the last layer, with the output of the previous time step as input for the new time step}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        length (int)}
\PY{l+s+sd}{            Number of time steps to extrapolate.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Returns}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        output\PYZus{}extra (ndarray)}
\PY{l+s+sd}{            Extrapolated output from the RNN. It has the shape (number of batches, extrapolation length, number of features).}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{} Initialize extrapolated output}
        \PY{n}{output} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output}
        \PY{n}{n\PYZus{}batches} \PY{o}{=} \PY{n}{output}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{n\PYZus{}features} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features\PYZus{}output}
        \PY{n}{output\PYZus{}extra\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{n}{n\PYZus{}batches}\PY{p}{,} \PY{n}{length}\PY{p}{,} \PY{n}{n\PYZus{}features}\PY{p}{)}
        \PY{n}{output\PYZus{}extra} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{output\PYZus{}extra\PYZus{}shape}\PY{p}{)}

        \PY{c+c1}{\PYZsh{}\PYZsh{} Extrapolate}
        \PY{n}{y\PYZus{}prev} \PY{o}{=} \PY{n}{output}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{]} \PY{c+c1}{\PYZsh{} Output from last layer at last time step}

        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{length}\PY{p}{)}\PY{p}{:}
            \PY{n}{h\PYZus{}layer} \PY{o}{=} \PY{n}{y\PYZus{}prev} \PY{c+c1}{\PYZsh{} h\PYZus{}layer at first layer = input = previous output}
            \PY{k}{for} \PY{n}{l} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}layers}\PY{p}{)}\PY{p}{:}
                \PY{n}{layer} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{n}{l}\PY{p}{]}
                \PY{n}{node\PYZus{}prev} \PY{o}{=} \PY{n}{layer}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{c+c1}{\PYZsh{} Node at previous time step}
                \PY{n}{h\PYZus{}time} \PY{o}{=} \PY{n}{node\PYZus{}prev}\PY{o}{.}\PY{n}{h\PYZus{}output}
                \PY{c+c1}{\PYZsh{} print(h\PYZus{}layer, h\PYZus{}time)}
                \PY{n}{layer}\PY{o}{.}\PY{n}{add\PYZus{}node}\PY{p}{(}\PY{p}{)}
                \PY{n}{node} \PY{o}{=} \PY{n}{layer}\PY{o}{.}\PY{n}{nodes}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{c+c1}{\PYZsh{} Current node}
                \PY{k}{if} \PY{n}{l} \PY{o}{==} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}layers}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                    \PY{c+c1}{\PYZsh{} If this is the output layer (last layer), set h\PYZus{}time to None}
                    \PY{n}{h\PYZus{}time} \PY{o}{=} \PY{k+kc}{None}
                \PY{n}{node}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{h\PYZus{}layer}\PY{p}{,} \PY{n}{h\PYZus{}time}\PY{p}{)}
                \PY{n}{h\PYZus{}layer} \PY{o}{=} \PY{n}{node}\PY{o}{.}\PY{n}{h\PYZus{}output} \PY{c+c1}{\PYZsh{} Update h\PYZus{}layer}
            \PY{n}{y\PYZus{}prev} \PY{o}{=} \PY{n}{h\PYZus{}layer}
            \PY{n}{output\PYZus{}extra}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{,}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{y\PYZus{}prev}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}extra} \PY{o}{=} \PY{n}{output\PYZus{}extra}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}extra}

    
    \PY{k}{def} \PY{n+nf}{backpropagate}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{output}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{,}
            \PY{n}{target}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{,}
            \PY{n}{lmbd}\PY{p}{:} \PY{n+nb}{float} \PY{o}{=} \PY{l+m+mf}{0.01}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Backpropagate through the RNN one layer at a time, and update all the weights and biases.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        output (ndarray)}
\PY{l+s+sd}{            The output we get from forward propagation, used to compare with the target values.}
\PY{l+s+sd}{        target (ndarray)}
\PY{l+s+sd}{            The target output that we want to compare our results to.}
\PY{l+s+sd}{        lmbd (float)}
\PY{l+s+sd}{            Regularization parameter for finding the cost gradient with respect to the weights.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{} Find gradient of cost function}
        \PY{n}{grad\PYZus{}cost} \PY{o}{=} \PY{n}{derivate}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{cost\PYZus{}func}\PY{p}{(}\PY{n}{target}\PY{p}{)}\PY{p}{)}
        \PY{n}{dC} \PY{o}{=} \PY{n}{grad\PYZus{}cost}\PY{p}{(}\PY{n}{output}\PY{p}{)}

        \PY{c+c1}{\PYZsh{}\PYZsh{} Backpropagate through all layers}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{backpropagate}\PY{p}{(}\PY{n}{dC}\PY{p}{,} \PY{n}{lmbd}\PY{o}{=}\PY{n}{lmbd}\PY{p}{)}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}layers}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
            \PY{n}{layer} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{n}{i}\PY{p}{]}
            \PY{n}{next\PYZus{}layer} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}
            \PY{n}{layer}\PY{o}{.}\PY{n}{backpropagate}\PY{p}{(}\PY{n}{next\PYZus{}layer}\PY{p}{,} \PY{n}{lmbd}\PY{p}{)}
        
    \PY{k}{def} \PY{n+nf}{train}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{X\PYZus{}train}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{,}
            \PY{n}{t\PYZus{}train}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{,}
            \PY{n}{X\PYZus{}val}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray} \PY{o}{=} \PY{k+kc}{None}\PY{p}{,}
            \PY{n}{t\PYZus{}val}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray} \PY{o}{=} \PY{k+kc}{None}\PY{p}{,}
            \PY{n}{epochs}\PY{p}{:} \PY{n+nb}{int} \PY{o}{=} \PY{l+m+mi}{100}\PY{p}{,}
            \PY{n}{batches}\PY{p}{:} \PY{n+nb}{int} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,}
            \PY{n}{lmbd}\PY{p}{:} \PY{n+nb}{float} \PY{o}{=} \PY{l+m+mf}{0.01}\PY{p}{,}
            \PY{n}{store\PYZus{}output}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray} \PY{o}{=} \PY{k+kc}{False}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Trains the RNN on data, tuning the weights and biases such that the network can make predictions on new unseen data.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        X\PYZus{}train (ndarray)}
\PY{l+s+sd}{            Input data to the network during training, with shape (batch size, sequence length, number of features).}
\PY{l+s+sd}{        t\PYZus{}train (ndarray)}
\PY{l+s+sd}{            Target output corresponding to the training input. Same shape as X\PYZus{}train if the output is sequential, (batch size, number of features) if not.}
\PY{l+s+sd}{        X\PYZus{}val (ndarray)}
\PY{l+s+sd}{            Optional. Validation data to see how the RNN performs on unseen data. Not used for training, only for calculating scores.}
\PY{l+s+sd}{        t\PYZus{}val (ndarray)}
\PY{l+s+sd}{            Optional. Target output corresponding to the validation data.}
\PY{l+s+sd}{        epochs (int)}
\PY{l+s+sd}{            Number of epochs to train for.}
\PY{l+s+sd}{        batches (int)}
\PY{l+s+sd}{            Number of batches to split data into at each epoch.}
\PY{l+s+sd}{        lmbd (float)}
\PY{l+s+sd}{            Regularization parameter for finding the cost gradient with respect to the weights.}
\PY{l+s+sd}{        store\PYZus{}output (bool)}
\PY{l+s+sd}{            Whether to store output from each epoch (if you want to see how the output evolves as the RNN trains).}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Returns}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        scores (dict)}
\PY{l+s+sd}{            The different scores from the training stored as a dictionary, with the following keys:}
\PY{l+s+sd}{            \PYZhy{} \PYZdq{}train\PYZus{}error\PYZdq{} : The error of the training data for each epoch.}
\PY{l+s+sd}{            \PYZhy{} \PYZdq{}train\PYZus{}accuracy\PYZdq{} : The accuracy of the training data for each epoch.}
\PY{l+s+sd}{            \PYZhy{} \PYZdq{}val\PYZus{}error\PYZdq{} : The error of the validation data for each epoch.}
\PY{l+s+sd}{            \PYZhy{} \PYZdq{}val\PYZus{}accuracy\PYZdq{} : The accuracy of the validation data for each epoch.}
\PY{l+s+sd}{            \PYZhy{} \PYZdq{}y\PYZus{}train\PYZus{}history\PYZdq{} : The output from the training data for each epoch.}
\PY{l+s+sd}{            \PYZhy{} \PYZdq{}y\PYZus{}val\PYZus{}history\PYZdq{} : The output from the validation data for each epoch.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reset\PYZus{}weights}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} Reset weights for new training}
        \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{/}\PY{o}{/} \PY{n}{batches}

        \PY{c+c1}{\PYZsh{} Initialize arrays for scores}
        \PY{n}{train\PYZus{}cost} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{cost\PYZus{}func}\PY{p}{(}\PY{n}{t\PYZus{}train}\PY{p}{)}
        \PY{n}{train\PYZus{}error} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}
        \PY{n}{train\PYZus{}accuracy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}

        \PY{k}{if} \PY{n}{X\PYZus{}val} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
            \PY{n}{val\PYZus{}cost} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{cost\PYZus{}func}\PY{p}{(}\PY{n}{t\PYZus{}val}\PY{p}{)}
            \PY{n}{val\PYZus{}error} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}
            \PY{n}{val\PYZus{}accuracy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{} Initialize arrays for output history if this should be stored}
        \PY{k}{if} \PY{n}{store\PYZus{}output}\PY{p}{:}
            \PY{n}{n\PYZus{}batches\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{seq\PYZus{}length\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
            \PY{n}{n\PYZus{}features\PYZus{}output} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features\PYZus{}output}

            \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{single\PYZus{}output}\PY{p}{:}
                \PY{n}{y\PYZus{}train\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{n\PYZus{}batches\PYZus{}train}\PY{p}{,} \PY{n}{n\PYZus{}features\PYZus{}output}\PY{p}{)}
            \PY{k}{else}\PY{p}{:}
                \PY{n}{y\PYZus{}train\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{n\PYZus{}batches\PYZus{}train}\PY{p}{,} \PY{n}{seq\PYZus{}length\PYZus{}train}\PY{p}{,} \PY{n}{n\PYZus{}features\PYZus{}output}\PY{p}{)}
            
            \PY{n}{y\PYZus{}train\PYZus{}history} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{y\PYZus{}train\PYZus{}shape}\PY{p}{)}
            
            \PY{k}{if} \PY{n}{X\PYZus{}val} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                \PY{n}{n\PYZus{}batches\PYZus{}val} \PY{o}{=} \PY{n}{X\PYZus{}val}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                \PY{n}{seq\PYZus{}length\PYZus{}val} \PY{o}{=} \PY{n}{X\PYZus{}val}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}

                \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{single\PYZus{}output}\PY{p}{:}
                    \PY{n}{y\PYZus{}val\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{n\PYZus{}batches\PYZus{}val}\PY{p}{,} \PY{n}{n\PYZus{}features\PYZus{}output}\PY{p}{)}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{y\PYZus{}val\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{n\PYZus{}batches\PYZus{}val}\PY{p}{,} \PY{n}{seq\PYZus{}length\PYZus{}val}\PY{p}{,} \PY{n}{n\PYZus{}features\PYZus{}output}\PY{p}{)}
                
                \PY{n}{y\PYZus{}val\PYZus{}history} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{y\PYZus{}val\PYZus{}shape}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Resample X and t}
        \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t\PYZus{}train} \PY{o}{=} \PY{n}{resample}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t\PYZus{}train}\PY{p}{,} \PY{n}{replace}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}

        \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{EPOCH: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{e}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}r}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{for} \PY{n}{b} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{batches}\PY{p}{)}\PY{p}{:}
                \PY{c+c1}{\PYZsh{}\PYZsh{} Extract a smaller batch from the training data}
                \PY{k}{if} \PY{n}{b} \PY{o}{==} \PY{n}{batches} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{:}
                    \PY{c+c1}{\PYZsh{} If this is the last batch, include all remaining elements}
                    \PY{n}{X\PYZus{}batch} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{b}\PY{o}{*}\PY{n}{batch\PYZus{}size} \PY{p}{:}\PY{p}{]}
                    \PY{n}{t\PYZus{}batch} \PY{o}{=} \PY{n}{t\PYZus{}train}\PY{p}{[}\PY{n}{b}\PY{o}{*}\PY{n}{batch\PYZus{}size} \PY{p}{:}\PY{p}{]}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{X\PYZus{}batch} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{b}\PY{o}{*}\PY{n}{batch\PYZus{}size} \PY{p}{:} \PY{p}{(}\PY{n}{b}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{n}{batch\PYZus{}size}\PY{p}{]}
                    \PY{n}{t\PYZus{}batch} \PY{o}{=} \PY{n}{t\PYZus{}train}\PY{p}{[}\PY{n}{b}\PY{o}{*}\PY{n}{batch\PYZus{}size} \PY{p}{:} \PY{p}{(}\PY{n}{b}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{n}{batch\PYZus{}size}\PY{p}{]}
                
                \PY{c+c1}{\PYZsh{}\PYZsh{} Train the network on this batch with gradient descent}
                \PY{n}{y\PYZus{}batch} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{X\PYZus{}batch}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{backpropagate}\PY{p}{(}\PY{n}{y\PYZus{}batch}\PY{p}{,} \PY{n}{t\PYZus{}batch}\PY{p}{,} \PY{n}{lmbd}\PY{p}{)}
            
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reset\PYZus{}schedulers}\PY{p}{(}\PY{p}{)}

            \PY{c+c1}{\PYZsh{}\PYZsh{} Compute scores for this epoch}
            \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
            \PY{n}{pred\PYZus{}train} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}

            \PY{n}{train\PYZus{}error}\PY{p}{[}\PY{n}{e}\PY{p}{]} \PY{o}{=} \PY{n}{train\PYZus{}cost}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}
            
            \PY{n}{train\PYZus{}acc\PYZus{}arr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{all}\PY{p}{(}\PY{n}{pred\PYZus{}train} \PY{o}{==} \PY{n}{t\PYZus{}train}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{train\PYZus{}accuracy}\PY{p}{[}\PY{n}{e}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{train\PYZus{}acc\PYZus{}arr}\PY{p}{)}

            \PY{k}{if} \PY{n}{X\PYZus{}val} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                \PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}
                \PY{n}{pred\PYZus{}val} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}
                
                \PY{n}{val\PYZus{}error}\PY{p}{[}\PY{n}{e}\PY{p}{]} \PY{o}{=} \PY{n}{val\PYZus{}cost}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{)}
            
                \PY{n}{val\PYZus{}acc\PYZus{}arr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{all}\PY{p}{(}\PY{n}{pred\PYZus{}val} \PY{o}{==} \PY{n}{t\PYZus{}val}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
                \PY{n}{val\PYZus{}accuracy}\PY{p}{[}\PY{n}{e}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{val\PYZus{}acc\PYZus{}arr}\PY{p}{)}
            
            \PY{k}{if} \PY{n}{store\PYZus{}output}\PY{p}{:}
                \PY{n}{y\PYZus{}train\PYZus{}history}\PY{p}{[}\PY{n}{e}\PY{p}{]} \PY{o}{=} \PY{n}{y\PYZus{}train}
                \PY{k}{if} \PY{n}{X\PYZus{}val} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                    \PY{n}{y\PYZus{}val\PYZus{}history}\PY{p}{[}\PY{n}{e}\PY{p}{]} \PY{o}{=} \PY{n}{y\PYZus{}val}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{} Create a dictionary for the scores, and return it}
        \PY{n}{scores} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train\PYZus{}error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{train\PYZus{}error}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train\PYZus{}accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{train\PYZus{}accuracy}\PY{p}{\PYZcb{}}
        \PY{k}{if} \PY{n}{X\PYZus{}val} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
            \PY{n}{scores}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{val\PYZus{}error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{val\PYZus{}error}
            \PY{n}{scores}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{val\PYZus{}accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{val\PYZus{}accuracy}
        
        \PY{k}{if} \PY{n}{store\PYZus{}output}\PY{p}{:}
            \PY{n}{scores}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{y\PYZus{}train\PYZus{}history}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{y\PYZus{}train\PYZus{}history}
            \PY{k}{if} \PY{n}{X\PYZus{}val} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                \PY{n}{scores}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{y\PYZus{}val\PYZus{}history}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{y\PYZus{}val\PYZus{}history}
        
        \PY{k}{return} \PY{n}{scores}

    \PY{k}{def} \PY{n+nf}{add\PYZus{}InputLayer}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{n\PYZus{}features}\PY{p}{:} \PY{n+nb}{int}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Adds an InputLayer to the RNN.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        n\PYZus{}features (int)}
\PY{l+s+sd}{            Number of features of the input.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{layer} \PY{o}{=} \PY{n}{InputLayer}\PY{p}{(}\PY{n}{n\PYZus{}features}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}add\PYZus{}layer}\PY{p}{(}\PY{n}{layer}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{add\PYZus{}RNNLayer}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{n\PYZus{}features}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,}
            \PY{n}{act\PYZus{}func}\PY{p}{:} \PY{n}{Callable}\PY{p}{[}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Adds an RNNLayer to the RNN.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        n\PYZus{}features (int)}
\PY{l+s+sd}{            Number of features in this layer.}
\PY{l+s+sd}{        act\PYZus{}func (Callable)}
\PY{l+s+sd}{            The activation function to use for this layer.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{scheduler} \PY{o}{=} \PY{n}{copy}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler}\PY{p}{)}
        \PY{n}{prev\PYZus{}layer} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{n\PYZus{}features\PYZus{}prev} \PY{o}{=} \PY{n}{prev\PYZus{}layer}\PY{o}{.}\PY{n}{n\PYZus{}features}
        \PY{n}{layer} \PY{o}{=} \PY{n}{RNNLayer}\PY{p}{(}\PY{n}{n\PYZus{}features}\PY{p}{,} \PY{n}{n\PYZus{}features\PYZus{}prev}\PY{p}{,} \PY{n}{act\PYZus{}func}\PY{p}{,} \PY{n}{scheduler}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}add\PYZus{}layer}\PY{p}{(}\PY{n}{layer}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{add\PYZus{}OutputLayer}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{n\PYZus{}features}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,}
            \PY{n}{act\PYZus{}func}\PY{p}{:} \PY{n}{Callable}\PY{p}{[}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Adds an OutputLayer to the RNN.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        n\PYZus{}features (int)}
\PY{l+s+sd}{            Number of features in this layer.}
\PY{l+s+sd}{        act\PYZus{}func (Callable)}
\PY{l+s+sd}{            The activation function to use for this layer.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{scheduler} \PY{o}{=} \PY{n}{copy}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler}\PY{p}{)}
        \PY{n}{prev\PYZus{}layer} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{n\PYZus{}features\PYZus{}prev} \PY{o}{=} \PY{n}{prev\PYZus{}layer}\PY{o}{.}\PY{n}{n\PYZus{}features}
        \PY{n}{layer} \PY{o}{=} \PY{n}{OutputLayer}\PY{p}{(}\PY{n}{n\PYZus{}features}\PY{p}{,} \PY{n}{n\PYZus{}features\PYZus{}prev}\PY{p}{,} \PY{n}{act\PYZus{}func}\PY{p}{,} \PY{n}{scheduler}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}add\PYZus{}layer}\PY{p}{(}\PY{n}{layer}\PY{p}{)}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{single\PYZus{}output} \PY{o}{=} \PY{k+kc}{False}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features\PYZus{}output} \PY{o}{=} \PY{n}{n\PYZus{}features}
    
    \PY{k}{def} \PY{n+nf}{add\PYZus{}DenseLayer}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{n\PYZus{}features}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,}
            \PY{n}{act\PYZus{}func}\PY{p}{:} \PY{n}{Callable}\PY{p}{[}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{]}\PY{p}{,}
            \PY{n}{is\PYZus{}last\PYZus{}layer}\PY{p}{:} \PY{n+nb}{bool} \PY{o}{=} \PY{k+kc}{False}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Adds a DenseLayer to the RNN.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        n\PYZus{}features (int)}
\PY{l+s+sd}{            Number of features in this layer.}
\PY{l+s+sd}{        act\PYZus{}func (Callable)}
\PY{l+s+sd}{            The activation function to use for this layer.}
\PY{l+s+sd}{        is\PYZus{}last\PYZus{}layer (bool)}
\PY{l+s+sd}{            True if this is the last layer of the network (the output layer). False if not.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{scheduler} \PY{o}{=} \PY{n}{copy}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scheduler}\PY{p}{)}
        \PY{n}{prev\PYZus{}layer} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{n\PYZus{}features\PYZus{}prev} \PY{o}{=} \PY{n}{prev\PYZus{}layer}\PY{o}{.}\PY{n}{n\PYZus{}features}
        \PY{n}{layer} \PY{o}{=} \PY{n}{DenseLayer}\PY{p}{(}\PY{n}{n\PYZus{}features}\PY{p}{,} \PY{n}{n\PYZus{}features\PYZus{}prev}\PY{p}{,} \PY{n}{act\PYZus{}func}\PY{p}{,} \PY{n}{scheduler}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}add\PYZus{}layer}\PY{p}{(}\PY{n}{layer}\PY{p}{)}

        \PY{k}{if} \PY{n}{is\PYZus{}last\PYZus{}layer}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{single\PYZus{}output} \PY{o}{=} \PY{k+kc}{True}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}features\PYZus{}output} \PY{o}{=} \PY{n}{n\PYZus{}features}

    \PY{k}{def} \PY{n+nf}{\PYZus{}add\PYZus{}layer}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{p}{,}
            \PY{n}{layer}\PY{p}{:} \PY{n}{Layer}
    \PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Adds a layer to the RNN by appending it to *layers*, and increases *n\PYZus{}layers* by one.}

\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        layer (Layer)}
\PY{l+s+sd}{            The layer to add to the RNN.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{layer}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}layers} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Examples on applications}\label{examples-on-applications}

    \subsubsection{Predicting the weather}\label{predicting-the-weather}

    We will now explore a dataset that considers how temperature evolves
over time. The dataset is obtained from the ``Zero to GPT'' course by
Vik Paruchuri at Dataquest, and can be found at
https://github.com/VikParuchuri/zero\_to\_gpt/blob/81cf89f68143535b7361990aa0fc49d253399c65/data/clean\_weather.csv.

Let's first look at the dataset provided.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\PY{n}{weather\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{https://raw.githubusercontent.com/VikParuchuri/zero\PYZus{}to\PYZus{}gpt/81cf89f68143535b7361990aa0fc49d253399c65/data/clean\PYZus{}weather.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{weather\PYZus{}data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{The dataset has }\PY{l+s+si}{\PYZob{}}\PY{n}{weather\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ rows.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
   Unnamed: 0  tmax  tmin  rain  tmax\_tomorrow
0  1970-01-01  60.0  35.0  0.00           52.0
1  1970-01-02  52.0  39.0  0.00           52.0
2  1970-01-03  52.0  35.0  0.00           53.0
3  1970-01-04  53.0  36.0  0.00           52.0
4  1970-01-05  52.0  35.0  0.00           50.0
5  1970-01-06  50.0  38.0  0.00           52.0
6  1970-01-07  52.0  43.0  0.00           56.0
7  1970-01-08  56.0  49.0  0.24           54.0
8  1970-01-09  54.0  50.0  0.40           57.0
9  1970-01-10  57.0  50.0  0.00           57.0

The dataset has 13509 rows.
    \end{Verbatim}

    The relevant columns for our purposes are \emph{tmax} and
\emph{tmax\_tomorrow}, so we will only retrieve these. Note that the
dataset has \(13\,509\) rows (time steps), which is too much to work
with for our simple RNN, and will quickly result in vanishing or
exploding gradients. We will therefore only retrieve the first \(100\)
rows. Notice also that the \(i\)'th element of \emph{tmax\_tomorrow} is
the \(i+1\)'th element of \emph{tmax}, as we want to use the RNN to
predict the temperature the next day based on the temperatures on
preceding days. Thus \emph{tmax} will be the input to the network, and
\emph{tmax\_tomorrow} the target output.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{N} \PY{o}{=} \PY{l+m+mi}{100}
\PY{n}{x\PYZus{}orig} \PY{o}{=} \PY{n}{weather\PYZus{}data}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tmax}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{n}{N}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}numpy}\PY{p}{(}\PY{p}{)}
\PY{n}{t\PYZus{}orig} \PY{o}{=} \PY{n}{weather\PYZus{}data}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tmax\PYZus{}tomorrow}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{n}{N}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}numpy}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    For the validation data, we will retrieve a subset of the data at some
later time, such that the training and validation data do not overlap.
Notice also that we let the validation data contain \(70\) time steps,
even though the training data contains \(100\) time steps. This
demonstrates one of the advantages of RNNs, their flexibility with input
lengths.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{val\PYZus{}start} \PY{o}{=} \PY{l+m+mi}{1000}
\PY{n}{N\PYZus{}val} \PY{o}{=} \PY{l+m+mi}{70}
\PY{n}{x\PYZus{}val\PYZus{}orig} \PY{o}{=} \PY{n}{weather\PYZus{}data}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tmax}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{[}\PY{n}{val\PYZus{}start}\PY{p}{:}\PY{n}{val\PYZus{}start}\PY{o}{+}\PY{n}{N\PYZus{}val}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}numpy}\PY{p}{(}\PY{p}{)}
\PY{n}{t\PYZus{}val\PYZus{}orig} \PY{o}{=} \PY{n}{weather\PYZus{}data}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tmax\PYZus{}tomorrow}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{[}\PY{n}{val\PYZus{}start}\PY{p}{:}\PY{n}{val\PYZus{}start}\PY{o}{+}\PY{n}{N\PYZus{}val}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}numpy}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    We need the input and output of the RNN to have shape \emph{(batch size,
sequence length, number of features)}, but the data we have read now are
one-dimensional since they only have one feature (the scalar
temperature) and one input (so batch size is \(1\)). We therefore have
to add axes such that the data is on the correct form. We will also
scale the data to make it easier for the RNN to train. The scaling is
done using scikit's \emph{MinMaxScaler}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{MinMaxScaler}

\PY{c+c1}{\PYZsh{} Add feature axis}
\PY{n}{x} \PY{o}{=} \PY{n}{x\PYZus{}orig}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
\PY{n}{t} \PY{o}{=} \PY{n}{t\PYZus{}orig}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
\PY{n}{x\PYZus{}val} \PY{o}{=} \PY{n}{x\PYZus{}val\PYZus{}orig}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
\PY{n}{t\PYZus{}val} \PY{o}{=} \PY{n}{t\PYZus{}val\PYZus{}orig}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Scale data}
\PY{n}{sc} \PY{o}{=} \PY{n}{MinMaxScaler}\PY{p}{(}\PY{p}{)}
\PY{n}{sc} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x}\PY{p}{)}

\PY{n}{x} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{x}\PY{p}{)}
\PY{n}{t} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{t}\PY{p}{)}
\PY{n}{x\PYZus{}val} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{x\PYZus{}val}\PY{p}{)}
\PY{n}{t\PYZus{}val} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{t\PYZus{}val}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Add batch axis}
\PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,}\PY{p}{:}\PY{p}{]}
\PY{n}{t} \PY{o}{=} \PY{n}{t}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,}\PY{p}{:}\PY{p}{]}
\PY{n}{x\PYZus{}val} \PY{o}{=} \PY{n}{x\PYZus{}val}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,}\PY{p}{:}\PY{p}{]}
\PY{n}{t\PYZus{}val} \PY{o}{=} \PY{n}{t\PYZus{}val}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    We can now create the network, by declaring an RNN object and adding
layers to it. We will use one hidden recurrent layer.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cost\PYZus{}func} \PY{o}{=} \PY{n}{CostOLS}
\PY{n}{act\PYZus{}func\PYZus{}hidden} \PY{o}{=} \PY{n}{identity}
\PY{n}{act\PYZus{}func\PYZus{}output} \PY{o}{=} \PY{n}{identity}
\PY{n}{eta} \PY{o}{=} \PY{l+m+mf}{0.001}
\PY{n}{scheduler} \PY{o}{=} \PY{n}{Adam}\PY{p}{(}\PY{n}{eta}\PY{o}{=}\PY{n}{eta}\PY{p}{,} \PY{n}{rho}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{,} \PY{n}{rho2}\PY{o}{=}\PY{l+m+mf}{0.999}\PY{p}{)}

\PY{c+c1}{\PYZsh{}\PYZsh{} Create RNN}
\PY{n}{rnn} \PY{o}{=} \PY{n}{RNN}\PY{p}{(}\PY{n}{cost\PYZus{}func}\PY{p}{,} \PY{n}{scheduler}\PY{p}{)}
\PY{n}{rnn}\PY{o}{.}\PY{n}{add\PYZus{}InputLayer}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{rnn}\PY{o}{.}\PY{n}{add\PYZus{}RNNLayer}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{act\PYZus{}func\PYZus{}hidden}\PY{p}{)}
\PY{n}{rnn}\PY{o}{.}\PY{n}{add\PYZus{}OutputLayer}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{act\PYZus{}func\PYZus{}output}\PY{p}{)}
\PY{n}{rnn}\PY{o}{.}\PY{n}{reset\PYZus{}weights}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    With the RNN in place we can train it on the weather dataset.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} Train network on data}
\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{50}
\PY{n}{batches} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n}{lmbd} \PY{o}{=} \PY{l+m+mf}{0.001}
\PY{n}{scores} \PY{o}{=} \PY{n}{rnn}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{t}\PY{p}{,} \PY{n}{x\PYZus{}val}\PY{p}{,} \PY{n}{t\PYZus{}val}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{batches}\PY{o}{=}\PY{n}{batches}\PY{p}{,} \PY{n}{lmbd}\PY{o}{=}\PY{n}{lmbd}\PY{p}{,} \PY{n}{store\PYZus{}output}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
EPOCH: 50/50
    \end{Verbatim}

    Now extract the results from the training. We scaled the input before
training, so in order to get the actual temperature predicted we need to
scale the ouput back using the scalers \emph{inverse\_transform()}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} Extract output}
\PY{n}{epoch\PYZus{}arr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{epochs}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{y\PYZus{}history\PYZus{}scaled} \PY{o}{=} \PY{n}{scores}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{y\PYZus{}train\PYZus{}history}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{train\PYZus{}error} \PY{o}{=} \PY{n}{scores}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train\PYZus{}error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{y\PYZus{}val\PYZus{}history\PYZus{}scaled} \PY{o}{=} \PY{n}{scores}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{y\PYZus{}val\PYZus{}history}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{val\PYZus{}error} \PY{o}{=} \PY{n}{scores}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{val\PYZus{}error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}

\PY{c+c1}{\PYZsh{}\PYZsh{} Inversely scale output}
\PY{n}{y\PYZus{}history} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{y\PYZus{}history\PYZus{}scaled}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{y\PYZus{}history\PYZus{}scaled}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
    \PY{n}{yi} \PY{o}{=} \PY{n}{y\PYZus{}history\PYZus{}scaled}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}
    \PY{n}{yi\PYZus{}scaleback} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{inverse\PYZus{}transform}\PY{p}{(}\PY{n}{yi}\PY{p}{)}
    \PY{n}{y\PYZus{}history}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{yi\PYZus{}scaleback}
\end{Verbatim}
\end{tcolorbox}

    We can now plot the error over time to see how the model performs during
training.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} \PYZpc{}matplotlib notebook}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}

\PY{c+c1}{\PYZsh{}\PYZsh{} Plot error during training}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epoch\PYZus{}arr}\PY{p}{,} \PY{n}{train\PYZus{}error}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epoch\PYZus{}arr}\PY{p}{,} \PY{n}{val\PYZus{}error}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Validation error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{yscale}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{log}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_143_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The error decreases and stabilizes, as it should. Let us also look at
how the output of the training data compares to the target after
training.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} Plot output and target after training}
\PY{n}{seq\PYZus{}ind} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{N}\PY{p}{)}
\PY{n}{y} \PY{o}{=} \PY{n}{y\PYZus{}history}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Maximum temperature over time for training data.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of days}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Temperature}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq\PYZus{}ind}\PY{p}{,} \PY{n}{t\PYZus{}orig}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Target}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq\PYZus{}ind}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Output}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{70}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}error}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{l+s+si}{:}\PY{l+s+s2}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_145_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We do the same for the validation data as well.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} Plot validation results}
\PY{n}{seq\PYZus{}val} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{N\PYZus{}val}\PY{p}{)}
\PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{y\PYZus{}val\PYZus{}history\PYZus{}scaled}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}
\PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{inverse\PYZus{}transform}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{)}
\PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{y\PYZus{}val}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Maximum temperature over time for validation data.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of days}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Temperature}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq\PYZus{}val}\PY{p}{,} \PY{n}{t\PYZus{}val\PYZus{}orig}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Target}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Output}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{55}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}error}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{l+s+si}{:}\PY{l+s+s2}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_147_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Both the training and validation output looks good, fitting the target
relatively well considering that we are using a simple RNN. Let's see if
the model can also extrapolate the output to get the temperature for the
next \(50\) days.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} Extrapolate data using network}
\PY{n}{length} \PY{o}{=} \PY{l+m+mi}{50}
\PY{n}{y\PYZus{}extra} \PY{o}{=} \PY{n}{rnn}\PY{o}{.}\PY{n}{extrapolate}\PY{p}{(}\PY{n}{length}\PY{p}{)}
\PY{n}{y\PYZus{}extra} \PY{o}{=} \PY{n}{y\PYZus{}extra}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}
\PY{n}{y\PYZus{}extra} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{inverse\PYZus{}transform}\PY{p}{(}\PY{n}{y\PYZus{}extra}\PY{p}{)}
\PY{n}{y\PYZus{}extra} \PY{o}{=} \PY{n}{y\PYZus{}extra}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{seq\PYZus{}extra} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{N}\PY{p}{,} \PY{n}{N}\PY{o}{+}\PY{n}{length}\PY{p}{)}
\PY{n}{t\PYZus{}extra} \PY{o}{=} \PY{n}{weather\PYZus{}data}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tmax\PYZus{}tomorrow}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{[}\PY{n}{N}\PY{p}{:}\PY{n}{N}\PY{o}{+}\PY{n}{length}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}numpy}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Maximum temperature over time for training data.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of days}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Temperature}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq\PYZus{}ind}\PY{p}{,} \PY{n}{t\PYZus{}orig}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Target}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq\PYZus{}extra}\PY{p}{,} \PY{n}{t\PYZus{}extra}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{k:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{seq\PYZus{}ind}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{seq\PYZus{}extra}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{t\PYZus{}orig}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{t\PYZus{}extra}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{k\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq\PYZus{}ind}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Output}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{seq\PYZus{}ind}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{seq\PYZus{}extra}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{y}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{y\PYZus{}extra}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq\PYZus{}extra}\PY{p}{,} \PY{n}{y\PYZus{}extra}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Extrapolation}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_149_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The extrapolation does not look very good. It seems to stay constant,
independent of the fluctuations in the true data.

    We have stored the output from the network at each epoch. We can use
this to create an animation of how the model fits to the data during
training.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{animation} \PY{k}{as} \PY{n+nn}{animation}
\PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k+kn}{import} \PY{n}{HTML}
\PY{c+c1}{\PYZsh{}\PYZsh{} Create animation of how the output fits to the target}
\PY{n}{y} \PY{o}{=} \PY{n}{y\PYZus{}history}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Maximum temperature over time.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of days}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Temperature}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{t\PYZus{}plot} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq\PYZus{}ind}\PY{p}{,} \PY{n}{t\PYZus{}orig}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Target}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{y\PYZus{}plot} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq\PYZus{}ind}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Output}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{epoch\PYZus{}text} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{70}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch: }\PY{l+s+si}{\PYZob{}}\PY{l+m+mi}{0}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{error\PYZus{}text} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{68}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}error}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{l+s+si}{:}\PY{l+s+s2}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{xlim}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{N}\PY{p}{]}\PY{p}{,} \PY{n}{ylim}\PY{o}{=}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{t\PYZus{}orig}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{t\PYZus{}orig}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{update\PYZus{}plot}\PY{p}{(}\PY{n}{frame}\PY{p}{)}\PY{p}{:}
    \PY{n}{y} \PY{o}{=} \PY{n}{y\PYZus{}history}\PY{p}{[}\PY{n}{frame}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{y\PYZus{}plot}\PY{o}{.}\PY{n}{set\PYZus{}ydata}\PY{p}{(}\PY{n}{y}\PY{p}{)}
    \PY{n}{epoch\PYZus{}text}\PY{o}{.}\PY{n}{set\PYZus{}text}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{frame}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{error\PYZus{}text}\PY{o}{.}\PY{n}{set\PYZus{}text}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}error}\PY{p}{[}\PY{n}{frame}\PY{p}{]}\PY{l+s+si}{:}\PY{l+s+s2}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{y\PYZus{}plot}

\PY{n}{anim} \PY{o}{=} \PY{n}{animation}\PY{o}{.}\PY{n}{FuncAnimation}\PY{p}{(}\PY{n}{fig}\PY{o}{=}\PY{n}{fig}\PY{p}{,} \PY{n}{func}\PY{o}{=}\PY{n}{update\PYZus{}plot}\PY{p}{,} \PY{n}{frames}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{interval}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{HTML}\PY{p}{(}\PY{n}{anim}\PY{o}{.}\PY{n}{to\PYZus{}jshtml}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.HTML object>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_152_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    And we can do the same for the validation data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} Create animation for validation results}
\PY{c+c1}{\PYZsh{}\PYZsh{} Inversely scale output}
\PY{n}{y\PYZus{}val\PYZus{}history} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{y\PYZus{}val\PYZus{}history\PYZus{}scaled}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{y\PYZus{}val\PYZus{}history\PYZus{}scaled}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
    \PY{n}{yi} \PY{o}{=} \PY{n}{y\PYZus{}val\PYZus{}history\PYZus{}scaled}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}
    \PY{n}{yi\PYZus{}scaleback} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{inverse\PYZus{}transform}\PY{p}{(}\PY{n}{yi}\PY{p}{)}
    \PY{n}{y\PYZus{}val\PYZus{}history}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{yi\PYZus{}scaleback}

\PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{y\PYZus{}val\PYZus{}history}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Maximum temperature over time.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of days}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Temperature}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{t\PYZus{}plot} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq\PYZus{}val}\PY{p}{,} \PY{n}{t\PYZus{}val\PYZus{}orig}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Target}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{y\PYZus{}plot} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Output}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{epoch\PYZus{}text} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{55}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch: }\PY{l+s+si}{\PYZob{}}\PY{l+m+mi}{0}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{error\PYZus{}text} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{53}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}error}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{l+s+si}{:}\PY{l+s+s2}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{xlim}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{N\PYZus{}val}\PY{p}{]}\PY{p}{,} \PY{n}{ylim}\PY{o}{=}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{t\PYZus{}val\PYZus{}orig}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{t\PYZus{}val\PYZus{}orig}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{update\PYZus{}plot}\PY{p}{(}\PY{n}{frame}\PY{p}{)}\PY{p}{:}
    \PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{y\PYZus{}val\PYZus{}history}\PY{p}{[}\PY{n}{frame}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{y\PYZus{}plot}\PY{o}{.}\PY{n}{set\PYZus{}ydata}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{)}
    \PY{n}{epoch\PYZus{}text}\PY{o}{.}\PY{n}{set\PYZus{}text}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{frame}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{error\PYZus{}text}\PY{o}{.}\PY{n}{set\PYZus{}text}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}error}\PY{p}{[}\PY{n}{frame}\PY{p}{]}\PY{l+s+si}{:}\PY{l+s+s2}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{y\PYZus{}plot}

\PY{n}{anim} \PY{o}{=} \PY{n}{animation}\PY{o}{.}\PY{n}{FuncAnimation}\PY{p}{(}\PY{n}{fig}\PY{o}{=}\PY{n}{fig}\PY{p}{,} \PY{n}{func}\PY{o}{=}\PY{n}{update\PYZus{}plot}\PY{p}{,} \PY{n}{frames}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{interval}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{HTML}\PY{p}{(}\PY{n}{anim}\PY{o}{.}\PY{n}{to\PYZus{}jshtml}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.HTML object>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_154_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{ODEs (damped harmonic
oscillator)}\label{odes-damped-harmonic-oscillator}

    We will now look at the application of our RNN to an ordinary
differential equation, namely, the damped harmonic oscillator. The force
driving the oscillator is

\[F(t) = -c \frac{dx}{dt} - kx,\]

where \(k\) is the spring constant, and \(c\) is the friction
coefficient. The equation of motion for this system then becomes

\[m \frac{d^2 x}{dt^2} + c \frac{dx}{dt} + kx = 0,\]

with \(m\) being the mass of the oscillating object. Let's implement
this in python.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Parameters for the damped harmonic oscillator}
\PY{n}{m} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{c} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{n}{k} \PY{o}{=} \PY{l+m+mf}{1.0}

\PY{k}{def} \PY{n+nf}{F}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{v}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} The damped harmonic oscillator force. \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{return} \PY{o}{\PYZhy{}}\PY{n}{c}\PY{o}{*}\PY{n}{v} \PY{o}{\PYZhy{}}\PY{n}{k}\PY{o}{*}\PY{n}{x}
\end{Verbatim}
\end{tcolorbox}

    We can solve this differential equation using Euler-Cromer, in order to
get the target output for our RNN model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Set time array}
\PY{n}{t\PYZus{}max} \PY{o}{=} \PY{l+m+mi}{30}
\PY{n}{dt} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{n}{t} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{t\PYZus{}max}\PY{p}{,} \PY{n}{dt}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Initialize position and velocity arrays}
\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{t}\PY{p}{)}
\PY{n}{v} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{t}\PY{p}{)}
\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{v}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}  \PY{c+c1}{\PYZsh{} Initial conditions}

\PY{c+c1}{\PYZsh{} Euler\PYZhy{}Cromer}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    \PY{n}{a} \PY{o}{=} \PY{n}{F}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{v}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)} \PY{o}{/} \PY{n}{m}
    \PY{n}{v}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{v}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+} \PY{n}{a} \PY{o}{*} \PY{n}{dt}
    \PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+} \PY{n}{v}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{dt}
\end{Verbatim}
\end{tcolorbox}

    Now, we can set up our network, and train it on the differential
equation. We will use the position as input, and for each \(x_i\) the
target output will be \(x_{i+1}\), as we want our model to predict the
position at the next time step. We also reshape the data to be three
dimensional (batch size, sequence length, number of features), even
though batch size and number of features are both \(1\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X} \PY{o}{=} \PY{n}{x}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{c+c1}{\PYZsh{} Input to model}
\PY{n}{y} \PY{o}{=} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]} \PY{c+c1}{\PYZsh{} Target output}

\PY{c+c1}{\PYZsh{} Reshape to fit RNN}
\PY{n}{X} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
\PY{n}{y} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    Let's set up the parameters we need for the RNN.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{eta} \PY{o}{=} \PY{l+m+mf}{0.001} \PY{c+c1}{\PYZsh{} Learning rate}
\PY{n}{lmbd} \PY{o}{=} \PY{l+m+mf}{0.0001} \PY{c+c1}{\PYZsh{} Regularization parameter}
\PY{n}{n\PYZus{}features\PYZus{}hidden} \PY{o}{=} \PY{l+m+mi}{3} \PY{c+c1}{\PYZsh{} Features in the RNN layer}

\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{10}
\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n}{batches} \PY{o}{=} \PY{l+m+mi}{1}

\PY{n}{scheduler} \PY{o}{=} \PY{n}{Adam}\PY{p}{(}\PY{n}{eta}\PY{p}{,} \PY{l+m+mf}{0.9}\PY{p}{,} \PY{l+m+mf}{0.999}\PY{p}{)}
\PY{n}{act\PYZus{}func\PYZus{}hidden} \PY{o}{=} \PY{n}{tanh}
\PY{n}{act\PYZus{}func\PYZus{}output} \PY{o}{=} \PY{n}{identity}
\PY{n}{cost\PYZus{}func} \PY{o}{=} \PY{n}{CostOLS}
\end{Verbatim}
\end{tcolorbox}

    We can now create the network and train it.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{rnn} \PY{o}{=} \PY{n}{RNN}\PY{p}{(}\PY{n}{cost\PYZus{}func}\PY{p}{,} \PY{n}{scheduler}\PY{p}{)}
\PY{n}{rnn}\PY{o}{.}\PY{n}{add\PYZus{}InputLayer}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{rnn}\PY{o}{.}\PY{n}{add\PYZus{}RNNLayer}\PY{p}{(}\PY{n}{n\PYZus{}features\PYZus{}hidden}\PY{p}{,} \PY{n}{act\PYZus{}func\PYZus{}hidden}\PY{p}{)}
\PY{n}{rnn}\PY{o}{.}\PY{n}{add\PYZus{}OutputLayer}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{act\PYZus{}func\PYZus{}output}\PY{p}{)}
\PY{n}{scores} \PY{o}{=} \PY{n}{rnn}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{batches}\PY{o}{=}\PY{n}{batches}\PY{p}{,} \PY{n}{lmbd}\PY{o}{=}\PY{n}{lmbd}\PY{p}{,} \PY{n}{store\PYZus{}output}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{output} \PY{o}{=} \PY{n}{rnn}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
EPOCH: 10/10
    \end{Verbatim}

    Lastly, we plot the error during training\ldots{}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{epoch\PYZus{}arr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{epochs}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epoch\PYZus{}arr}\PY{p}{,} \PY{n}{scores}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train\PYZus{}error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{My error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epochs}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_167_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \ldots and see how our prediction fits to the target data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{t}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True Values}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{t}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{output}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predictions}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Time \PYZdl{}t\PYZdl{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Position \PYZdl{}x\PYZdl{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_169_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Handwritten digits
(MNIST)}\label{handwritten-digits-mnist}

    Let us explore the \href{http://yann.lecun.com/exdb/mnist}{MNIST
handwritten digit database} by LeCun et al.~We start by obtaining the
dataset using tensorflow. Each input value is a number in the range
\([0,255]\), so we divide the input values by \(255\) to get values in
the range \([0,1]\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{tensorflow}\PY{n+nn}{.}\PY{n+nn}{keras} \PY{k+kn}{import} \PY{n}{datasets}

\PY{c+c1}{\PYZsh{}\PYZsh{} Retrieve and normalize the data (split into train and test)}
\PY{n}{digits} \PY{o}{=} \PY{n}{datasets}\PY{o}{.}\PY{n}{mnist}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{p}{)}
\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{)} \PY{o}{=} \PY{n}{digits}

\PY{c+c1}{\PYZsh{} Normalize data}
\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{/}\PY{l+m+mf}{255.0}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{o}{/}\PY{l+m+mf}{255.0}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(60000, 28, 28)
(60000,)
    \end{Verbatim}

    The training data contains \(60\,000\) images, so to speed ut the
training we will only use a subset of the data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Extract a subset of the data}
\PY{n}{data\PYZus{}frac} \PY{o}{=} \PY{l+m+mf}{0.01} \PY{c+c1}{\PYZsh{} Fraction of data to use}

\PY{n}{n\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{n\PYZus{}val} \PY{o}{=} \PY{n}{X\PYZus{}val}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{train\PYZus{}end} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{n\PYZus{}train}\PY{o}{*}\PY{n}{data\PYZus{}frac}\PY{p}{)}
\PY{n}{val\PYZus{}end} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{n\PYZus{}val}\PY{o}{*}\PY{n}{data\PYZus{}frac}\PY{p}{)}

\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{n}{train\PYZus{}end}\PY{p}{]}
\PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{X\PYZus{}val}\PY{p}{[}\PY{p}{:}\PY{n}{val\PYZus{}end}\PY{p}{]}
\PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{n}{train\PYZus{}end}\PY{p}{]}
\PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{y\PYZus{}val}\PY{p}{[}\PY{p}{:}\PY{n}{val\PYZus{}end}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    Next, the target values are integers ranging from \(0\) to \(9\), but we
need them to be \(10\)-element arrays with a \(1\) in the index
corresponding to the number, and a \(0\) elsewhere.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{tensorflow}\PY{n+nn}{.}\PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k+kn}{import} \PY{n}{to\PYZus{}categorical}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First three training targets before changing to array:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Transforming the labels from a single digit to an array of length 10 with the digit corresponding to the index}
\PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}
\PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First three training targets after changing to array:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
First three training targets before changing to array:
[5 0 4]
First three training targets after changing to array:
[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
    \end{Verbatim}

    Let's see what the data looks like. We will plot some of the handwritten
digits along with the target value.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} Plot some images for visual understanding}
\PY{n}{n\PYZus{}rows} \PY{o}{=} \PY{l+m+mi}{5}
\PY{n}{n\PYZus{}cols} \PY{o}{=} \PY{l+m+mi}{5}
\PY{n}{fig}\PY{p}{,} \PY{n}{axs} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{n\PYZus{}rows}\PY{p}{,} \PY{n}{n\PYZus{}cols}\PY{p}{,} \PY{n}{constrained\PYZus{}layout}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{k}{for} \PY{n}{row} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}rows}\PY{p}{)}\PY{p}{:}
    \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}cols}\PY{p}{)}\PY{p}{:}
        \PY{n}{ax} \PY{o}{=} \PY{n}{axs}\PY{p}{[}\PY{n}{row}\PY{p}{,}\PY{n}{col}\PY{p}{]}
        \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}axis\PYZus{}off}\PY{p}{(}\PY{p}{)}
        \PY{n}{ind} \PY{o}{=} \PY{n}{n\PYZus{}cols} \PY{o}{*} \PY{n}{row} \PY{o}{+} \PY{n}{col}
        \PY{n}{label} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{ind}\PY{p}{]}\PY{p}{)}
        \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{n}{label}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{left}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{ax}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{ind}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_178_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We can now build the RNN model\ldots{}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} Set RNN parameters}
\PY{n}{eta} \PY{o}{=} \PY{l+m+mf}{0.001}
\PY{n}{lmbd} \PY{o}{=} \PY{l+m+mf}{0.001}
\PY{n}{seed} \PY{o}{=} \PY{l+m+mi}{100}

\PY{n}{act\PYZus{}func\PYZus{}hidden} \PY{o}{=} \PY{n}{RELU}
\PY{n}{act\PYZus{}func\PYZus{}output} \PY{o}{=} \PY{n}{softmax}
\PY{n}{cost\PYZus{}func} \PY{o}{=} \PY{n}{CostLogReg}
\PY{n}{scheduler} \PY{o}{=} \PY{n}{Adam}\PY{p}{(}\PY{n}{eta}\PY{p}{,} \PY{l+m+mf}{0.9}\PY{p}{,} \PY{l+m+mf}{0.999}\PY{p}{)}

\PY{n}{n\PYZus{}features\PYZus{}input} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
\PY{n}{n\PYZus{}features\PYZus{}hidden} \PY{o}{=} \PY{l+m+mi}{300}
\PY{n}{n\PYZus{}features\PYZus{}output} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}

\PY{c+c1}{\PYZsh{}\PYZsh{} Create RNN}
\PY{n}{rnn} \PY{o}{=} \PY{n}{RNN}\PY{p}{(}\PY{n}{cost\PYZus{}func}\PY{p}{,} \PY{n}{scheduler}\PY{p}{,} \PY{n}{seed}\PY{p}{)}
\PY{n}{rnn}\PY{o}{.}\PY{n}{add\PYZus{}InputLayer}\PY{p}{(}\PY{n}{n\PYZus{}features\PYZus{}input}\PY{p}{)}
\PY{n}{rnn}\PY{o}{.}\PY{n}{add\PYZus{}RNNLayer}\PY{p}{(}\PY{n}{n\PYZus{}features\PYZus{}hidden}\PY{p}{,} \PY{n}{act\PYZus{}func\PYZus{}hidden}\PY{p}{)}
\PY{n}{rnn}\PY{o}{.}\PY{n}{add\PYZus{}DenseLayer}\PY{p}{(}\PY{n}{n\PYZus{}features\PYZus{}output}\PY{p}{,} \PY{n}{act\PYZus{}func\PYZus{}output}\PY{p}{,} \PY{n}{is\PYZus{}last\PYZus{}layer}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \ldots train it on the MNIST dataset\ldots{}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} Train network}
\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{20}
\PY{n}{batches} \PY{o}{=} \PY{l+m+mi}{60}
\PY{n}{scores} \PY{o}{=} \PY{n}{rnn}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{epochs}\PY{p}{,} \PY{n}{batches}\PY{p}{,} \PY{n}{lmbd}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
EPOCH: 20/20
    \end{Verbatim}

    \ldots and read out the scores.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} Extract output}
\PY{n}{epoch\PYZus{}arr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{epochs}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{train\PYZus{}error} \PY{o}{=} \PY{n}{scores}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train\PYZus{}error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{val\PYZus{}error} \PY{o}{=} \PY{n}{scores}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{val\PYZus{}error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{train\PYZus{}accuracy} \PY{o}{=} \PY{n}{scores}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train\PYZus{}accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{val\PYZus{}accuracy} \PY{o}{=} \PY{n}{scores}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{val\PYZus{}accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    We then plot the error during training\ldots{}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} Plot error during training}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epoch\PYZus{}arr}\PY{p}{,} \PY{n}{train\PYZus{}error}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epoch\PYZus{}arr}\PY{p}{,} \PY{n}{val\PYZus{}error}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Validation error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_186_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \ldots and the accuracy

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} Plot accuracy during training}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epoch\PYZus{}arr}\PY{p}{,} \PY{n}{train\PYZus{}accuracy}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epoch\PYZus{}arr}\PY{p}{,} \PY{n}{val\PYZus{}accuracy}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Validation accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_188_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training accuracy: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}accuracy}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{l+s+si}{:}\PY{l+s+s2}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Validation accuracy: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}accuracy}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{l+s+si}{:}\PY{l+s+s2}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Training accuracy: 0.59
Validation accuracy: 0.52
    \end{Verbatim}

    The model successfully predicts \(52\%\) of the digits in the validation
data. This isn't very much, so feel free to adjust the network
parameters or the number of layers and nodes, and try to improve the
results.

Let's end with looking at some of the MNIST digits again, this time
along with our model's prediction.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} Plot some images for visual understanding}
\PY{n}{pred} \PY{o}{=} \PY{n}{rnn}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}
\PY{n}{n\PYZus{}rows} \PY{o}{=} \PY{l+m+mi}{5}
\PY{n}{n\PYZus{}cols} \PY{o}{=} \PY{l+m+mi}{5}
\PY{n}{fig}\PY{p}{,} \PY{n}{axs} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{n\PYZus{}rows}\PY{p}{,} \PY{n}{n\PYZus{}cols}\PY{p}{,} \PY{n}{constrained\PYZus{}layout}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{k}{for} \PY{n}{row} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}rows}\PY{p}{)}\PY{p}{:}
    \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}cols}\PY{p}{)}\PY{p}{:}
        \PY{n}{ax} \PY{o}{=} \PY{n}{axs}\PY{p}{[}\PY{n}{row}\PY{p}{,}\PY{n}{col}\PY{p}{]}
        \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}axis\PYZus{}off}\PY{p}{(}\PY{p}{)}
        \PY{n}{ind} \PY{o}{=} \PY{n}{n\PYZus{}cols} \PY{o}{*} \PY{n}{row} \PY{o}{+} \PY{n}{col}
        \PY{n}{label} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{n}{ind}\PY{p}{]}\PY{p}{)}
        \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{n}{label}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{center}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{ax}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{[}\PY{n}{ind}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_191_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
