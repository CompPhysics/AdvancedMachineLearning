% !TEX program = pdflatex
\documentclass[aspectratio=169]{beamer}

\usetheme{Madrid}
\usecolortheme{default}

\usepackage{amsmath,amssymb,bm,mathtools}
\usepackage{physics}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{graphicx}

% -----------------------
% Listings (PyTorch)
% -----------------------
\definecolor{codebg}{RGB}{245,245,245}
\definecolor{codeframe}{RGB}{210,210,210}
\lstset{
  language=Python,
  basicstyle=\ttfamily\scriptsize,
  backgroundcolor=\color{codebg},
  frame=single,
  rulecolor=\color{codeframe},
  breaklines=true,
  showstringspaces=false,
  tabsize=2
}

% -----------------------
% Macros
% -----------------------
\newcommand{\I}{\mathbb{I}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\sigma_x}
\newcommand{\Y}{\sigma_y}
\newcommand{\Z}{\sigma_z}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\sr}{\rho} % spectral radius

\title[RNN Mini-Course]{Recurrent Neural Networks:\\Mathematics, Dynamics, and Training (Vanilla RNNs)}
\author{Graduate mini-course (3--4$\times$45 min)}
\date{}

\begin{document}

% =========================================================
% 0) Title + roadmap
% =========================================================
\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Learning goals}
By the end you should be able to:
\begin{itemize}
\item Write and manipulate the vanilla RNN equations as a discrete-time dynamical system.
\item Derive Backpropagation Through Time (BPTT) and identify where vanishing/exploding gradients come from.
\item Use linearization and spectral arguments to analyze stability and memory.
\item Implement a minimal RNN in PyTorch and train it on sequence prediction and classification.
\item Diagnose failure modes (optimization, identifiability, data leakage) and apply practical fixes (clipping, initialization, truncation).
\end{itemize}
\end{frame}

\begin{frame}{Structure (4 lectures)}
\textbf{Lecture 1: Foundations \& notation}
\begin{itemize}
\item Sequences in physical sciences; probabilistic framing
\item Vanilla RNN definition; unrolling; parameter sharing
\item Linear RNN closed forms; impulse response and memory
\end{itemize}

\medskip
\textbf{Lecture 2: RNNs as dynamical systems}
\begin{itemize}
\item Fixed points, Jacobians, stability criteria
\item Contraction maps (proof); Lyapunov-style intuition
\item Echo state / spectral radius heuristics (for vanilla RNNs)
\end{itemize}

\medskip
\textbf{Lecture 3: Training mathematics}
\begin{itemize}
\item BPTT derivation (proof-level)
\item Vanishing/exploding gradients (spectral bound proof)
\item Truncated BPTT; clipping; initialization
\end{itemize}

\medskip
\textbf{Lecture 4: Worked examples \& PyTorch}
\begin{itemize}
\item Sine-wave prediction; AR processes; noisy oscillators
\item Sequence classification (peak counting / regimes)
\item Diagnostics, ablations, and exercises
\end{itemize}
\end{frame}

% =========================================================
\section{Lecture 1: Foundations}
% =========================================================

\begin{frame}{Why RNNs in the physical sciences?}
Many data sets are \textbf{ordered} and \textbf{correlated}:
\begin{itemize}
\item detector readout streams, time-of-flight, waveform traces
\item climate/geo time series; turbulence probes; seismology
\item molecular dynamics trajectories; spectroscopy scans
\item state estimation in control/feedback; sensor fusion
\end{itemize}
We want models for
\[
p(x_{1:T}) \quad\text{or}\quad p(y_{1:T}\mid x_{1:T}) \quad\text{or}\quad p(y\mid x_{1:T}).
\]
\end{frame}

\begin{frame}{Sequence modeling as conditional probability}
Two canonical tasks:
\begin{itemize}
\item \textbf{Sequence-to-sequence:} $x_{1:T}\mapsto y_{1:T}$ (filtering, forecasting).
\item \textbf{Sequence-to-label:} $x_{1:T}\mapsto y$ (classification, regime detection).
\end{itemize}
A Markovian latent-state view:
\[
h_t = f_\theta(h_{t-1},x_t),\qquad y_t = g_\theta(h_t).
\]
RNNs provide a \emph{parameterized} $f_\theta,g_\theta$.
\end{frame}

\begin{frame}{Notation}
Sequence length $T$.
\[
x_t\in\R^{n},\quad h_t\in\R^{m},\quad y_t\in\R^{p}.
\]
Parameters:
\[
W_h\in\R^{m\times m},\; W_x\in\R^{m\times n},\; b\in\R^m,\; W_y\in\R^{p\times m},\; c\in\R^p.
\]
Nonlinearity $\sigma$ applied elementwise (e.g. $\tanh$).
\end{frame}

\begin{frame}{Vanilla RNN definition}
\[
a_t = W_h h_{t-1} + W_x x_t + b,\qquad h_t = \sigma(a_t)
\]
\[
o_t = W_y h_t + c,\qquad \hat{y}_t = \phi(o_t)
\]
where $\phi$ depends on the task (identity for regression, softmax for classification).
\end{frame}

\begin{frame}{Unrolling: RNN as a deep network with shared weights}
The computation graph expands in time:
\[
h_t = \sigma(W_h h_{t-1} + W_x x_t + b)
\]
so $h_t$ is a nested composition of $\sigma$ and affine maps back to $h_0$.
\medskip

\textbf{Key:} depth grows with $T$, but parameters are \emph{tied} across time.
\end{frame}

\begin{frame}{Computational graph picture (conceptual)}
Unrolled for $t=1,\dots,T$:
\[
x_1\to h_1\to h_2\to\cdots\to h_T
\]
with recurrent edges carrying $h_{t-1}\to h_t$.
\medskip

This is why training uses \textbf{Backpropagation Through Time (BPTT)}.
\end{frame}

\begin{frame}{A physics-friendly viewpoint: latent state}
RNN hidden state $h_t$ plays the role of:
\begin{itemize}
\item a \textbf{coarse-grained state} (like reduced coordinates),
\item a \textbf{sufficient statistic} for prediction,
\item a learned \textbf{filter} of the past.
\end{itemize}
Compare: Kalman filter state, hidden Markov model state, Mori--Zwanzig projection.
\end{frame}

\begin{frame}{Linear RNN as a baseline (no nonlinearity)}
Let $\sigma(z)=z$. Then
\[
h_t = W_h h_{t-1}+W_x x_t + b.
\]
This is a vector autoregressive (VAR) state-space model.
Closed-form solution (assuming $h_0=0$ for clarity):
\[
h_t = \sum_{k=0}^{t-1} W_h^k (W_x x_{t-k} + b).
\]
\end{frame}

\begin{frame}{Impulse response and memory kernel}
For linear RNN:
\[
h_t = \sum_{k\ge 0} W_h^k W_x x_{t-k} + \sum_{k\ge 0} W_h^k b.
\]
So the model is a \textbf{convolution in time} with matrix-valued kernel $W_h^k W_x$.
Memory decays if $\|W_h^k\|$ decays.
\end{frame}

\begin{frame}{Spectral radius controls decay (linear case)}
If $W_h$ is diagonalizable: $W_h = V\Lambda V^{-1}$,
\[
W_h^k = V\Lambda^k V^{-1}.
\]
Then $\|W_h^k\|$ behaves roughly like $\sr(W_h)^k$ (up to conditioning).
\[
\sr(W_h) < 1 \Rightarrow \text{exponentially fading memory.}
\]
\[
\sr(W_h) > 1 \Rightarrow \text{unstable growth unless controlled by nonlinearity.}
\]
\end{frame}

\begin{frame}{Nonlinearity: why $\tanh$ is common}
$\tanh$:
\begin{itemize}
\item bounded state: $h_t\in(-1,1)^m$,
\item smooth derivative: $0<\tanh'(z)\le 1$,
\item odd symmetry.
\end{itemize}
But boundedness alone does \emph{not} prevent exploding gradients (training issue).
\end{frame}

\begin{frame}{Readout choices for physical tasks}
\begin{itemize}
\item Regression/forecast: $\phi = \mathrm{id}$, loss $\ell=\frac12\|y_t-\hat y_t\|^2$.
\item Binary classification: $\phi=\sigma_{\text{sigmoid}}$, BCE loss.
\item Multi-class classification: $\phi=\mathrm{softmax}$, cross-entropy.
\end{itemize}
Often we only output at final time: $\hat y = \phi(W_y h_T+c)$.
\end{frame}

\begin{frame}{Worked toy example (by hand): $m=1$ linear RNN}
Scalar:
\[
h_t = \alpha h_{t-1} + \beta x_t,\quad h_0=0.
\]
Then
\[
h_T = \beta \sum_{k=0}^{T-1} \alpha^k x_{T-k}.
\]
This is an exponentially weighted moving average.
\medskip

\textbf{Takeaway:} even the simplest RNN implements a physically interpretable memory kernel.
\end{frame}

\begin{frame}{Exercise set 1}
\begin{enumerate}
\item For scalar linear RNN above, compute $h_T$ for $x_t=\sin(\omega t)$ and analyze amplitude vs $\alpha$.
\item Show that if $|\alpha|<1$, the contribution of $x_{t-k}$ decays as $|\alpha|^k$.
\item If $x_t$ is white noise, compute $\Var(h_t)$ (stationary if $|\alpha|<1$).
\end{enumerate}
\end{frame}

% =========================================================
\section{Lecture 2: Dynamical systems}
% =========================================================

\begin{frame}{Autonomous RNN as a discrete-time dynamical system}
With $x_t=0$ (or constant), the update is
\[
h_t = F(h_{t-1}) := \sigma(W_h h_{t-1}+b).
\]
We can analyze:
\begin{itemize}
\item fixed points $h^\star = F(h^\star)$,
\item stability via Jacobian,
\item attractors / limit cycles (in high dimensions).
\end{itemize}
\end{frame}

\begin{frame}{Fixed points}
A fixed point $h^\star$ satisfies
\[
h^\star = \sigma(W_h h^\star + b).
\]
Even existence/uniqueness can be nontrivial for nonlinear $\sigma$.
We’ll give a sufficient condition for uniqueness using contraction mapping.
\end{frame}

\begin{frame}{Jacobian of the RNN map}
Let $a = W_h h + b$ and $h^+=\sigma(a)$.
Then
\[
J(h) := \frac{\partial h^+}{\partial h}
= \diag(\sigma'(a))\, W_h.
\]
For $\tanh$, $\sigma'(a)=1-\tanh^2(a)\in(0,1]$.
\end{frame}

\begin{frame}{Local stability criterion (standard theorem)}
If $h^\star$ is a fixed point and
\[
\sr\big(J(h^\star)\big) < 1,
\]
then $h^\star$ is locally asymptotically stable (small perturbations decay).
If $\sr(J(h^\star))>1$, it is unstable.
\end{frame}

\begin{frame}{Proof sketch: linearization}
Let $\delta_t = h_t - h^\star$.
First-order Taylor:
\[
\delta_{t} = J(h^\star)\delta_{t-1} + \mathcal{O}(\|\delta_{t-1}\|^2).
\]
If $\sr(J(h^\star))<1$, then $J^t\to 0$, so $\delta_t\to 0$ for small enough initial perturbation.
\end{frame}

\begin{frame}{Contraction mapping theorem (tool)}
Let $(\mathcal{X},\|\cdot\|)$ be complete.
If $F:\mathcal{X}\to\mathcal{X}$ and there exists $L<1$ such that
\[
\|F(u)-F(v)\|\le L\|u-v\|,\quad \forall u,v\in\mathcal{X},
\]
then:
\begin{itemize}
\item $F$ has a unique fixed point $u^\star$,
\item iterating $u_{t+1}=F(u_t)$ converges to $u^\star$ for any start.
\end{itemize}
\end{frame}

\begin{frame}{Proof: sufficient condition for global contraction}
For $F(h)=\sigma(W_h h+b)$ with Lipschitz $\sigma$:
if $\sigma$ is $L_\sigma$-Lipschitz, then
\[
\|F(u)-F(v)\| = \|\sigma(W_h u+b)-\sigma(W_h v+b)\|
\le L_\sigma \|W_h(u-v)\|
\le L_\sigma \|W_h\|\,\|u-v\|.
\]
So $F$ is a contraction if
\[
L_\sigma \|W_h\| < 1.
\]
For $\tanh$, $L_\sigma=1$ (in Euclidean norm).
\end{frame}

\begin{frame}{Implication: unique stable fixed point regime}
If $\|W_h\|<1$ (or more generally $L_\sigma\|W_h\|<1$), then:
\begin{itemize}
\item the autonomous RNN has a unique fixed point,
\item the hidden dynamics are globally stable,
\item memory fades quickly (good for stability, bad for long dependencies).
\end{itemize}
This foreshadows the vanishing gradient problem.
\end{frame}

\begin{frame}{Driven system: input as forcing}
With input:
\[
h_t = \sigma(W_h h_{t-1} + W_x x_t + b).
\]
This is a nonlinear forced system.
In physics language: $W_x x_t$ is an external drive.
Questions:
\begin{itemize}
\item does the system forget initial conditions?
\item how sensitive is $h_t$ to old inputs?
\end{itemize}
\end{frame}

\begin{frame}{Echo-state style heuristic (vanilla RNN)}
Heuristic for stable driven dynamics and usable memory:
\[
\sr(W_h)\approx 1 \quad (\text{not too small, not too large}).
\]
Too small: fades too quickly.
Too large: chaotic/unstable internal dynamics (and training issues).
\end{frame}

\begin{frame}{Memory and observability (conceptual)}
RNN state $h_t$ is a compressed summary of past inputs.
Good performance requires:
\begin{itemize}
\item \textbf{separability:} different histories produce separable $h_t$,
\item \textbf{robustness:} small noise in $x$ doesn’t destroy $h_t$.
\end{itemize}
This is akin to observability/controllability tradeoffs in state-space models.
\end{frame}

\begin{frame}{Worked example: stable vs unstable scalar nonlinear RNN}
Scalar autonomous:
\[
h_{t}=\tanh(\alpha h_{t-1}).
\]
Fixed point $h^\star=0$ always exists.
Jacobian at $0$: $J(0)=\alpha$ (since $\tanh'(0)=1$).
Thus:
\[
|\alpha|<1 \Rightarrow 0 \text{ stable},\qquad |\alpha|>1 \Rightarrow 0 \text{ unstable}.
\]
For $|\alpha|>1$, additional nonzero fixed points emerge (bifurcation).
\end{frame}

\begin{frame}{Exercise set 2}
\begin{enumerate}
\item For scalar $h=\tanh(\alpha h)$, show that nonzero fixed points appear for $|\alpha|>1$.
\item Simulate $h_{t}=\tanh(\alpha h_{t-1})$ for $\alpha=0.5,1.2,2$ and different initial conditions.
\item Relate stability to memory in the driven case $h_t=\tanh(\alpha h_{t-1}+\beta x_t)$.
\end{enumerate}
\end{frame}

% =========================================================
\section{Lecture 3: Training mathematics (BPTT)}
% =========================================================

\begin{frame}{Training objective (general)}
Given targets $\{y_t\}_{t=1}^T$ and predictions $\{\hat y_t\}$:
\[
\mathcal{L}(\theta)=\sum_{t=1}^T \ell(\hat y_t, y_t)
\]
with $\theta=\{W_h,W_x,b,W_y,c\}$.
We seek $\nabla_\theta \mathcal{L}$.
\end{frame}

\begin{frame}{BPTT: main idea}
Unroll the network across time and apply backpropagation.
Challenge: parameters are shared across all time steps, so gradients sum over time contributions.
\end{frame}

\begin{frame}{Core recurrence for backprop}
Define pre-activation $a_t = W_h h_{t-1} + W_x x_t + b$ and $h_t=\sigma(a_t)$.
Let
\[
\delta_t := \frac{\partial \mathcal{L}}{\partial a_t}\in\R^m.
\]
We will derive a backward recursion for $\delta_t$.
\end{frame}

\begin{frame}{Derivation: chain rule structure}
Loss at time $t$ depends on $a_t$ via $h_t$ and then possibly future states.
\[
\frac{\partial \mathcal{L}}{\partial a_t}
=
\underbrace{\frac{\partial \mathcal{L}}{\partial h_t}\odot \sigma'(a_t)}_{\text{local}}
\quad + \quad
\underbrace{\left(\frac{\partial \mathcal{L}}{\partial a_{t+1}}\right)\frac{\partial a_{t+1}}{\partial h_t}\odot \sigma'(a_t)}_{\text{through future}}.
\]
Since $a_{t+1}=W_h h_t + \cdots$, we have $\partial a_{t+1}/\partial h_t = W_h$.
\end{frame}

\begin{frame}{BPTT recursion (clean form)}
Let $g_t:=\partial \mathcal{L}/\partial h_t$ (accumulated from output loss).
Then
\[
\delta_t = \left(g_t + W_h^\top \delta_{t+1}\right)\odot \sigma'(a_t),
\quad t=T,\dots,1,
\]
with terminal $\delta_{T+1}:=0$.
This is the fundamental BPTT equation.
\end{frame}

\begin{frame}{Gradients w.r.t. parameters}
Using $a_t=W_h h_{t-1}+W_x x_t+b$:
\[
\frac{\partial \mathcal{L}}{\partial W_h}=\sum_{t=1}^T \delta_t\, h_{t-1}^\top,
\quad
\frac{\partial \mathcal{L}}{\partial W_x}=\sum_{t=1}^T \delta_t\, x_t^\top,
\quad
\frac{\partial \mathcal{L}}{\partial b}=\sum_{t=1}^T \delta_t.
\]
For readout (if $\hat y_t$ depends on $o_t=W_y h_t+c$):
\[
\frac{\partial \mathcal{L}}{\partial W_y}=\sum_t \frac{\partial \mathcal{L}}{\partial o_t} h_t^\top,\qquad
\frac{\partial \mathcal{L}}{\partial c}=\sum_t \frac{\partial \mathcal{L}}{\partial o_t}.
\]
\end{frame}

\begin{frame}{Where vanishing/exploding gradients come from}
Consider the sensitivity of $h_T$ to $h_{t}$:
\[
\frac{\partial h_T}{\partial h_t} =
\prod_{k=t+1}^{T} J_k,\qquad
J_k := \frac{\partial h_k}{\partial h_{k-1}} = \diag(\sigma'(a_k))W_h.
\]
Products of many Jacobians drive gradient magnitude.
\end{frame}

\begin{frame}{A spectral bound (proof-level)}
Using submultiplicativity of operator norm:
\[
\left\|\prod_{k=t+1}^{T} J_k\right\|
\le \prod_{k=t+1}^{T} \|J_k\|.
\]
If $\|J_k\|\le \gamma<1$ uniformly, then
\[
\left\|\frac{\partial h_T}{\partial h_t}\right\| \le \gamma^{T-t}\to 0
\quad\Rightarrow\quad \text{vanishing gradients.}
\]
If $\|J_k\|\ge \gamma>1$ often, then the product can blow up $\Rightarrow$ exploding gradients.
\end{frame}

\begin{frame}{Relating $J_k$ to $W_h$ and $\sigma'$}
For $\tanh$: $\sigma'(z)=1-\tanh^2(z)\in(0,1]$, so $\|\diag(\sigma'(a_k))\|\le 1$.
Thus
\[
\|J_k\| \le \|W_h\|.
\]
So if $\|W_h\|<1$, vanishing gradients are typical; if $\|W_h\|>1$, exploding gradients become possible.
This ties training pathology to dynamical stability regimes.
\end{frame}

\begin{frame}{Linear RNN: exact gradient growth/decay}
If $\sigma$ is identity:
\[
h_t=W_h h_{t-1}+W_x x_t,
\quad\Rightarrow\quad
\frac{\partial h_T}{\partial h_t}=W_h^{T-t}.
\]
Then
\[
\|W_h^{T-t}\|\approx \sr(W_h)^{T-t}
\]
(up to conditioning).
So $\sr(W_h)$ controls gradient magnitude exactly in the linear case.
\end{frame}

\begin{frame}{Practical fixes (vanilla RNN)}
Common mitigations (still within vanilla RNN):
\begin{itemize}
\item \textbf{Gradient clipping:} cap $\|\nabla\|$ to avoid explosion.
\item \textbf{Orthogonal / scaled init:} initialize $W_h$ with $\sr(W_h)\approx 1$.
\item \textbf{Truncated BPTT:} backprop only $K$ steps (controls compute and stabilizes).
\item \textbf{Input/target normalization:} helps conditioning.
\end{itemize}
We will implement these in PyTorch.
\end{frame}

\begin{frame}{Truncated BPTT (mathematical statement)}
Instead of full $T$-step gradient, approximate:
\[
\frac{\partial \mathcal{L}}{\partial \theta}
\approx
\sum_{t=1}^{T} \sum_{k=0}^{K-1}
\frac{\partial \ell_t}{\partial h_t}
\frac{\partial h_t}{\partial h_{t-k}}
\frac{\partial h_{t-k}}{\partial \theta}.
\]
This biases gradients but can improve optimization stability.
\end{frame}

\end{document}


\begin{frame}{Initialization: orthogonal recurrent matrix}
For stability and gradient flow, initialize the recurrent weights close to orthogonal:
\[
W_h^\top W_h \approx I \quad\Rightarrow\quad \sr(W_h)\approx 1.
\]
In PyTorch:
\begin{lstlisting}
def init_orthogonal_rnn(model, gain=1.0):
    for name, p in model.named_parameters():
        if "Wh" in name and p.dim() == 2:
            nn.init.orthogonal_(p, gain=gain)
\end{lstlisting}
\end{frame}

\begin{frame}{Gradient clipping (implementation)}
\begin{lstlisting}
max_norm = 1.0
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
\end{lstlisting}
Clipping bounds exploding gradients; it does not fix vanishing gradients, but helps training stability.
\end{frame}

% ---------------- Worked example 1: sine forecasting ----------------

\begin{frame}{Worked example 1: noisy sine forecasting}
Task: given past samples, predict next samples.
Generate sequences:
\[
x_t = \sin(\omega t + \phi) + \epsilon_t,\quad \epsilon_t\sim\mathcal{N}(0,\sigma^2)
\]
Train to predict $x_{t+1}$.
This is a controlled test of memory and stability.
\end{frame}

\begin{frame}{Data generation (PyTorch)}
\begin{lstlisting}
import math
def make_sine_batch(B, T, sigma=0.05, device="cpu"):
    w = torch.rand(B, 1, device=device) * 1.0 + 0.2
    phi = torch.rand(B, 1, device=device) * 2*math.pi
    t = torch.arange(T+1, device=device).float()[None, :]
    x = torch.sin(w * t + phi)
    x = x + sigma * torch.randn_like(x)
    # input: first T, target: next T
    x_in = x[:, :T].unsqueeze(-1)       # (B,T,1)
    x_tgt = x[:, 1:T+1].unsqueeze(-1)   # (B,T,1)
    return x_in, x_tgt
\end{lstlisting}
\end{frame}

\begin{frame}{Training loop (regression)}
\begin{lstlisting}
import torch.optim as optim
loss_fn = nn.MSELoss()
opt = optim.Adam(model.parameters(), lr=3e-3)

for step in range(2000):
    x_in, x_tgt = make_sine_batch(B=128, T=60, device=device)
    y_pred, _ = model(x_in)
    loss = loss_fn(y_pred, x_tgt)
    opt.zero_grad(set_to_none=True)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    opt.step()
\end{lstlisting}
\end{frame}

\begin{frame}{What to monitor in this experiment}
\begin{itemize}
\item Training/validation MSE vs steps.
\item Gradient norm $\|\nabla\|$ (detect explosion/vanishing).
\item Sensitivity to sequence length $T$.
\item Effect of $\sr(W_h)$ via initialization gain.
\end{itemize}
\end{frame}

\begin{frame}{Diagnostic: gradient norm logging}
\begin{lstlisting}
def grad_norm(model):
    total = 0.0
    for p in model.parameters():
        if p.grad is not None:
            total += p.grad.detach().norm(2).item()**2
    return total**0.5
\end{lstlisting}
Use: record `grad_norm(model)` each step (or every N steps).
\end{frame}

% ---------------- Worked example 2: AR(1) / OU-like ----------------

\begin{frame}{Worked example 2: AR(1) / OU-like process}
Discrete OU / AR(1):
\[
x_{t+1} = \alpha x_t + \epsilon_t,\quad |\alpha|<1.
\]
Optimal predictor is linear. Compare:
\begin{itemize}
\item RNN performance vs linear baseline
\item learned effective $\alpha$ behavior
\end{itemize}
This demonstrates when RNNs are over-parameterized.
\end{frame}

\begin{frame}{AR(1) generation and baseline}
\begin{lstlisting}
def make_ar1_batch(B, T, alpha=0.9, sigma=0.1, device="cpu"):
    x = torch.zeros(B, T+1, device=device)
    x[:, 0] = torch.randn(B, device=device)
    for t in range(T):
        x[:, t+1] = alpha * x[:, t] + sigma * torch.randn(B, device=device)
    x_in = x[:, :T].unsqueeze(-1)
    x_tgt = x[:, 1:T+1].unsqueeze(-1)
    return x_in, x_tgt
\end{lstlisting}
Linear baseline: $\hat x_{t+1}=\hat\alpha x_t$.
\end{frame}

% ---------------- Worked example 3: Sequence classification ----------------

\begin{frame}{Worked example 3: sequence-to-label classification}
Physics-style setup: classify sequences by regime.
Example: signal contains 1 vs 2 peaks (or low vs high frequency).
We use the final state $h_T$ and a classifier:
\[
\hat y = \mathrm{sigmoid}(w^\top h_T + c).
\]
\end{frame}

\begin{frame}{Model variant: final-step classifier head}
\begin{lstlisting}
class RNNClassifier(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.rnn = VanillaRNN(input_size, hidden_size, output_size=hidden_size)
        self.head = nn.Linear(hidden_size, 1)

    def forward(self, x):
        _, hT = self.rnn(x)
        logit = self.head(hT).squeeze(-1)
        return logit
\end{lstlisting}
\end{frame}

\begin{frame}{Synthetic data: 1-peak vs 2-peak pulses}
Generate sequences with Gaussian pulses at random times.
Class 0: one pulse; Class 1: two pulses.
This is analogous to peak-counting tasks in detector readouts.
\end{frame}

\begin{frame}{Pulse data generator (PyTorch)}
\begin{lstlisting}
def make_pulse_batch(B, T, two_pulse_prob=0.5, noise=0.02, device="cpu"):
    t = torch.arange(T, device=device).float()[None, :]
    y = (torch.rand(B, device=device) < two_pulse_prob).long()
    x = torch.zeros(B, T, device=device)
    for i in range(B):
        centers = [torch.randint(5, T-5, (1,), device=device).item()]
        if y[i].item() == 1:
            c2 = torch.randint(5, T-5, (1,), device=device).item()
            centers.append(c2)
        for c in centers:
            x[i] += torch.exp(-0.5*((t[0]-c)/2.0)**2)
    x = x + noise*torch.randn_like(x)
    return x.unsqueeze(-1), y
\end{lstlisting}
\end{frame}

\begin{frame}{Training loop (binary classification)}
\begin{lstlisting}
loss_fn = nn.BCEWithLogitsLoss()
opt = torch.optim.Adam(model.parameters(), lr=1e-3)

for step in range(3000):
    x_in, y = make_pulse_batch(256, 80, device=device)
    logit = model(x_in)
    loss = loss_fn(logit, y.float())
    opt.zero_grad(set_to_none=True)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    opt.step()
\end{lstlisting}
Compute accuracy from `sigmoid(logit) > 0.5`.
\end{frame}

% ---------------- Training theory slides: connecting practice ----------------

\begin{frame}{Why classification can still fail with long sequences}
Even if the task is simple, long $T$ can break vanilla RNN training:
\begin{itemize}
\item vanishing gradients: early pulse information can’t influence loss
\item saturation: $\tanh(a_t)\approx \pm 1 \Rightarrow \sigma'(a_t)\approx 0$
\item internal dynamics drift: hidden state loses separability
\end{itemize}
Mitigate via initialization, clipping, normalization, shorter effective horizons (truncated BPTT).
\end{frame}

\begin{frame}{Practical: truncated BPTT in code}
Idea: process in chunks of length $K$ and detach hidden state:
\begin{lstlisting}
h = torch.zeros(B, H, device=device)
for chunk in range(0, T, K):
    x_chunk = x[:, chunk:chunk+K]
    # forward over chunk, producing new h
    ...
    loss_chunk.backward()
    h = h.detach()  # truncate history
\end{lstlisting}
This reduces gradient path length from $T$ to $K$.
\end{frame}

\begin{frame}{Ablation checklist for a physics dataset}
When moving to real experimental sequences:
\begin{itemize}
\item Vary binning / sampling rate and verify invariance of conclusions.
\item Check label noise (ambiguous peak separation) and use soft labels if needed.
\item Study class imbalance; use weighted loss / focal loss.
\item Verify no leakage: split by run, file, or time block if correlations exist.
\item Report uncertainty: bootstrap or repeated seeds.
\end{itemize}
\end{frame}

% ---------------- A more formal proof slide set ----------------

\begin{frame}{Proof: vanishing gradients under uniform contraction}
Assume $\|J_k\|\le \gamma<1$ for all $k$.
Then
\[
\left\|\frac{\partial h_T}{\partial h_t}\right\|
= \left\|\prod_{k=t+1}^T J_k\right\|
\le \prod_{k=t+1}^T \|J_k\|
\le \gamma^{T-t}\xrightarrow[T-t\to\infty]{}0.
\]
Therefore gradients w.r.t. parameters affecting early times decay exponentially in the horizon length.
\end{frame}

\begin{frame}{Proof: explosion possible when $\|J_k\|>1$ often}
If for a subsequence of indices $\mathcal{K}$ we have $\|J_k\|\ge \gamma>1$, then
\[
\left\|\prod_{k=t+1}^T J_k\right\|
\ge \prod_{k\in\mathcal{K}} \|J_k\|
\ge \gamma^{|\mathcal{K}|},
\]
so the product can grow exponentially in the number of ``expansive'' steps.
In practice, non-commutativity complicates tight lower bounds, but growth events are common.
\end{frame}

\begin{frame}{Why orthogonal initialization helps (intuition)}
If $W_h$ is orthogonal, $\|W_h v\|=\|v\|$.
When $\sigma'$ is not saturated (near 1), $J_k\approx W_h$ and thus preserves norms.
This delays vanishing/exploding gradients, improving trainability for moderate horizons.
\end{frame}

\begin{frame}{But orthogonal is not a silver bullet}
Even with orthogonal $W_h$:
\begin{itemize}
\item $\diag(\sigma'(a_t))$ can have many small entries if $\tanh$ saturates.
\item Input drive can push $a_t$ into saturation.
\item Optimization landscape can still have plateaus/saddles.
\end{itemize}
Hence normalization and careful scaling of inputs are crucial.
\end{frame}

% ---------------- Connections to physics / math ----------------

\begin{frame}{Connection to discretized ODEs}
Write
\[
h_{t+1} = h_t + \Delta t\, f_\theta(h_t,x_t)
\]
as a forward Euler step for an ODE.
Vanilla RNN is not exactly residual, but you can interpret it as a discrete map approximating flow.
This motivates links to stability (CFL-like constraints), stiffness, and time-scale separation.
\end{frame}

\begin{frame}{Connection to linear response / Green's functions}
Linear RNN solution
\[
h_t = \sum_{k\ge 0} W_h^k W_x x_{t-k}
\]
resembles a discrete Green’s function:
\[
G_k := W_h^k W_x.
\]
Learning an RNN can be seen as learning an effective memory kernel.
\end{frame}

\begin{frame}{Connection to statistical physics (mean-field intuition)}
For large random $W_h$, one can analyze typical dynamics with mean-field ideas:
\begin{itemize}
\item variance of $a_t$ under random weights,
\item onset of chaotic regimes when effective gain is too large,
\item memory vs stability tradeoff.
\end{itemize}
We won’t do full random-matrix theory here, but the spectral radius heuristic comes from this line of reasoning.
\end{frame}

% ---------------- Evaluation / reporting ----------------

\begin{frame}{Evaluation metrics}
Regression:
\begin{itemize}
\item MSE, MAE, spectral error (Fourier domain)
\item multi-step forecasting error vs horizon
\end{itemize}
Classification:
\begin{itemize}
\item accuracy, ROC-AUC, PR-AUC
\item calibration (reliability curves) if probabilities matter
\end{itemize}
Always report confidence intervals via multiple seeds or bootstrap.
\end{frame}

\begin{frame}{Common pitfalls in scientific RNN modeling}
\begin{itemize}
\item Leakage due to temporal correlations in train/test splits.
\item Overfitting: too many parameters for limited experimental runs.
\item Wrong normalization: RNN saturates or becomes unstable.
\item Mis-specified target: classification labels encode ambiguous peak definitions.
\end{itemize}
\end{frame}

% ---------------- Exercises / mini-projects ----------------

\begin{frame}{Mini-project 1 (theory): stability + memory}
\begin{enumerate}
\item For linear RNN, show stationary covariance exists iff $\sr(W_h)<1$ (hint: Lyapunov equation).
\item Derive how the effective memory length scales as $1/(1-\sr(W_h))$ near $\sr\to 1^-$ (heuristic).
\end{enumerate}
\end{frame}

\begin{frame}{Mini-project 2 (coding): reproduce gradient pathology}
\begin{enumerate}
\item Train the sine predictor with increasing $T$: 30, 60, 120, 240.
\item Log gradient norms and training loss.
\item Show where training breaks without clipping/orthogonal init.
\item Fix it with clipping + orthogonal init + normalization.
\end{enumerate}
\end{frame}

\begin{frame}{Mini-project 3 (physics-style): regime classification}
\begin{enumerate}
\item Create two regimes: low-frequency vs high-frequency mixtures.
\item Train RNN classifier; compare with 1D CNN baseline.
\item Discuss inductive bias: locality (CNN) vs memory (RNN).
\end{enumerate}
\end{frame}

% ---------------- Summary slides ----------------

\begin{frame}{Summary: key mathematical takeaways}
\begin{itemize}
\item Vanilla RNN is a discrete-time nonlinear state-space model:
\[
h_t=\sigma(W_h h_{t-1}+W_x x_t+b).
\]
\item Dynamics: fixed points, Jacobian $J=\diag(\sigma')W_h$, stability via $\sr(J)$.
\item Training: BPTT yields Jacobian products $\prod J_k$ $\Rightarrow$ vanishing/exploding gradients.
\item Memory and trainability are linked to spectral properties and saturation of $\sigma'$.
\end{itemize}
\end{frame}

\begin{frame}{Summary: practical takeaways}
\begin{itemize}
\item Start with: normalization, orthogonal/scaled init, gradient clipping.
\item Use truncated BPTT for long sequences.
\item Benchmark against simpler baselines (AR models, CNNs) to justify RNN complexity.
\item For scientific credibility: careful splits, uncertainty estimates, ablations.
\end{itemize}
\end{frame}

\end{document}


