\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces General structure of an autoencoder.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:arch}{{1}{3}{General structure of an autoencoder}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Regularization in autoencoders}{4}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Feed Forward Autoencoders}{5}{section.2}\protected@file@percent }
\newlabel{fig:arch2}{{2}{5}{Feed Forward Autoencoders}{section.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A typical architecture of a Feed-Forward Autoencoder. The number of neurons in the layers at first goes down as we move through the network until it reaches the middle and then starts to grow again until the last layer has the same number of neurons as the input dimensions.}}{5}{figure.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Activation Function of the Output Layer}{6}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}ReLU}{6}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Sigmoid}{7}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Loss Function}{7}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Mean Square Error}{8}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Binary Cross-Entropy}{9}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Reconstruction Error}{11}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Example: reconstruction of hand-written digits}{11}{subsection.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces An example of big and small reconstruction error when an autoencoder tries to reconstruct an image.}}{12}{figure.3}\protected@file@percent }
\newlabel{fig:rec_err}{{3}{12}{An example of big and small reconstruction error when an autoencoder tries to reconstruct an image}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces n the top line you can see the original digits from the MNIST dataset. While the line below are the digits reconstructed by the autoencoder with number of neurons equal to (784, 64, 784).}}{12}{figure.5}\protected@file@percent }
\newlabel{fig:rec_3}{{5}{12}{n the top line you can see the original digits from the MNIST dataset. While the line below are the digits reconstructed by the autoencoder with number of neurons equal to (784, 64, 784)}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces In the top line, you can see the original digits from the MNIST dataset. In contrast, the line below contains the digits reconstructed by the autoencoder with number of neurons equal to (784, 16, 784). }}{13}{figure.4}\protected@file@percent }
\newlabel{fig:rec_2}{{4}{13}{In the top line, you can see the original digits from the MNIST dataset. In contrast, the line below contains the digits reconstructed by the autoencoder with number of neurons equal to (784, 16, 784)}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces In the top line you can see the original digits from the MNIST dataset. In contrast, the line below contains the digits reconstructed by the autoencoder with number of neurons equal to (784, 8, 784). }}{13}{figure.6}\protected@file@percent }
\newlabel{fig:rec_4}{{6}{13}{In the top line you can see the original digits from the MNIST dataset. In contrast, the line below contains the digits reconstructed by the autoencoder with number of neurons equal to (784, 8, 784)}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces In the top line, you can see the original digits from the MNIST dataset. The second line of digits contains the digits reconsructed by the FFA (784,8,784), the third by the FFA (784,16,784), and the last one by the FFA (784,64,784).}}{14}{figure.7}\protected@file@percent }
\newlabel{fig:rec_5}{{7}{14}{In the top line, you can see the original digits from the MNIST dataset. The second line of digits contains the digits reconsructed by the FFA (784,8,784), the third by the FFA (784,16,784), and the last one by the FFA (784,64,784)}{figure.7}{}}
\newlabel{fig:rec_6}{{2.4}{14}{Example: reconstruction of hand-written digits}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces In the top line, you can see ten random original digits from the MNIST dataset. The second line of digits contains the digits reconstructed with an FFA with 16 neurons in the middle layer and the binary cross-entropy as the loss function. The last line contains images reconstructed with the MSE as loss function.}}{14}{figure.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Autoencoders Applications}{15}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dimensionality Reduction}{15}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Equivalence with PCA}{15}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Classification}{16}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Classification with Latent Features}{16}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces the different in accuracy and running time when applying the kNN algorithm to the original 784 features or the 8 latent features for the MNIST dataset.}}{17}{table.1}\protected@file@percent }
\newlabel{tab:run1}{{1}{17}{the different in accuracy and running time when applying the kNN algorithm to the original 784 features or the 8 latent features for the MNIST dataset}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces the difference in accuracy and running time when applying the kNN algorithm to the original 784 features with a FFA with 8 neurons and with a FFA with 16 neurons for the Fashion MNIST dataset.}}{17}{table.2}\protected@file@percent }
\newlabel{tab:run2}{{2}{17}{the difference in accuracy and running time when applying the kNN algorithm to the original 784 features with a FFA with 8 neurons and with a FFA with 16 neurons for the Fashion MNIST dataset}{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Curse of dimensionality – a small detour}{17}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Length \( l\) of the smallest hyper-cube to contain at least \( 1\) point from a population of randomly distributed \( m\) points.}}{18}{table.3}\protected@file@percent }
\newlabel{tab:res1}{{3}{18}{Length \( l\) of the smallest hyper-cube to contain at least \( 1\) point from a population of randomly distributed \( m\) points}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Anomaly Detection}{19}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces one random image from the Zalando MNIST dataset.}}{19}{figure.9}\protected@file@percent }
\newlabel{fig:anom1}{{9}{19}{one random image from the Zalando MNIST dataset}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The shoe and the autoencoder's reconstruction trained on the 60000 hand-written images of the MNIST dataset. This image has the biggest RE in the entire 10001 test dataset we built with a value of 0.062.}}{20}{figure.10}\protected@file@percent }
\newlabel{fig:recon2}{{10}{20}{The shoe and the autoencoder's reconstruction trained on the 60000 hand-written images of the MNIST dataset. This image has the biggest RE in the entire 10001 test dataset we built with a value of 0.062}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces the image with the second biggest RE in the 10001 test dataset: 0.022.}}{21}{figure.11}\protected@file@percent }
\newlabel{fig:recon3}{{11}{21}{the image with the second biggest RE in the 10001 test dataset: 0.022}{figure.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Model Stability – a short note}{22}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Denoising autoencoders}{23}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Beyond FFA – autoencoders with convolutional layers}{23}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Code Examples}{24}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Further Readings}{24}{section.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Results of a denoising FFA autoencoder with 3 layers and 32 neurons in the middle layer. The noise has been generating adding a real number between 0 and 1 taken from a normal distribution. For details see the code at \url  {https://adl.toelt.ai}.}}{25}{figure.12}\protected@file@percent }
\newlabel{fig:recon5}{{12}{25}{Results of a denoising FFA autoencoder with 3 layers and 32 neurons in the middle layer. The noise has been generating adding a real number between 0 and 1 taken from a normal distribution. For details see the code at \url {https://adl.toelt.ai}}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Comparison of the results of a FAA (with architecture (784,32,784)) and of a Convolutional Autoencoder (CA) (with architecture ((28x28), (26x26x64), (24x24,32), (26x26x64), (28x28); keep in mind the layers are convolutions, so the first two numbers indicate the tensor dimensions and the third the number of kernels, with kernel size 3x3). The two autoencoders have been trained with the same parameters. You can check the code at \url  {https://adl.toelt.ai}.}}{26}{figure.13}\protected@file@percent }
\newlabel{fig:recon6}{{13}{26}{Comparison of the results of a FAA (with architecture (784,32,784)) and of a Convolutional Autoencoder (CA) (with architecture ((28x28), (26x26x64), (24x24,32), (26x26x64), (28x28); keep in mind the layers are convolutions, so the first two numbers indicate the tensor dimensions and the third the number of kernels, with kernel size 3x3). The two autoencoders have been trained with the same parameters. You can check the code at \url {https://adl.toelt.ai}}{figure.13}{}}
\gdef \@abspage@last{26}
