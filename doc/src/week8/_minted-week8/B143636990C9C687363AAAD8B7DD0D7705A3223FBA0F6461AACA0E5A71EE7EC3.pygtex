\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`\$=3\catcode`\^=7\catcode`\_=8\relax}]
\PYG{c+c1}{\PYGZsh{} FORMAT\PYGZus{}DATA}
\PYG{k}{def} \PYG{n+nf}{format\PYGZus{}data}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{length\PYGZus{}of\PYGZus{}sequence} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{):}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Inputs:}
\PYG{l+s+sd}{            data(a numpy array): the data that will be the inputs to the recurrent neural}
\PYG{l+s+sd}{                network}
\PYG{l+s+sd}{            length\PYGZus{}of\PYGZus{}sequence (an int): the number of elements in one iteration of the}
\PYG{l+s+sd}{                sequence patter.  For a function approximator use length\PYGZus{}of\PYGZus{}sequence = 2.}
\PYG{l+s+sd}{        Returns:}
\PYG{l+s+sd}{            rnn\PYGZus{}input (a 3D numpy array): the input data for the recurrent neural network.  Its}
\PYG{l+s+sd}{                dimensions are length of data \PYGZhy{} length of sequence, length of sequence,}
\PYG{l+s+sd}{                dimnsion of data}
\PYG{l+s+sd}{            rnn\PYGZus{}output (a numpy array): the training data for the neural network}
\PYG{l+s+sd}{        Formats data to be used in a recurrent neural network.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{n}{X}\PYG{p}{,} \PYG{n}{Y} \PYG{o}{=} \PYG{p}{[],} \PYG{p}{[]}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{length\PYGZus{}of\PYGZus{}sequence}\PYG{p}{):}
        \PYG{c+c1}{\PYGZsh{} Get the next length\PYGZus{}of\PYGZus{}sequence elements}
        \PYG{n}{a} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{i}\PYG{p}{:}\PYG{n}{i}\PYG{o}{+}\PYG{n}{length\PYGZus{}of\PYGZus{}sequence}\PYG{p}{]}
        \PYG{c+c1}{\PYGZsh{} Get the element that immediately follows that}
        \PYG{n}{b} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{n}{length\PYGZus{}of\PYGZus{}sequence}\PYG{p}{]}
        \PYG{c+c1}{\PYGZsh{} Reshape so that each data point is contained in its own array}
        \PYG{n}{a} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{reshape} \PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{a}\PYG{p}{),} \PYG{l+m+mi}{1}\PYG{p}{))}
        \PYG{n}{X}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}
        \PYG{n}{Y}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{b}\PYG{p}{)}
    \PYG{n}{rnn\PYGZus{}input} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
    \PYG{n}{rnn\PYGZus{}output} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{rnn\PYGZus{}input}\PYG{p}{,} \PYG{n}{rnn\PYGZus{}output}


\PYG{c+c1}{\PYGZsh{} \PYGZsh{}\PYGZsh{} Defining the Recurrent Neural Network Using Keras}
\PYG{c+c1}{\PYGZsh{}}
\PYG{c+c1}{\PYGZsh{} The following method defines a simple recurrent neural network in keras consisting of one input layer, one hidden layer, and one output layer.}

\PYG{k}{def} \PYG{n+nf}{rnn}\PYG{p}{(}\PYG{n}{length\PYGZus{}of\PYGZus{}sequences}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{stateful} \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{):}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Inputs:}
\PYG{l+s+sd}{            length\PYGZus{}of\PYGZus{}sequences (an int): the number of y values in \PYGZdq{}x data\PYGZdq{}.  This is determined}
\PYG{l+s+sd}{                when the data is formatted}
\PYG{l+s+sd}{            batch\PYGZus{}size (an int): Default value is None.  See Keras documentation of SimpleRNN.}
\PYG{l+s+sd}{            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.}
\PYG{l+s+sd}{        Returns:}
\PYG{l+s+sd}{            model (a Keras model): The recurrent neural network that is built and compiled by this}
\PYG{l+s+sd}{                method}
\PYG{l+s+sd}{        Builds and compiles a recurrent neural network with one hidden layer and returns the model.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{c+c1}{\PYGZsh{} Number of neurons in the input and output layers}
    \PYG{n}{in\PYGZus{}out\PYGZus{}neurons} \PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{c+c1}{\PYGZsh{} Number of neurons in the hidden layer}
    \PYG{n}{hidden\PYGZus{}neurons} \PYG{o}{=} \PYG{l+m+mi}{200}
    \PYG{c+c1}{\PYGZsh{} Define the input layer}
    \PYG{n}{inp} \PYG{o}{=} \PYG{n}{Input}\PYG{p}{(}\PYG{n}{batch\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,}
                \PYG{n}{length\PYGZus{}of\PYGZus{}sequences}\PYG{p}{,}
                \PYG{n}{in\PYGZus{}out\PYGZus{}neurons}\PYG{p}{))}
    \PYG{c+c1}{\PYGZsh{} Define the hidden layer as a simple RNN layer with a set number of neurons and add it to}
    \PYG{c+c1}{\PYGZsh{} the network immediately after the input layer}
    \PYG{n}{rnn} \PYG{o}{=} \PYG{n}{SimpleRNN}\PYG{p}{(}\PYG{n}{hidden\PYGZus{}neurons}\PYG{p}{,}
                    \PYG{n}{return\PYGZus{}sequences}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}
                    \PYG{n}{stateful} \PYG{o}{=} \PYG{n}{stateful}\PYG{p}{,}
                    \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}RNN\PYGZdq{}}\PYG{p}{)(}\PYG{n}{inp}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Define the output layer as a dense neural network layer (standard neural network layer)}
    \PYG{c+c1}{\PYGZsh{}and add it to the network immediately after the hidden layer.}
    \PYG{n}{dens} \PYG{o}{=} \PYG{n}{Dense}\PYG{p}{(}\PYG{n}{in\PYGZus{}out\PYGZus{}neurons}\PYG{p}{,}\PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}dense\PYGZdq{}}\PYG{p}{)(}\PYG{n}{rnn}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Create the machine learning model starting with the input layer and ending with the}
    \PYG{c+c1}{\PYGZsh{} output layer}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{Model}\PYG{p}{(}\PYG{n}{inputs}\PYG{o}{=}\PYG{p}{[}\PYG{n}{inp}\PYG{p}{],}\PYG{n}{outputs}\PYG{o}{=}\PYG{p}{[}\PYG{n}{dens}\PYG{p}{])}
    \PYG{c+c1}{\PYGZsh{} Compile the machine learning model using the mean squared error function as the loss}
    \PYG{c+c1}{\PYGZsh{} function and an Adams optimizer.}
    \PYG{n}{model}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{loss}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}mean\PYGZus{}squared\PYGZus{}error\PYGZdq{}}\PYG{p}{,} \PYG{n}{optimizer}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}adam\PYGZdq{}}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{model}


\end{Verbatim}
