\frametitle{Example to illustrate dying gradients}

Consider a simple NN in which the inputs $\bm{x}, the weights
$\bm{W}$, the biases $\bm{b}$ and the ouputs
$\bm{\tilde{y}}=f(\bm{x};\bm{\Theta})$ are just scalars and that we
have two layers only, that is the output layer is labeled with $L=2$.

Our output is then (no boldfaced symbols since all quantities are scalars)
\[
\tilde{y}=f(x;Theta))=\sigma_{L=2}(w_2\sigma_1(w_1x+b_1)+b_2).
\]

For the back-propagation algorithm we will need various partial derivatives. One of these is
\[
\frac{\partial f(x;\Theta)}{\partial w_1)=\sigma_2^'(w_2\sigma_1(w_1x+b_1)+b_2)\times w_2\sigma_1^'(w_1x+b_1)x.
\]

\textbf{See whiteboard notes for rest}.
