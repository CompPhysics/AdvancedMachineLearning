TITLE: Advanced machine learning and data analysis for the physical sciences
AUTHOR: Morten Hjorth-Jensen {copyright, 1999-present|CC BY-NC} at Department of Physics and Center for Computing in Science Education, University of Oslo, Norway
DATE: March 6

!split
===== Plans for the week of March 3-7  =====

!bblock
o Reminder on basics of recurrent neural networks (RNNs)
o Mathematics of RNNs
o Writing our own codes for RNNs
o "Video of lecture":"https://youtu.be/MeYh5rGIRBM"
o "Whiteboard notes":"https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/HandwrittenNotes/2025/NotesMarch6.pdf
o Reading recommendations:
  o Goodfellow, Bengio and Courville's chapter 10 from "Deep Learning":"https://www.deeplearningbook.org/"
  o "Sebastian Rashcka et al, chapter 15, Machine learning with Sickit-Learn and PyTorch":"https://sebastianraschka.com/blog/2022/ml-pytorch-book.html"
  o "David Foster, Generative Deep Learning with TensorFlow, see chapter 5":"https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html"
!eblock

The last two books have codes for RNNs in PyTorch and TensorFlow/Keras.



!split
===== What is a recurrent NN? =====

A recurrent neural network (RNN), as opposed to a regular fully
connected neural network (FCNN) or just neural network (NN), has
layers that are connected to themselves.
    

In an FCNN there are no connections between nodes in a single
layer. For instance, $(h_1^1$ is not connected to $(h_2^1$. In
addition, the input and output are always of a fixed length.

In an RNN, however, this is no longer the case. Nodes in the hidden
layers are connected to themselves.

    
!split
===== Why RNNs? =====

Recurrent neural networks work very well when working with
sequential data, that is data where the order matters. In a regular
fully connected network, the order of input doesn't really matter.

Another property of  RNNs is that they can handle variable input
and output. Consider again the simplified breast cancer dataset. If you
have trained a regular FCNN on the dataset with the two features, it
makes no sense to suddenly add a third feature. The network would not
know what to do with it, and would reject such inputs with three
features (or any other number of features that isn't two, for that
matter).



!split
===== Basic layout,  "Figures from Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch":"https://sebastianraschka.com/blog/2022/ml-pytorch-book.html" =====

FIGURE: [figslides/RNN1.png, width=700 frac=0.9]


!split
===== RNNs in more detail  =====

FIGURE: [figslides/RNN2.png, width=700 frac=0.9]

!split
===== RNNs in more detail, part 2  =====

FIGURE: [figslides/RNN3.png, width=700 frac=0.9]


!split
===== RNNs in more detail, part 3  =====

FIGURE: [figslides/RNN4.png, width=700 frac=0.9]

!split
===== RNNs in more detail, part 4  =====

FIGURE: [figslides/RNN5.png, width=700 frac=0.9]

!split
===== RNNs in more detail, part 5  =====

FIGURE: [figslides/RNN6.png, width=700 frac=0.9]

!split
===== RNNs in more detail, part 6  =====

FIGURE: [figslides/RNN7.png, width=700 frac=0.9]

!split
===== RNNs in more detail, part 7  =====

FIGURE: [figslides/RNN8.png, width=700 frac=0.9]





!split
===== Backpropagation through time =====

!bblock
We can think of the recurrent net as a layered, feed-forward
net with shared weights and then train the feed-forward net
with weight constraints.
!eblock

We can also think of this training algorithm in the time domain:
o The forward pass builds up a stack of the activities of all the units at each time step.
o The backward pass peels activities off the stack to compute the error derivatives at each time step.
o After the backward pass we add together the derivatives at all the different times for each weight. 

!split
===== The backward pass is linear =====

o There is a big difference between the forward and backward passes.
o In the forward pass we use squashing functions (like the logistic) to prevent the activity vectors from exploding.
o The backward pass, is completely linear. If you double the error derivatives at the final layer, all the error derivatives will double.

The forward pass determines the slope of the linear function used for
backpropagating through each neuron


!split 
===== The problem of exploding or vanishing gradients =====
* What happens to the magnitude of the gradients as we backpropagate through many layers?
  o If the weights are small, the gradients shrink exponentially.
  o If the weights are big the gradients grow exponentially.
* Typical feed-forward neural nets can cope with these exponential effects because they only have a few hidden layers.
* In an RNN trained on long sequences (e.g. 100 time steps) the gradients can easily explode or vanish.
  o We can avoid this by initializing the weights very carefully.
* Even with good initial weights, its very hard to detect that the current target output depends on an input from many time-steps ago.


RNNs have difficulty dealing with long-range dependencies. 


!split
===== The mathematics of RNNs, the basic architecture  =====

See notebook at URL:"https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week7/ipynb/rnnmath.ipynb"

!split
===== Four effective ways to learn an RNN and preparing for next week =====
o Long Short Term Memory Make the RNN out of little modules that are designed to remember values for a long time.
o Hessian Free Optimization: Deal with the vanishing gradients problem by using a fancy optimizer that can detect directions with a tiny gradient but even smaller curvature.
o Echo State Networks (ESN): Initialize the input a hidden and hidden-hidden and output-hidden connections very carefully so that the hidden state has a huge reservoir of weakly coupled oscillators which can be selectively driven by the input. ESNs only need to learn the hidden-output connections.
o Good initialization with momentum Initialize like in Echo State Networks, but then learn all of the connections using momentum




!split
===== Long Short Term Memory (LSTM) =====

LSTM uses a memory cell for   modeling long-range dependencies and avoid vanishing gradient  problems.

o Introduced by Hochreiter and Schmidhuber (1997) who solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).
o They designed a memory cell using logistic and linear units with multiplicative interactions.
o Information gets into the cell whenever its “write” gate is on.
o The information stays in the cell so long as its _keep_ gate is on.
o Information can be read from the cell by turning on its _read_ gate. 








