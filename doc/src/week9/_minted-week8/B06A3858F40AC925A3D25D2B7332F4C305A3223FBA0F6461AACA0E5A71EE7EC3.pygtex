\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`\$=3\catcode`\^=7\catcode`\_=8\relax}]
\PYG{k}{def} \PYG{n+nf}{lstm\PYGZus{}2layers}\PYG{p}{(}\PYG{n}{length\PYGZus{}of\PYGZus{}sequences}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{stateful} \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{):}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Inputs:}
\PYG{l+s+sd}{            length\PYGZus{}of\PYGZus{}sequences (an int): the number of y values in \PYGZdq{}x data\PYGZdq{}.  This is determined}
\PYG{l+s+sd}{                when the data is formatted}
\PYG{l+s+sd}{            batch\PYGZus{}size (an int): Default value is None.  See Keras documentation of SimpleRNN.}
\PYG{l+s+sd}{            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.}
\PYG{l+s+sd}{        Returns:}
\PYG{l+s+sd}{            model (a Keras model): The recurrent neural network that is built and compiled by this}
\PYG{l+s+sd}{                method}
\PYG{l+s+sd}{        Builds and compiles a recurrent neural network with two LSTM hidden layers and returns the model.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{c+c1}{\PYGZsh{} Number of neurons on the input/output layer and the number of neurons in the hidden layer}
    \PYG{n}{in\PYGZus{}out\PYGZus{}neurons} \PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{n}{hidden\PYGZus{}neurons} \PYG{o}{=} \PYG{l+m+mi}{250}
    \PYG{c+c1}{\PYGZsh{} Input Layer}
    \PYG{n}{inp} \PYG{o}{=} \PYG{n}{Input}\PYG{p}{(}\PYG{n}{batch\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,}
                \PYG{n}{length\PYGZus{}of\PYGZus{}sequences}\PYG{p}{,}
                \PYG{n}{in\PYGZus{}out\PYGZus{}neurons}\PYG{p}{))}
    \PYG{c+c1}{\PYGZsh{} Hidden layers (in this case they are LSTM layers instead if SimpleRNN layers)}
    \PYG{n}{rnn}\PYG{o}{=} \PYG{n}{LSTM}\PYG{p}{(}\PYG{n}{hidden\PYGZus{}neurons}\PYG{p}{,}
                    \PYG{n}{return\PYGZus{}sequences}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
                    \PYG{n}{stateful} \PYG{o}{=} \PYG{n}{stateful}\PYG{p}{,}
                    \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}RNN\PYGZdq{}}\PYG{p}{,} \PYG{n}{use\PYGZus{}bias}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}tanh\PYGZsq{}}\PYG{p}{)(}\PYG{n}{inp}\PYG{p}{)}
    \PYG{n}{rnn1} \PYG{o}{=} \PYG{n}{LSTM}\PYG{p}{(}\PYG{n}{hidden\PYGZus{}neurons}\PYG{p}{,}
                    \PYG{n}{return\PYGZus{}sequences}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}
                    \PYG{n}{stateful} \PYG{o}{=} \PYG{n}{stateful}\PYG{p}{,}
                    \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}RNN1\PYGZdq{}}\PYG{p}{,} \PYG{n}{use\PYGZus{}bias}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}tanh\PYGZsq{}}\PYG{p}{)(}\PYG{n}{rnn}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Output layer}
    \PYG{n}{dens} \PYG{o}{=} \PYG{n}{Dense}\PYG{p}{(}\PYG{n}{in\PYGZus{}out\PYGZus{}neurons}\PYG{p}{,}\PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}dense\PYGZdq{}}\PYG{p}{)(}\PYG{n}{rnn1}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Define the midel}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{Model}\PYG{p}{(}\PYG{n}{inputs}\PYG{o}{=}\PYG{p}{[}\PYG{n}{inp}\PYG{p}{],}\PYG{n}{outputs}\PYG{o}{=}\PYG{p}{[}\PYG{n}{dens}\PYG{p}{])}
    \PYG{c+c1}{\PYGZsh{} Compile the model}
    \PYG{n}{model}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{loss}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}mean\PYGZus{}squared\PYGZus{}error\PYGZsq{}}\PYG{p}{,} \PYG{n}{optimizer}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}adam\PYGZsq{}}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Return the model}
    \PYG{k}{return} \PYG{n}{model}

\PYG{k}{def} \PYG{n+nf}{dnn2\PYGZus{}gru2}\PYG{p}{(}\PYG{n}{length\PYGZus{}of\PYGZus{}sequences}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{stateful} \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{):}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Inputs:}
\PYG{l+s+sd}{            length\PYGZus{}of\PYGZus{}sequences (an int): the number of y values in \PYGZdq{}x data\PYGZdq{}.  This is determined}
\PYG{l+s+sd}{                when the data is formatted}
\PYG{l+s+sd}{            batch\PYGZus{}size (an int): Default value is None.  See Keras documentation of SimpleRNN.}
\PYG{l+s+sd}{            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.}
\PYG{l+s+sd}{        Returns:}
\PYG{l+s+sd}{            model (a Keras model): The recurrent neural network that is built and compiled by this}
\PYG{l+s+sd}{                method}
\PYG{l+s+sd}{        Builds and compiles a recurrent neural network with four hidden layers (two dense followed by}
\PYG{l+s+sd}{        two GRU layers) and returns the model.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{c+c1}{\PYGZsh{} Number of neurons on the input/output layers and hidden layers}
    \PYG{n}{in\PYGZus{}out\PYGZus{}neurons} \PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{n}{hidden\PYGZus{}neurons} \PYG{o}{=} \PYG{l+m+mi}{250}
    \PYG{c+c1}{\PYGZsh{} Input layer}
    \PYG{n}{inp} \PYG{o}{=} \PYG{n}{Input}\PYG{p}{(}\PYG{n}{batch\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,}
                \PYG{n}{length\PYGZus{}of\PYGZus{}sequences}\PYG{p}{,}
                \PYG{n}{in\PYGZus{}out\PYGZus{}neurons}\PYG{p}{))}
    \PYG{c+c1}{\PYGZsh{} Hidden Dense (feedforward) layers}
    \PYG{n}{dnn} \PYG{o}{=} \PYG{n}{Dense}\PYG{p}{(}\PYG{n}{hidden\PYGZus{}neurons}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}relu\PYGZsq{}}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}dnn\PYGZsq{}}\PYG{p}{)(}\PYG{n}{inp}\PYG{p}{)}
    \PYG{n}{dnn1} \PYG{o}{=} \PYG{n}{Dense}\PYG{p}{(}\PYG{n}{hidden\PYGZus{}neurons}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}relu\PYGZsq{}}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}dnn1\PYGZsq{}}\PYG{p}{)(}\PYG{n}{dnn}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Hidden GRU layers}
    \PYG{n}{rnn1} \PYG{o}{=} \PYG{n}{GRU}\PYG{p}{(}\PYG{n}{hidden\PYGZus{}neurons}\PYG{p}{,}
                    \PYG{n}{return\PYGZus{}sequences}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
                    \PYG{n}{stateful} \PYG{o}{=} \PYG{n}{stateful}\PYG{p}{,}
                    \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}RNN1\PYGZdq{}}\PYG{p}{,} \PYG{n}{use\PYGZus{}bias}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)(}\PYG{n}{dnn1}\PYG{p}{)}
    \PYG{n}{rnn} \PYG{o}{=} \PYG{n}{GRU}\PYG{p}{(}\PYG{n}{hidden\PYGZus{}neurons}\PYG{p}{,}
                    \PYG{n}{return\PYGZus{}sequences}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}
                    \PYG{n}{stateful} \PYG{o}{=} \PYG{n}{stateful}\PYG{p}{,}
                    \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}RNN\PYGZdq{}}\PYG{p}{,} \PYG{n}{use\PYGZus{}bias}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)(}\PYG{n}{rnn1}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Output layer}
    \PYG{n}{dens} \PYG{o}{=} \PYG{n}{Dense}\PYG{p}{(}\PYG{n}{in\PYGZus{}out\PYGZus{}neurons}\PYG{p}{,}\PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}dense\PYGZdq{}}\PYG{p}{)(}\PYG{n}{rnn}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Define the model}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{Model}\PYG{p}{(}\PYG{n}{inputs}\PYG{o}{=}\PYG{p}{[}\PYG{n}{inp}\PYG{p}{],}\PYG{n}{outputs}\PYG{o}{=}\PYG{p}{[}\PYG{n}{dens}\PYG{p}{])}
    \PYG{c+c1}{\PYGZsh{} Compile the mdoel}
    \PYG{n}{model}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{loss}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}mean\PYGZus{}squared\PYGZus{}error\PYGZsq{}}\PYG{p}{,} \PYG{n}{optimizer}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}adam\PYGZsq{}}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Return the model}
    \PYG{k}{return} \PYG{n}{model}

\PYG{c+c1}{\PYGZsh{} Check to make sure the data set is complete}
\PYG{k}{assert} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X\PYGZus{}tot}\PYG{p}{)} \PYG{o}{==} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y\PYGZus{}tot}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} This is the number of points that will be used in as the training data}
\PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{12}

\PYG{c+c1}{\PYGZsh{} Separate the training data from the whole data set}
\PYG{n}{X\PYGZus{}train} \PYG{o}{=} \PYG{n}{X\PYGZus{}tot}\PYG{p}{[:}\PYG{n}{dim}\PYG{p}{]}
\PYG{n}{y\PYGZus{}train} \PYG{o}{=} \PYG{n}{y\PYGZus{}tot}\PYG{p}{[:}\PYG{n}{dim}\PYG{p}{]}


\PYG{c+c1}{\PYGZsh{} Generate the training data for the RNN, using a sequence of 2}
\PYG{n}{rnn\PYGZus{}input}\PYG{p}{,} \PYG{n}{rnn\PYGZus{}training} \PYG{o}{=} \PYG{n}{format\PYGZus{}data}\PYG{p}{(}\PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} Create a recurrent neural network in Keras and produce a summary of the}
\PYG{c+c1}{\PYGZsh{} machine learning model}
\PYG{c+c1}{\PYGZsh{} Change the method name to reflect which network you want to use}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{dnn2\PYGZus{}gru2}\PYG{p}{(}\PYG{n}{length\PYGZus{}of\PYGZus{}sequences} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{()}

\PYG{c+c1}{\PYGZsh{} Start the timer.  Want to time training+testing}
\PYG{n}{start} \PYG{o}{=} \PYG{n}{timer}\PYG{p}{()}
\PYG{c+c1}{\PYGZsh{} Fit the model using the training data genenerated above using 150 training iterations and a 5\PYGZpc{}}
\PYG{c+c1}{\PYGZsh{} validation split.  Setting verbose to True prints information about each training iteration.}
\PYG{n}{hist} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{rnn\PYGZus{}input}\PYG{p}{,} \PYG{n}{rnn\PYGZus{}training}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{150}\PYG{p}{,}
                 \PYG{n}{verbose}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{validation\PYGZus{}split}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} This section plots the training loss and the validation loss as a function of training iteration.}
\PYG{c+c1}{\PYGZsh{} This is not required for analyzing the couple cluster data but can help determine if the network is}
\PYG{c+c1}{\PYGZsh{} being overtrained.}
\PYG{k}{for} \PYG{n}{label} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}loss\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}val\PYGZus{}loss\PYGZdq{}}\PYG{p}{]:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{hist}\PYG{o}{.}\PYG{n}{history}\PYG{p}{[}\PYG{n}{label}\PYG{p}{],}\PYG{n}{label}\PYG{o}{=}\PYG{n}{label}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}loss\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}epoch\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}The final validation loss: }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{hist}\PYG{o}{.}\PYG{n}{history}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}val\PYGZus{}loss\PYGZdq{}}\PYG{p}{][}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]))}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{()}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{()}

\PYG{c+c1}{\PYGZsh{} Use the trained neural network to predict more points of the data set}
\PYG{n}{test\PYGZus{}rnn}\PYG{p}{(}\PYG{n}{X\PYGZus{}tot}\PYG{p}{,} \PYG{n}{y\PYGZus{}tot}\PYG{p}{,} \PYG{n}{X\PYGZus{}tot}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{],} \PYG{n}{X\PYGZus{}tot}\PYG{p}{[}\PYG{n}{dim}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{])}
\PYG{c+c1}{\PYGZsh{} Stop the timer and calculate the total time needed.}
\PYG{n}{end} \PYG{o}{=} \PYG{n}{timer}\PYG{p}{()}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}Time: \PYGZsq{}}\PYG{p}{,} \PYG{n}{end}\PYG{o}{\PYGZhy{}}\PYG{n}{start}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} \PYGZsh{}\PYGZsh{}\PYGZsh{} Training Recurrent Neural Networks in the Standard Way (i.e. learning the relationship between the X and Y data)}
\PYG{c+c1}{\PYGZsh{}}
\PYG{c+c1}{\PYGZsh{} Finally, comparing the performace of a recurrent neural network using the standard data formatting to the performance of the network with time sequence data formatting shows the benefit of this type of data formatting with extrapolation.}

\PYG{c+c1}{\PYGZsh{} Check to make sure the data set is complete}
\PYG{k}{assert} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X\PYGZus{}tot}\PYG{p}{)} \PYG{o}{==} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y\PYGZus{}tot}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} This is the number of points that will be used in as the training data}
\PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{12}

\PYG{c+c1}{\PYGZsh{} Separate the training data from the whole data set}
\PYG{n}{X\PYGZus{}train} \PYG{o}{=} \PYG{n}{X\PYGZus{}tot}\PYG{p}{[:}\PYG{n}{dim}\PYG{p}{]}
\PYG{n}{y\PYGZus{}train} \PYG{o}{=} \PYG{n}{y\PYGZus{}tot}\PYG{p}{[:}\PYG{n}{dim}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Reshape the data for Keras specifications}
\PYG{n}{X\PYGZus{}train} \PYG{o}{=} \PYG{n}{X\PYGZus{}train}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{((}\PYG{n}{dim}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{))}
\PYG{n}{y\PYGZus{}train} \PYG{o}{=} \PYG{n}{y\PYGZus{}train}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{((}\PYG{n}{dim}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{))}


\PYG{c+c1}{\PYGZsh{} Create a recurrent neural network in Keras and produce a summary of the}
\PYG{c+c1}{\PYGZsh{} machine learning model}
\PYG{c+c1}{\PYGZsh{} Set the sequence length to 1 for regular data formatting}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{rnn}\PYG{p}{(}\PYG{n}{length\PYGZus{}of\PYGZus{}sequences} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{()}

\PYG{c+c1}{\PYGZsh{} Start the timer.  Want to time training+testing}
\PYG{n}{start} \PYG{o}{=} \PYG{n}{timer}\PYG{p}{()}
\PYG{c+c1}{\PYGZsh{} Fit the model using the training data genenerated above using 150 training iterations and a 5\PYGZpc{}}
\PYG{c+c1}{\PYGZsh{} validation split.  Setting verbose to True prints information about each training iteration.}
\PYG{n}{hist} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{150}\PYG{p}{,}
                 \PYG{n}{verbose}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{validation\PYGZus{}split}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} This section plots the training loss and the validation loss as a function of training iteration.}
\PYG{c+c1}{\PYGZsh{} This is not required for analyzing the couple cluster data but can help determine if the network is}
\PYG{c+c1}{\PYGZsh{} being overtrained.}
\PYG{k}{for} \PYG{n}{label} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}loss\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}val\PYGZus{}loss\PYGZdq{}}\PYG{p}{]:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{hist}\PYG{o}{.}\PYG{n}{history}\PYG{p}{[}\PYG{n}{label}\PYG{p}{],}\PYG{n}{label}\PYG{o}{=}\PYG{n}{label}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}loss\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}epoch\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}The final validation loss: }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{hist}\PYG{o}{.}\PYG{n}{history}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}val\PYGZus{}loss\PYGZdq{}}\PYG{p}{][}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]))}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{()}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{()}

\PYG{c+c1}{\PYGZsh{} Use the trained neural network to predict the remaining data points}
\PYG{n}{X\PYGZus{}pred} \PYG{o}{=} \PYG{n}{X\PYGZus{}tot}\PYG{p}{[}\PYG{n}{dim}\PYG{p}{:]}
\PYG{n}{X\PYGZus{}pred} \PYG{o}{=} \PYG{n}{X\PYGZus{}pred}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{((}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X\PYGZus{}pred}\PYG{p}{),} \PYG{l+m+mi}{1}\PYG{p}{))}
\PYG{n}{y\PYGZus{}model} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}pred}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{((}\PYG{n}{y\PYGZus{}tot}\PYG{p}{[:}\PYG{n}{dim}\PYG{p}{],} \PYG{n}{y\PYGZus{}model}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{()))}

\PYG{c+c1}{\PYGZsh{} Plot the known data set and the predicted data set.  The red box represents the region that was used}
\PYG{c+c1}{\PYGZsh{} for the training data.}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{()}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X\PYGZus{}tot}\PYG{p}{,} \PYG{n}{y\PYGZus{}tot}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}true\PYGZdq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X\PYGZus{}tot}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}g\PYGZhy{}.\PYGZsq{}}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}predicted\PYGZdq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{()}
\PYG{c+c1}{\PYGZsh{} Created a red region to represent the points used in the training data.}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{axvspan}\PYG{p}{(}\PYG{n}{X\PYGZus{}tot}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{],} \PYG{n}{X\PYGZus{}tot}\PYG{p}{[}\PYG{n}{dim}\PYG{p}{],} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.25}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}red\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{()}

\PYG{c+c1}{\PYGZsh{} Stop the timer and calculate the total time needed.}
\PYG{n}{end} \PYG{o}{=} \PYG{n}{timer}\PYG{p}{()}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}Time: \PYGZsq{}}\PYG{p}{,} \PYG{n}{end}\PYG{o}{\PYGZhy{}}\PYG{n}{start}\PYG{p}{)}


\end{Verbatim}
