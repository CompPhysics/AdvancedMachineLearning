\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`\$=3\catcode`\^=7\catcode`\_=8\relax}]
\PYG{k}{def} \PYG{n+nf}{rnn\PYGZus{}2layers}\PYG{p}{(}\PYG{n}{length\PYGZus{}of\PYGZus{}sequences}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{stateful} \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{):}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Inputs:}
\PYG{l+s+sd}{            length\PYGZus{}of\PYGZus{}sequences (an int): the number of y values in \PYGZdq{}x data\PYGZdq{}.  This is determined}
\PYG{l+s+sd}{                when the data is formatted}
\PYG{l+s+sd}{            batch\PYGZus{}size (an int): Default value is None.  See Keras documentation of SimpleRNN.}
\PYG{l+s+sd}{            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.}
\PYG{l+s+sd}{        Returns:}
\PYG{l+s+sd}{            model (a Keras model): The recurrent neural network that is built and compiled by this}
\PYG{l+s+sd}{                method}
\PYG{l+s+sd}{        Builds and compiles a recurrent neural network with two hidden layers and returns the model.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{c+c1}{\PYGZsh{} Number of neurons in the input and output layers}
    \PYG{n}{in\PYGZus{}out\PYGZus{}neurons} \PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{c+c1}{\PYGZsh{} Number of neurons in the hidden layer, increased from the first network}
    \PYG{n}{hidden\PYGZus{}neurons} \PYG{o}{=} \PYG{l+m+mi}{500}
    \PYG{c+c1}{\PYGZsh{} Define the input layer}
    \PYG{n}{inp} \PYG{o}{=} \PYG{n}{Input}\PYG{p}{(}\PYG{n}{batch\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,}
                \PYG{n}{length\PYGZus{}of\PYGZus{}sequences}\PYG{p}{,}
                \PYG{n}{in\PYGZus{}out\PYGZus{}neurons}\PYG{p}{))}
    \PYG{c+c1}{\PYGZsh{} Create two hidden layers instead of one hidden layer.  Explicitly set the activation}
    \PYG{c+c1}{\PYGZsh{} function to be the sigmoid function (the default value is hyperbolic tangent)}
    \PYG{n}{rnn1} \PYG{o}{=} \PYG{n}{SimpleRNN}\PYG{p}{(}\PYG{n}{hidden\PYGZus{}neurons}\PYG{p}{,}
                    \PYG{n}{return\PYGZus{}sequences}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} This needs to be True if another hidden layer is to follow}
                    \PYG{n}{stateful} \PYG{o}{=} \PYG{n}{stateful}\PYG{p}{,} \PYG{n}{activation} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}sigmoid\PYGZsq{}}\PYG{p}{,}
                    \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}RNN1\PYGZdq{}}\PYG{p}{)(}\PYG{n}{inp}\PYG{p}{)}
    \PYG{n}{rnn2} \PYG{o}{=} \PYG{n}{SimpleRNN}\PYG{p}{(}\PYG{n}{hidden\PYGZus{}neurons}\PYG{p}{,}
                    \PYG{n}{return\PYGZus{}sequences}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{activation} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}sigmoid\PYGZsq{}}\PYG{p}{,}
                    \PYG{n}{stateful} \PYG{o}{=} \PYG{n}{stateful}\PYG{p}{,}
                    \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}RNN2\PYGZdq{}}\PYG{p}{)(}\PYG{n}{rnn1}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Define the output layer as a dense neural network layer (standard neural network layer)}
    \PYG{c+c1}{\PYGZsh{}and add it to the network immediately after the hidden layer.}
    \PYG{n}{dens} \PYG{o}{=} \PYG{n}{Dense}\PYG{p}{(}\PYG{n}{in\PYGZus{}out\PYGZus{}neurons}\PYG{p}{,}\PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}dense\PYGZdq{}}\PYG{p}{)(}\PYG{n}{rnn2}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Create the machine learning model starting with the input layer and ending with the}
    \PYG{c+c1}{\PYGZsh{} output layer}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{Model}\PYG{p}{(}\PYG{n}{inputs}\PYG{o}{=}\PYG{p}{[}\PYG{n}{inp}\PYG{p}{],}\PYG{n}{outputs}\PYG{o}{=}\PYG{p}{[}\PYG{n}{dens}\PYG{p}{])}
    \PYG{c+c1}{\PYGZsh{} Compile the machine learning model using the mean squared error function as the loss}
    \PYG{c+c1}{\PYGZsh{} function and an Adams optimizer.}
    \PYG{n}{model}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{loss}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}mean\PYGZus{}squared\PYGZus{}error\PYGZdq{}}\PYG{p}{,} \PYG{n}{optimizer}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}adam\PYGZdq{}}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{model}

\PYG{c+c1}{\PYGZsh{} Check to make sure the data set is complete}
\PYG{k}{assert} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X\PYGZus{}tot}\PYG{p}{)} \PYG{o}{==} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y\PYGZus{}tot}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} This is the number of points that will be used in as the training data}
\PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{12}

\PYG{c+c1}{\PYGZsh{} Separate the training data from the whole data set}
\PYG{n}{X\PYGZus{}train} \PYG{o}{=} \PYG{n}{X\PYGZus{}tot}\PYG{p}{[:}\PYG{n}{dim}\PYG{p}{]}
\PYG{n}{y\PYGZus{}train} \PYG{o}{=} \PYG{n}{y\PYGZus{}tot}\PYG{p}{[:}\PYG{n}{dim}\PYG{p}{]}


\PYG{c+c1}{\PYGZsh{} Generate the training data for the RNN, using a sequence of 2}
\PYG{n}{rnn\PYGZus{}input}\PYG{p}{,} \PYG{n}{rnn\PYGZus{}training} \PYG{o}{=} \PYG{n}{format\PYGZus{}data}\PYG{p}{(}\PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} Create a recurrent neural network in Keras and produce a summary of the}
\PYG{c+c1}{\PYGZsh{} machine learning model}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{rnn\PYGZus{}2layers}\PYG{p}{(}\PYG{n}{length\PYGZus{}of\PYGZus{}sequences} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{()}

\PYG{c+c1}{\PYGZsh{} Start the timer.  Want to time training+testing}
\PYG{n}{start} \PYG{o}{=} \PYG{n}{timer}\PYG{p}{()}
\PYG{c+c1}{\PYGZsh{} Fit the model using the training data genenerated above using 150 training iterations and a 5\PYGZpc{}}
\PYG{c+c1}{\PYGZsh{} validation split.  Setting verbose to True prints information about each training iteration.}
\PYG{n}{hist} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{rnn\PYGZus{}input}\PYG{p}{,} \PYG{n}{rnn\PYGZus{}training}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{150}\PYG{p}{,}
                 \PYG{n}{verbose}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{validation\PYGZus{}split}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} This section plots the training loss and the validation loss as a function of training iteration.}
\PYG{c+c1}{\PYGZsh{} This is not required for analyzing the couple cluster data but can help determine if the network is}
\PYG{c+c1}{\PYGZsh{} being overtrained.}
\PYG{k}{for} \PYG{n}{label} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}loss\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}val\PYGZus{}loss\PYGZdq{}}\PYG{p}{]:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{hist}\PYG{o}{.}\PYG{n}{history}\PYG{p}{[}\PYG{n}{label}\PYG{p}{],}\PYG{n}{label}\PYG{o}{=}\PYG{n}{label}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}loss\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}epoch\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}The final validation loss: }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{hist}\PYG{o}{.}\PYG{n}{history}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}val\PYGZus{}loss\PYGZdq{}}\PYG{p}{][}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]))}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{()}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{()}

\PYG{c+c1}{\PYGZsh{} Use the trained neural network to predict more points of the data set}
\PYG{n}{test\PYGZus{}rnn}\PYG{p}{(}\PYG{n}{X\PYGZus{}tot}\PYG{p}{,} \PYG{n}{y\PYGZus{}tot}\PYG{p}{,} \PYG{n}{X\PYGZus{}tot}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{],} \PYG{n}{X\PYGZus{}tot}\PYG{p}{[}\PYG{n}{dim}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{])}
\PYG{c+c1}{\PYGZsh{} Stop the timer and calculate the total time needed.}
\PYG{n}{end} \PYG{o}{=} \PYG{n}{timer}\PYG{p}{()}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}Time: \PYGZsq{}}\PYG{p}{,} \PYG{n}{end}\PYG{o}{\PYGZhy{}}\PYG{n}{start}\PYG{p}{)}

\end{Verbatim}
