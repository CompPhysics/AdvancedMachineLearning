\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`\$=3\catcode`\^=7\catcode`\_=8\relax}]
\PYG{k}{def} \PYG{n+nf}{test\PYGZus{}rnn} \PYG{p}{(}\PYG{n}{x1}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{plot\PYGZus{}min}\PYG{p}{,} \PYG{n}{plot\PYGZus{}max}\PYG{p}{):}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Inputs:}
\PYG{l+s+sd}{            x1 (a list or numpy array): The complete x component of the data set}
\PYG{l+s+sd}{            y\PYGZus{}test (a list or numpy array): The complete y component of the data set}
\PYG{l+s+sd}{            plot\PYGZus{}min (an int or float): the smallest x value used in the training data}
\PYG{l+s+sd}{            plot\PYGZus{}max (an int or float): the largest x valye used in the training data}
\PYG{l+s+sd}{        Returns:}
\PYG{l+s+sd}{            None.}
\PYG{l+s+sd}{        Uses a trained recurrent neural network model to predict future points in the}
\PYG{l+s+sd}{        series.  Computes the MSE of the predicted data set from the true data set, saves}
\PYG{l+s+sd}{        the predicted data set to a csv file, and plots the predicted and true data sets w}
\PYG{l+s+sd}{        while also displaying the data range used for training.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{c+c1}{\PYGZsh{} Add the training data as the first dim points in the predicted data array as these}
    \PYG{c+c1}{\PYGZsh{} are known values.}
    \PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{y\PYGZus{}test}\PYG{p}{[:}\PYG{n}{dim}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{()}
    \PYG{c+c1}{\PYGZsh{} Generate the first input to the trained recurrent neural network using the last two}
    \PYG{c+c1}{\PYGZsh{} points of the training data.  Based on how the network was trained this means that it}
    \PYG{c+c1}{\PYGZsh{} will predict the first point in the data set after the training data.  All of the}
    \PYG{c+c1}{\PYGZsh{} brackets are necessary for Tensorflow.}
    \PYG{n}{next\PYGZus{}input} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{([[[}\PYG{n}{y\PYGZus{}test}\PYG{p}{[}\PYG{n}{dim}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{]],} \PYG{p}{[}\PYG{n}{y\PYGZus{}test}\PYG{p}{[}\PYG{n}{dim}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]]]])}
    \PYG{c+c1}{\PYGZsh{} Save the very last point in the training data set.  This will be used later.}
    \PYG{n}{last} \PYG{o}{=} \PYG{p}{[}\PYG{n}{y\PYGZus{}test}\PYG{p}{[}\PYG{n}{dim}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]]}

    \PYG{c+c1}{\PYGZsh{} Iterate until the complete data set is created.}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range} \PYG{p}{(}\PYG{n}{dim}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{)):}
        \PYG{c+c1}{\PYGZsh{} Predict the next point in the data set using the previous two points.}
        \PYG{n+nb}{next} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{next\PYGZus{}input}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} Append just the number of the predicted data set}
        \PYG{n}{y\PYGZus{}pred}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n+nb}{next}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{][}\PYG{l+m+mi}{0}\PYG{p}{])}
        \PYG{c+c1}{\PYGZsh{} Create the input that will be used to predict the next data point in the data set.}
        \PYG{n}{next\PYGZus{}input} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{([[}\PYG{n}{last}\PYG{p}{,} \PYG{n+nb}{next}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]]],} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}
        \PYG{n}{last} \PYG{o}{=} \PYG{n+nb}{next}

    \PYG{c+c1}{\PYGZsh{} Print the mean squared error between the known data set and the predicted data set.}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}MSE: \PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{subtract}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{))}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{())}
    \PYG{c+c1}{\PYGZsh{} Save the predicted data set as a csv file for later use}
    \PYG{n}{name} \PYG{o}{=} \PYG{n}{datatype} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}Predicted\PYGZsq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{dim}\PYG{p}{)}\PYG{o}{+}\PYG{l+s+s1}{\PYGZsq{}.csv\PYGZsq{}}
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{savetxt}\PYG{p}{(}\PYG{n}{name}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{delimiter}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{},\PYGZsq{}}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Plot the known data set and the predicted data set.  The red box represents the region that was used}
    \PYG{c+c1}{\PYGZsh{} for the training data.}
    \PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{()}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x1}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}true\PYGZdq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x1}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}g\PYGZhy{}.\PYGZsq{}}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}predicted\PYGZdq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{()}
    \PYG{c+c1}{\PYGZsh{} Created a red region to represent the points used in the training data.}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{axvspan}\PYG{p}{(}\PYG{n}{plot\PYGZus{}min}\PYG{p}{,} \PYG{n}{plot\PYGZus{}max}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.25}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}red\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{()}

\PYG{c+c1}{\PYGZsh{} Check to make sure the data set is complete}
\PYG{k}{assert} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X\PYGZus{}tot}\PYG{p}{)} \PYG{o}{==} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y\PYGZus{}tot}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} This is the number of points that will be used in as the training data}
\PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{12}

\PYG{c+c1}{\PYGZsh{} Separate the training data from the whole data set}
\PYG{n}{X\PYGZus{}train} \PYG{o}{=} \PYG{n}{X\PYGZus{}tot}\PYG{p}{[:}\PYG{n}{dim}\PYG{p}{]}
\PYG{n}{y\PYGZus{}train} \PYG{o}{=} \PYG{n}{y\PYGZus{}tot}\PYG{p}{[:}\PYG{n}{dim}\PYG{p}{]}


\PYG{c+c1}{\PYGZsh{} Generate the training data for the RNN, using a sequence of 2}
\PYG{n}{rnn\PYGZus{}input}\PYG{p}{,} \PYG{n}{rnn\PYGZus{}training} \PYG{o}{=} \PYG{n}{format\PYGZus{}data}\PYG{p}{(}\PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} Create a recurrent neural network in Keras and produce a summary of the}
\PYG{c+c1}{\PYGZsh{} machine learning model}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{rnn}\PYG{p}{(}\PYG{n}{length\PYGZus{}of\PYGZus{}sequences} \PYG{o}{=} \PYG{n}{rnn\PYGZus{}input}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{])}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{()}

\PYG{c+c1}{\PYGZsh{} Start the timer.  Want to time training+testing}
\PYG{n}{start} \PYG{o}{=} \PYG{n}{timer}\PYG{p}{()}
\PYG{c+c1}{\PYGZsh{} Fit the model using the training data genenerated above using 150 training iterations and a 5\PYGZpc{}}
\PYG{c+c1}{\PYGZsh{} validation split.  Setting verbose to True prints information about each training iteration.}
\PYG{n}{hist} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{rnn\PYGZus{}input}\PYG{p}{,} \PYG{n}{rnn\PYGZus{}training}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{150}\PYG{p}{,}
                 \PYG{n}{verbose}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{validation\PYGZus{}split}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{label} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}loss\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}val\PYGZus{}loss\PYGZdq{}}\PYG{p}{]:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{hist}\PYG{o}{.}\PYG{n}{history}\PYG{p}{[}\PYG{n}{label}\PYG{p}{],}\PYG{n}{label}\PYG{o}{=}\PYG{n}{label}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}loss\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}epoch\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}The final validation loss: }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{hist}\PYG{o}{.}\PYG{n}{history}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}val\PYGZus{}loss\PYGZdq{}}\PYG{p}{][}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]))}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{()}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{()}

\PYG{c+c1}{\PYGZsh{} Use the trained neural network to predict more points of the data set}
\PYG{n}{test\PYGZus{}rnn}\PYG{p}{(}\PYG{n}{X\PYGZus{}tot}\PYG{p}{,} \PYG{n}{y\PYGZus{}tot}\PYG{p}{,} \PYG{n}{X\PYGZus{}tot}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{],} \PYG{n}{X\PYGZus{}tot}\PYG{p}{[}\PYG{n}{dim}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{])}
\PYG{c+c1}{\PYGZsh{} Stop the timer and calculate the total time needed.}
\PYG{n}{end} \PYG{o}{=} \PYG{n}{timer}\PYG{p}{()}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}Time: \PYGZsq{}}\PYG{p}{,} \PYG{n}{end}\PYG{o}{\PYGZhy{}}\PYG{n}{start}\PYG{p}{)}

\end{Verbatim}
