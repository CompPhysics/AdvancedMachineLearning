The autoencoder is an effective unsupervised learning model which is
widely used in deep learning. It is well known that an autoencoder
with a single fully-connected hidden layer, a linear activation
function and a squared error cost function trains weights that span
the same subspace as the one spanned by the principal component
loading vectors, but that they are not identical to the loading
vectors. In this paper, we show how to recover the loading vectors
from the autoencoder weights.

!split
===== Background =====

Principal Component Analysis (PCA) is a linear transformation that
transforms a set of observations to a new coordinate system in which
the values of the first coordinate have the largest possible variance,
and the values of each succeeding coordinate have the largest possible
variance under the constraint that they are uncorrelated with the
preceding coordinates. They are often found by either computing the
eigendecomposition of the covariance matrix or by computing the
singular value decomposition of the observations.

By keeping only the first few principal components, PCA can be used
for dimensionality reduction. The decorrelation of the coordinates is
also a useful property, and PCA is sometimes used as a preprocessing
step for whitening a dataset before using it as an input into an
optimization problem such as a neural network classifier.

One of the properties of PCA is that out of all possible linear
transformations, the reconstructions of the observations from the
leading principal components have the least total squared
error. Autoencoders are neural networks that aim to minimize the error
of reconstructions of observations. It is well known that an
autoencoder with a single fully-connected hidden layer, a linear
activation function and a squared error cost function is closely
related to PCA - its weights span the principal subspace, which is the
subspace spanned by the first loading vectors \cite{AutoencoderSVD},
\cite{WithoutLocalMinima}. However, they are not equal to the loading
vectors.

This paper proposes a simple method for recovering the loading vectors from the weights of a linear autoencoder. This allows the usage of autoencoders for computing PCA, unlike existing methods which merely find the principal subspace. After recovering the loading vectors, the solution has the following properties: (i) it is unique; (ii) in the transformed data, different coordinates are uncorrelated; (iii) the coordinates are sorted in descending order of variance; and (iv) the solutions for reduction to different dimensions are nested: when reducing the data from dimension $n$ to dimension $m_1$, the first $m_2$ vectors ($m_2 < m_1$) are the same as the solution for reduction from dimension $n$ to $m_2$. These properties do not hold for a general basis for the principal subspace, which is what the autoencoder weights converge to.

Even though any method for computing PCA using autoencoders is highly
inefficient for small datasets, for large datasets such a method is
desirable because of its various advantages compared to the standard
methods. Autoencoders can be trained by a variety of stochastic
optimization methods that have been developed for training deep neural
networks. These optimizers can handle high-dimensional training data
such as images, and a large number of them. They are suitable for the
online learning scenario in which new data arrives over time, and they
do not require subtracting from each example the element-wise mean of
the entire training set. 

!split
===== Principal Component Analysis =====

Let $\left\{ \textbf{y}_i \right\}_{i=1}^N$ be a set of $N$ observation vectors, each of dimension $n$. We assume that $n \leq N$, which is the more common scenario in machine learning, but a similar analysis can be done for the case of $n > N$. Let $\text{Y} \in \mathbb{R}^{n \times N}$ be a matrix whose columns are $\left\{ \textbf{y}_i \right\}_{i=1}^N$,
!bt
\[
\text{Y} = \left[\begin{array}{ccc}
\mid &  & \mid\\
\textbf{y}_{1} & \cdots & \textbf{y}_{N}\\
\mid &  & \mid
\end{array}\right].
\]
!et
The element-wise average of the $N$ observations is an $n$ dimensional signal which may be written as:
\[
\bar{\textbf{y}} = \frac{1}{N}\overset{N}{\underset{i=1}{\sum}} \textbf{y}_i = \frac{1}{N} \text{Y} \mathbb{1}_N,
\]
where $\mathbb{1}_N \in \mathbb{R}^{N \times 1}$ is a column vector of all-ones. Let $\text{Y}_0$ be a matrix whose columns are the centered observations (we center each observation $\textbf{y}_i$ by subtracting $\bar{\textbf{y}}$ from it): 
\[
\text{Y}_0 = \text{Y} - \bar{\textbf{y}} \mathbb{1}_N^T.
\]

A linear transformation of a finite dimensional vector may be expressed as a matrix multiplication:
\[
\textbf{x}_i = \text{W}^T\textbf{y}_i,
\]
where $\textbf{y}_i \in \mathbb{R}^n$, $\textbf{x}_i \in \mathbb{R}^m$, and $\text{W} \in \mathbb{R}^{n \times m}$.
Each element $j$ in the vector $\textbf{x}_i$ is an inner product between $\textbf{y}_i$ and the $j$-th column of $\text{W}$, which we denote by $\textbf{w}_j$.

Let $\text{X} \in \mathbb{R}^{m \times N}$ be a matrix whose columns are the set of $N$ vectors of transformed observations, let $\bar{\textbf{x}} = \frac{1}{N}\overset{N}{\underset{i=1}{\sum}} \textbf{x}_i = \frac{1}{N} \text{X} \mathbb{1}_N$ be the element-wise average, and $\text{X}_0 = \text{X} - \bar{\textbf{x}} \mathbb{1}_N^T$ the centered matrix. Clearly, $\text{X} = \text{W}^T \text{Y}$ and $\text{X}_0 = \text{W}^T \text{Y}_0$. 

When the matrix $\text{W}^T$ represents the transformation that applies principal component analysis, we denote $\text{W}=\text{P}$, and the columns of $\text{P}$, denoted $\left\{ \textbf{p}_j \right\}_{j=1}^n$, are referred to as \emph{loading vectors}. The transformed vectors $\left\{ \textbf{x}_i \right\}_{i=1}^N$ are referred to as \emph{principal components} or \emph{scores}. The first loading vector is defined as the unit vector with which the inner products of the observations have the greatest variance: 
\begin{equation}
\textbf{p}_1 = \underset{\textbf{w}_1}{\max}{~\textbf{w}_1^T \text{Y}_0 \text{Y}_0^T \textbf{w}_1 }~s.t.~\textbf{w}_1^T \textbf{w}_1 = 1.
\label{eq:maxvar}
\end{equation}
The solution to (\ref{eq:maxvar}) is known to be the eigenvector of the sample covariance matrix $\text{Y}_0 \text{Y}_0^T$ corresponding to its largest eigenvalue. We normalize the eigenvector and disregard its sign. Next, $\textbf{p}_2$ is the unit vector which has the largest variance of inner products between it and the observations after removing the orthogonal projections of the observations onto $\textbf{p}_1$. It may be found by solving:
\begin{multline}
\textbf{p}_2 = \underset{\textbf{w}_2}{\max}{~\textbf{w}_2^T \left( \text{Y}_0 - \textbf{p}_1 \textbf{p}_1^T \text{Y}_0 \right) \left( \text{Y}_0 - \textbf{p}_1 \textbf{p}_1^T \text{Y}_0 \right)^T \textbf{w}_2 }\\~s.t.~ \textbf{w}_2^T \textbf{w}_2 = 1.
\label{eq:p2}
\end{multline}
The solution to (\ref{eq:p2}) is known to be the eigenvector corresponding to the largest eigenvalue under the constraint that it is not collinear with $\textbf{p}_1$. Similarly, the remaining loading vectors are equal to the remaining eigenvectors of $\text{Y}_0 \text{Y}_0^T$ corresponding to descending eigenvalues. The eigenvalues of $\text{Y}_0 \text{Y}_0^T$, which is a positive semi-definite matrix, are non-negative. They are not necessarily distinct, but since it is a symmetric matrix it has $n$ eigenvectors that are all orthogonal, and it is always diagonalizable. Thus, the matrix $\text{P}$ may be computed by diagonalizing the covariance matrix:
\[
\text{Y}_0 \text{Y}_0^T = \text{P} \Lambda \text{P}^{-1} = \text{P} \Lambda \text{P}^T,
\]
where $\Lambda = \text{X}_0 \text{X}_0^T$ is a diagonal matrix whose diagonal elements $\left\{ \lambda_i \right\}_{i=1}^n$ are sorted in descending order, the columns of $\text{P}$ are orthonormal, i.e. $\text{P}^{-1}=\text{P}^T$. The transformation back to the observations is $\text{Y} = \text{P} \text{X}$. The fact that the covariance matrix of $\text{X}$ is diagonal means that PCA is a decorrelation transformation. By dividing each coordinate by the square root of its corresponding eigenvalue, PCA can be used as a whitening transformation, which is sometimes used as a preprocessing step in order to cause optimization problems to converge more easily.

!split
===== Dimensionality reduction =====
PCA is often used as a method for dimensionality reduction, the process of reducing the number of variables in a model in order to avoid the curse of dimensionality. This is done by simply keeping the first $m$ principal components ($m<n$), i.e., applying the truncated transformation
\[
\text{X}_m = \text{P}_m^T \text{Y},
\]
where each column of $\text{X}_m \in \mathbb{R}^{m \times N}$ is a vector whose elements are the first $m$ principal components, and $\text{P}_m$ is a matrix whose columns are the first $m$ loading vectors,
\[
\text{P}_m = \left[\begin{array}{ccc}
\mid &  & \mid\\
\textbf{p}_{1} & \cdots & \textbf{p}_{m}\\
\mid &  & \mid
\end{array}\right] \in \mathbb{R}^{n \times m}.
\]
Intuitively, by keeping only $m$ principal components, we are losing information, and we minimize this loss of information by maximizing their variances. Many iterative algorithms (e.g., QR algorithm, Jacobi algorithm, and the power method) can efficiently find the $m$ largest eigenvalues of $\text{Y}_0 \text{Y}_0^T$ and their corresponding eigenvectors without having to fully diagonalize the matrix. Yet, for high dimensional data, computing $\text{Y}_0 \text{Y}_0^T$ may be prohibitive.

\subsection{Singular-value decomposition}
Any matrix $\text{Y}_0 \in \mathbb{R}^{n \times N}$ may be factorized as $\text{Y}_0 = \text{U} \Sigma \text{V}^T$, where $\text{U} \in \mathbb{R}^{n \times n}$ and $\text{V} \in \mathbb{R}^{N \times N}$ are both orthogonal matrices and $\Sigma \in \mathbb{R}^{n \times N}$ is a matrix whose elements are non-negative real numbers on the diagonal and zero elsewhere. The diagonal elements $\left\{ \sigma_i \right\}_{i=1}^n$, referred to as singular values, are sorted in descending order, and the columns of $\text{U}$ and $\text{V}$ are referred to, respectively, as left and right singular vectors. Assuming $n \leq N$ and $\text{Y}_0$ is full-rank, the columns of $\text{U}$ are an orthonormal basis for $\mathbb{R}^n$. If $n > N$ (the number of observation is smaller than the dimension of each observation), then the first $n$ columns of $\text{U}$ are an orthonormal basis for the column space of $\text{Y}_0$, and the remaining $N-n$ columns are an orthonormal basis for its nullspace.

The covariance matrix may be written as:
\[
\text{Y}_0 \text{Y}_0^T = \text{U} \Sigma \text{V}^T \text{V} \Sigma^T  \text{U}^T =  \text{U} \Sigma \Sigma^T \text{U}^{-1},
\]
where $\Sigma \Sigma^T$ is a diagonal matrix. Thus, singular-value decomposition of $\text{Y}_0$ is equivalent to eigendecomposition of $\text{Y}_0 \text{Y}_0^T$. The singular values of $\text{Y}_0$ are the square roots of the eigenvalues of $\text{Y}_0 \text{Y}_0^T$, and the left singular vectors of $\text{Y}_0$ are the eigenvectors of $\text{Y}_0 \text{Y}_0^T$. Eigendecomposition is unique up to the scale of the eigenvectors, which we normalize, and to permutations of the eigenvectors and their corresponding eigenvalues, which we sort in descending order. Therefore, the left singular vectors of $\text{Y}_0$ must be equal to the loading vectors of $\text{Y}$ (up to their sign, which we disregard).

Iterative algorithms (e.g., QR algorithm, one-sided Jacobi algorithm) can efficiently find the $m$ largest singular values and their corresponding left singular vectors without having to fully decompose the matrix.

\subsection{PCA and large sets of high-dimensional data}
When the number of observations $N$ is small enough to fit in memory, SVD is often the preferred method for computing the loading vectors, as it avoids computing the covariance matrix $\text{Y}_0 \text{Y}_0^T$, which is desirable especially when $n$ is large. 

When the number of observations $N$ is large but the dimension $n$ is sufficiently small, $\text{Y}_0 \text{Y}_0^T = \overset{N}{\underset{i=1}{\sum}} \left(\textbf{y}_i-\bar{\textbf{y}}\right) \left(\textbf{y}_i-\bar{\textbf{y}}\right)^T$ may be computed sequentially, with a memory requirement of $O \left( n^2 \right)$ instead of $O \left( nN \right)$. Thus, there is no need to load the entire dataset into memory.

However, when the number of observations is large and each observation is high-dimensional (i.e., both $N$ and $n$ are large), this too may be infeasible. Online methods \cite{Online1}, \cite{Online2}, \cite{Online3} iterate through the dataset one example at a time or in minibatches. When applied to images, the images are more often divided into small patches, and PCA is applied to patches rather than to the entire images; this is known as local PCA \cite{LocalPCA}.

\subsection{Minimum total squared reconstruction error}
Interestingly, $\text{P}_m$ is also a solution to:
\begin{equation}
\underset{\text{W} \in \mathbb{R}^{n \times m}}{\text{min}}{~\left\Vert \text{Y}_0-\text{W} \text{W}^T \text{Y}_0 \right\Vert_F^2}~s.t.~ \text{W}^T \text{W} = \text{I}_{m \times m},
\label{eq:MMSE}
\end{equation}
where $F$ denotes the Frobenius norm. According to this formulation, the $m$ leading loading vectors are an orthonormal basis which spans the $m$ dimensional subspace onto which the projections of the centered observations have the minimum squared difference from the original centered observations. In other words, $\text{P}_m$ compresses each centered vector of length $n$ into a vector of length $m$ (where $m \leq n$) in such a way that minimizes the sum of total squared reconstruction errors.

It is well known \cite{EckardtYoung} and easily verified that $\text{P}_m$ indeed solves (\ref{eq:MMSE}). Yet, the minimizer of (\ref{eq:MMSE}) is not unique: $\text{W} = \text{P}_m \text{Q}$ is also a solution, where $\text{Q} \in \mathbb{R}^{m \times m}$ is any orthogonal matrix, $\text{Q}^T = \text{Q}^{-1}$. Multiplying $\text{P}_m$ from the right by $\text{Q}$ transforms the first $m$ loading vectors into a different orthonormal basis for the same subspace.

\subsection{Autoencoders}
A neural network that is trained to learn the identity function is called an autoencoder. Its output layer has the same number of nodes as the input layer, and the cost function is some measure of the reconstruction error. Autoencoders are unsupervised learning models, and they are often used for the purpose of dimensionality reduction \cite{Dimreduction}. A simple autoencoder that implements dimensionality reduction is a feed-forward autoencoder with at least one layer that has a smaller number of nodes, which functions as a bottleneck. After training the neural network using backpropagation, it is separated into two parts: the layers up to the bottleneck are used as an encoder, and the remaining layers are used as a decoder.

In the simplest case, there is only one hidden layer (the bottleneck), and the layers in the network are fully connected. A vector $\textbf{y}_i \in \mathbb{R}^{n \times 1}$ passes through the hidden layer, which outputs $\textbf{x}_i \in \mathbb{R}^{m \times 1}$ according to the mapping $\textbf{x}_i = a\left( \text{W}_1 \textbf{y}_i + \textbf{b}_1 \right)$, where $\text{W}_1 \in \mathbb{R}^{m \times n}$ is referred to as the weight matrix of the first layer, $\textbf{b}_1 \in \mathbb{R}^{m \times 1}$ is referred to as the bias vector of the first layer, and $m < n$. The function $a$, referred to as the activation function, operates element-wise and is typically a non-linear function such as the rectified linear unit (ReLU). The second layer maps $\textbf{x}_i$ to $\hat{\textbf{y}}_i \in \mathbb{R}^{n \times 1}$ according to $\hat{\textbf{y}}_i = a\left( \text{W}_2 \textbf{x}_i + \textbf{b}_2 \right) = a\left( \text{W}_2 a\left( \text{W}_1 \textbf{y}_i + \textbf{b}_1 \right)+ \textbf{b}_2 \right)$, where $\text{W}_2 \in \mathbb{R}^{n \times m}$ and $\textbf{b}_2 \in \mathbb{R}^{n \times 1}$ are the weight matrix and bias vector of the second layer. The parameters $\text{W}_1, \textbf{b}_1, \text{W}_2, \textbf{b}_2$ are found by minimizing some cost function measuring the difference between the output $\hat{\textbf{y}}_i$ and the input $\textbf{y}_i$. Using backpropagation with an optimizer such as stochastic gradient descent, each data sample from $\left\{ \textbf{y}_i \right\}_{i=1}^N$ is fed through the network to compute $\textbf{x}_i$ and $\hat{\textbf{y}}_i$, which are then used to compute the gradients and to update the parameters.

\subsection{Linear autoencoders}
In the case that no non-linear activation function is used, $\textbf{x}_i = \text{W}_1 \textbf{y}_i + \textbf{b}_1$ and $\hat{\textbf{y}}_i = \text{W}_2 \textbf{x}_i + \textbf{b}_2$. If the cost function is the total squared difference between output and input, then training the autoencoder on the input data matrix $\text{Y}$ solves:
\begin{equation}
\underset{\text{W}_1, \textbf{b}_1, \text{W}_2, \textbf{b}_2 }{\text{min}}{~
\left\Vert \text{Y} - \left(
\text{W}_2 \left(\text{W}_1 \text{Y} + \textbf{b}_1 \mathbb{1}_N^T \right) + \textbf{b}_2 \mathbb{1}_N^T
\right) \right\Vert_F^2 }.
\label{eq:NNMSE}
\end{equation}
In \cite{AutoencoderSVD}, it is shown that if we set the partial derivative with respect to $\textbf{b}_2$ to zero and insert the solution into (\ref{eq:NNMSE}), then the problem becomes:
\[
\underset{\text{W}_1, \text{W}_2 }{\text{min}}{~\left\Vert \text{Y}_0 - \text{W}_2 \text{W}_1 \text{Y}_0 \right\Vert_F^2}
\]
Thus, for any $\textbf{b}_1$, the optimal $\textbf{b}_2$ is such that the problem becomes independent of $\textbf{b}_1$ and of $\bar{\textbf{y}}$. Therefore, we may focus only on the weights $\text{W}_1$, $\text{W}_2$. In \cite{WithoutLocalMinima} it is shown that when setting the gradients to zero, $\text{W}_1$ is the left Moore-Penrose pseudoinverse of $\text{W}_2$ (and $\text{W}_2$ is the right pseudoinverse of $\text{W}_1$):
\[
\text{W}_1 = \text{W}_2^{\dagger} = \left( \text{W}_2^T \text{W}_2 \right)^{-1} \text{W}_2^T
\]
Thus, the minimization remains with respect to a single matrix:
\begin{equation}
\underset{\text{W}_2 \in \mathbb{R}^{n \times m}}{\text{min}}{~\left\Vert \text{Y}_0 - \text{W}_2 \text{W}_2^{\dagger} \text{Y}_0 \right\Vert_F^2}
\label{eq:NNsubspace}
\end{equation}
The matrix $\text{W}_2 \text{W}_2^{\dagger} = \text{W}_2 \left( \text{W}_2^T \text{W}_2 \right)^{-1} \text{W}_2^T$ is the orthogonal projection operator onto the column space of $\text{W}_2$ when its columns are not necessarily orthonormal. This problem is very similar to (\ref{eq:MMSE}), but without the orthonormality constraint.

In \cite{AutoencoderSVD} and \cite{WithoutLocalMinima} it is shown that $\text{W}_2$ is a minimizer of (\ref{eq:NNsubspace}) if and only if its column space is spanned by the first $m$ loading vectors of $\text{Y}$. This can also be shown by applying the QR decomposition to $\text{W}_2$, which transforms the problem (\ref{eq:NNsubspace}) into the one in (\ref{eq:MMSE}). As a result, it is possible to solve (\ref{eq:MMSE}) by first solving the unconstrained problem (\ref{eq:NNsubspace}), and then orthonormalizing the columns of the solution, e.g. using the Gram-Schmidt process. However, this does not recover the loading vectors $\text{P}_m$, but rather $\text{P}_m \text{Q}$ for some unknown orthogonal matrix $\text{Q}$.

The linear autoencoder is said to apply PCA to the input data in the sense that its output is a projection of the data onto the low dimensional principal subspace. However, unlike actual PCA, the coordinates of the output of the bottleneck are correlated and are not sorted in descending order of variance. In addition, the solutions for reduction to different dimensions are not nested: when reducing the data from dimension $n$ to dimension $m_1$, the first $m_2$ vectors ($m_2 < m_1$) are not an optimal solution to reduction from dimension $n$ to $m_2$, which therefore requires training an entirely new autoencoder.

Several methods have been proposed for neural networks that compute the exact loading vectors \cite{NNPCA1}, \cite{NNPCA2}, \cite{NNPCA3}, \cite{NNPCA4}, \cite{NNPCA5}. However, they require specific algorithms for iteratively updating the weights, and as such are similar to online PCA methods. No method has so far been proposed for recovering the loading vectors from a simple linear autoencoder that is independent of the optimization method used for training it.

\section{Method}
%Recovering the loading vectors from the weights of a linear autoencoder (\ref{eq:NNsubspace}) is remarkably simple.

\emph{Hypothesis}: The first $m$ loading vectors of $\text{Y}$ are the first $m$ left singular vectors of the matrix $\text{W}_2$ which minimizes (\ref{eq:NNsubspace}).

\emph{Note: an earlier version of this work attempted to prove this statement, but was found to be erroneous.}

\begin{comment}
\emph{Proof:}

Assuming $\text{W}_2 \in \mathbb{R}^{n \times m}$ is a full-rank matrix and $m \leq n$, i.e., $\text{rank}\left( \text{W}_2 \right) = m$, then $\text{rank}\left( \text{W}_2 \text{W}_2^{\dagger} \right)=m$. This is a property of the orthogonal projection matrix \cite{Freedman}. 
Then, $\text{rank}\left( \text{W}_2 \text{W}_2^{\dagger} \text{Y}_0 \right) = m$, assuming $\text{rank}\left( \text{Y}_0 \right) \geq m$.

Denote $ \hat{\text{Y}}_0 = \text{W}_2 \text{W}_2^{\dagger} \text{Y}_0$. Then, (\ref{eq:NNsubspace}) can be regarded as the problem of optimizing a low rank approximation in terms of the Frobenius norm:
\begin{equation}
\underset{\hat{\text{Y}}_0}{\text{min}}{~\left\Vert \text{Y}_0 - \hat{\text{Y}}_0 \right\Vert_F^2}~~s.t.~~\text{rank}\left( \hat{\text{Y}}_0 \right) = m.
\label{eq:lowrank}
\end{equation}
The Eckart-Young-Mirsky theorem for the Frobenius norm \cite{EckardtYoung}, \cite{Antoulas} states that the best rank $m$ approximation for a matrix whose singular value decomposition is $\text{Y}_0 = \text{P} \text{S} \text{Q}^T = \overset{n}{\underset{j=1}{\sum}} s_j \textbf{p}_j \textbf{q}_j^T$ is $\hat{\text{Y}}_0 = \overset{m}{\underset{j=1}{\sum}} s_j \textbf{p}_j \textbf{q}_j^T$. Furthermore, the theorem states that this solution is the unique global minimizer of (\ref{eq:lowrank}) provided that $s_m \neq s_{m+1}$ (which may be assumed for all practical purposes). Therefore, a matrix $\text{W}_2$ satisfies \[\text{W}_2 \text{W}_2^{\dagger} \text{Y}_0 = \hat{\text{Y}}_0 = \overset{m}{\underset{j=1}{\sum}} s_j \textbf{p}_j \textbf{q}_j^T\] if and only if it is a (non-unique) global minimizer of (\ref{eq:NNsubspace}).

Let $\text{W}_2 = \text{U} \Sigma \text{V}^T = \overset{n}{\underset{j=1}{\sum}} \sigma_j \textbf{u}_j \textbf{v}_j^T$ be the singular value decomposition of $\text{W}_2 \in \mathbb{R}^{n \times m}$, which we assume is full-rank. As $m \leq n$, the first $m$ left singular vectors, $\left\{ \textbf{u}_j \right\}_{j=1}^m$, are an orthonormal basis for the column space of $\text{W}_2$, and the remaining $n-m$ left singular vectors, $\left\{ \textbf{u}_j \right\}_{j=m+1}^n$, are an orthonormal basis for its nullspace. Then,
\begin{equation}
\text{W}_2 \text{W}_2^{\dagger} = \text{U} \Sigma \text{V}^T \text{V} \Sigma^{\dagger} \text{U}^T = \text{U} \Sigma \Sigma^{\dagger} \text{U}^T = \text{U}_m \text{U}_m^T,
\label{eq:eigu}
\end{equation}
where we used the fact that $\left( \text{V}^T \right)^{\dagger} = \text{V}$ and that $\Sigma^{\dagger} \in \mathbb{R}^{m \times n}$ is a matrix whose diagonal elements are $\frac{1}{\sigma_j}$ (assuming $\sigma_j \neq 0$, and $0$ otherwise). The matrix $\Sigma \Sigma^{\dagger}$ is a diagonal matrix whose first $m$ diagonal elements are equal to one and the other $n-m$ elements are equal to zero. The matrix $\text{U}_m \in \mathbb{R}^{n \times m}$ is a matrix whose columns are the first $m$ left singular vectors of $\text{W}_2$. Then, $\text{W}_2$ is a solution to (\ref{eq:NNsubspace}) if and only if
\begin{equation}
\text{U}_m \text{U}_m^T \overset{n}{\underset{i=1}{\sum}} s_i \textbf{p}_i \textbf{q}_i^T = \overset{m}{\underset{j=1}{\sum}} s_j \textbf{p}_j \textbf{q}_j^T,
\label{eq:uuw}
\end{equation}
We multiply both sides of the equation from the right by $\textbf{q}_k$ for some $1 \leq k \leq m$, use the orthonormality of the right singular vectors, and divide both sides of the equation by $s_k$ (which is non-zero assuming $\text{W}_2$ is full-rank):
\begin{equation}
\text{U}_m \text{U}_m^T \textbf{p}_k = \textbf{p}_k.
\label{eq:eigup}
\end{equation}
This familiar equation states that $\textbf{p}_k$ is an eigenvector of $\text{U}_m \text{U}_m^T$ with an eigenvalue of one.  But from (\ref{eq:eigu}), we know that the eigendecomposition of $\text{U}_m \text{U}_m^T$ is $\text{U} \Sigma \Sigma^{\dagger} \text{U}^{-1}$, and eigendecomposition is unique up to the scale of the eigenvectors, which we normalize, and to permutations of the eigenvectors and their corresponding eigenvalues, which we sort in descending order. Thus, the first $m$ eigenvectors of $\text{U}_m \text{U}_m^T$ are the columns of $\text{U}_m$ themselves, and they correspond to an eigenvalue of one. The other $m-n$ eigenvectors correspond to an eigenvalue of zero. Since (\ref{eq:eigup}) must be true for all $1 \leq k \leq m$, the first $m$ left singular vectors of $\text{W}_2$ must be equal (up to sign) to the first $m$ loading vectors of $\text{Y}$. $\blacksquare$
\end{comment}
In other words, instead of computing the first $m$ left singular vectors of $\text{Y}_0 \in \mathbb{R}^{n \times N}$, we may train a linear autoencoder on the (non-centered) dataset $\text{Y}$ and then compute the first $m$ left singular vectors of $\text{W}_2 \in \mathbb{R}^{n \times m}$, where typically $m << N$. The loading vectors may also be recovered from the weights of the hidden layer, $\text{W}_1$. If $\text{W}_2 = \text{U} \Sigma \text{V}^T$, then $\text{W}_1 = \text{W}_2^{\dagger} = \text{V} \Sigma^{\dagger} \text{U}^T$, and $\text{W}_1^T = \text{U} \left( \Sigma^{\dagger} \right)^T \text{V}^T$. Thus, the first $m$ left singular vectors of $\text{W}_1^T \in \mathbb{R}^{n \times m}$ are also equal to the first $m$ loading vectors of $\text{Y}$.


