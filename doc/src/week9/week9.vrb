\frametitle{Other techniques}

There are many other dimensionality reduction techniques, several of which are available in Scikit-Learn.

Here are some of the most popular:
\begin{itemize}
\item \textbf{Multidimensional Scaling (MDS)} reduces dimensionality while trying to preserve the distances between the instances.

\item \textbf{Isomap} creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the geodesic distances between the instances.

\item \textbf{t-Distributed Stochastic Neighbor Embedding} (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space (e.g., to visualize the MNIST images in 2D).

\item Linear Discriminant Analysis (LDA) is actually a classification algorithm, but during training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane onto which to project the data. The benefit is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm such as a Support Vector Machine (SVM) classifier discussed in the SVM lectures.
\end{itemize}

\noindent
