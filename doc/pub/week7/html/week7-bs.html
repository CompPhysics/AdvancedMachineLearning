<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week7.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week7-bs --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Advanced machine learning and data analysis for the physical sciences">
<title>Advanced machine learning and data analysis for the physical sciences</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html week7.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week7-bs --no_mako -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plans for the week of March 3-7',
               2,
               None,
               'plans-for-the-week-of-march-3-7'),
              ('What is a recurrent NN?', 2, None, 'what-is-a-recurrent-nn'),
              ('Why RNNs?', 2, None, 'why-rnns'),
              ('Basic layout,  "Figures from Sebastian Rashcka et al, Machine '
               'learning with Sickit-Learn and '
               'PyTorch":"https://sebastianraschka.com/blog/2022/ml-pytorch-book.html"',
               2,
               None,
               'basic-layout-figures-from-sebastian-rashcka-et-al-machine-learning-with-sickit-learn-and-pytorch-https-sebastianraschka-com-blog-2022-ml-pytorch-book-html'),
              ('RNNs in more detail', 2, None, 'rnns-in-more-detail'),
              ('RNNs in more detail, part 2',
               2,
               None,
               'rnns-in-more-detail-part-2'),
              ('RNNs in more detail, part 3',
               2,
               None,
               'rnns-in-more-detail-part-3'),
              ('RNNs in more detail, part 4',
               2,
               None,
               'rnns-in-more-detail-part-4'),
              ('RNNs in more detail, part 5',
               2,
               None,
               'rnns-in-more-detail-part-5'),
              ('RNNs in more detail, part 6',
               2,
               None,
               'rnns-in-more-detail-part-6'),
              ('RNNs in more detail, part 7',
               2,
               None,
               'rnns-in-more-detail-part-7'),
              ('Backpropagation through time',
               2,
               None,
               'backpropagation-through-time'),
              ('The backward pass is linear',
               2,
               None,
               'the-backward-pass-is-linear'),
              ('The problem of exploding or vanishing gradients',
               2,
               None,
               'the-problem-of-exploding-or-vanishing-gradients'),
              ('The mathematics of RNNs, the basic architecture',
               2,
               None,
               'the-mathematics-of-rnns-the-basic-architecture'),
              ('Four effective ways to learn an RNN and preparing for next '
               'week',
               2,
               None,
               'four-effective-ways-to-learn-an-rnn-and-preparing-for-next-week'),
              ('Long Short Term Memory (LSTM)',
               2,
               None,
               'long-short-term-memory-lstm')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week7-bs.html">Advanced machine learning and data analysis for the physical sciences</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="#plans-for-the-week-of-march-3-7" style="font-size: 80%;">Plans for the week of March 3-7</a></li>
     <!-- navigation toc: --> <li><a href="#what-is-a-recurrent-nn" style="font-size: 80%;">What is a recurrent NN?</a></li>
     <!-- navigation toc: --> <li><a href="#why-rnns" style="font-size: 80%;">Why RNNs?</a></li>
     <!-- navigation toc: --> <li><a href="#basic-layout-figures-from-sebastian-rashcka-et-al-machine-learning-with-sickit-learn-and-pytorch-https-sebastianraschka-com-blog-2022-ml-pytorch-book-html" style="font-size: 80%;">Basic layout,  "Figures from Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch":"https://sebastianraschka.com/blog/2022/ml-pytorch-book.html"</a></li>
     <!-- navigation toc: --> <li><a href="#rnns-in-more-detail" style="font-size: 80%;">RNNs in more detail</a></li>
     <!-- navigation toc: --> <li><a href="#rnns-in-more-detail-part-2" style="font-size: 80%;">RNNs in more detail, part 2</a></li>
     <!-- navigation toc: --> <li><a href="#rnns-in-more-detail-part-3" style="font-size: 80%;">RNNs in more detail, part 3</a></li>
     <!-- navigation toc: --> <li><a href="#rnns-in-more-detail-part-4" style="font-size: 80%;">RNNs in more detail, part 4</a></li>
     <!-- navigation toc: --> <li><a href="#rnns-in-more-detail-part-5" style="font-size: 80%;">RNNs in more detail, part 5</a></li>
     <!-- navigation toc: --> <li><a href="#rnns-in-more-detail-part-6" style="font-size: 80%;">RNNs in more detail, part 6</a></li>
     <!-- navigation toc: --> <li><a href="#rnns-in-more-detail-part-7" style="font-size: 80%;">RNNs in more detail, part 7</a></li>
     <!-- navigation toc: --> <li><a href="#backpropagation-through-time" style="font-size: 80%;">Backpropagation through time</a></li>
     <!-- navigation toc: --> <li><a href="#the-backward-pass-is-linear" style="font-size: 80%;">The backward pass is linear</a></li>
     <!-- navigation toc: --> <li><a href="#the-problem-of-exploding-or-vanishing-gradients" style="font-size: 80%;">The problem of exploding or vanishing gradients</a></li>
     <!-- navigation toc: --> <li><a href="#the-mathematics-of-rnns-the-basic-architecture" style="font-size: 80%;">The mathematics of RNNs, the basic architecture</a></li>
     <!-- navigation toc: --> <li><a href="#four-effective-ways-to-learn-an-rnn-and-preparing-for-next-week" style="font-size: 80%;">Four effective ways to learn an RNN and preparing for next week</a></li>
     <!-- navigation toc: --> <li><a href="#long-short-term-memory-lstm" style="font-size: 80%;">Long Short Term Memory (LSTM)</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<!-- ------------------- main content ---------------------- -->
<div class="jumbotron">
<center>
<h1>Advanced machine learning and data analysis for the physical sciences</h1>
</center>  <!-- document title -->

<!-- author(s): Morten Hjorth-Jensen -->
<center>
<b>Morten Hjorth-Jensen</b> 
</center>
<!-- institution -->
<center>
<b>Department of Physics and Center for Computing in Science Education, University of Oslo, Norway</b>
</center>
<br>
<center>
<h4>March 6</h4>
</center> <!-- date -->
<br>

<!-- potential-jumbotron-button -->
</div> <!-- end jumbotron -->

<!-- !split -->
<h2 id="plans-for-the-week-of-march-3-7" class="anchor">Plans for the week of March 3-7  </h2>

<div class="panel panel-default">
<div class="panel-body">
<!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
<ol>
<li> Reminder on basics of recurrent neural networks (RNNs)</li>
<li> Mathematics of RNNs</li>
<li> Writing our own codes for RNNs</li>
<li> <a href="https://youtu.be/MeYh5rGIRBM" target="_self">Video of lecture</a></li>
<li> "Whiteboard notes":"https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/HandwrittenNotes/2025/NotesMarch6.pdf</li>
<li> Reading recommendations:
<ol type="a"></li>
 <li> Goodfellow, Bengio and Courville's chapter 10 from <a href="https://www.deeplearningbook.org/" target="_self">Deep Learning</a></li>
 <li> <a href="https://sebastianraschka.com/blog/2022/ml-pytorch-book.html" target="_self">Sebastian Rashcka et al, chapter 15, Machine learning with Sickit-Learn and PyTorch</a></li>
 <li> <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html" target="_self">David Foster, Generative Deep Learning with TensorFlow, see chapter 5</a></li>
</ol>
</ol>
</div>
</div>


<p>The last two books have codes for RNNs in PyTorch and TensorFlow/Keras.</p>

<!-- !split -->
<h2 id="what-is-a-recurrent-nn" class="anchor">What is a recurrent NN? </h2>

<p>A recurrent neural network (RNN), as opposed to a regular fully
connected neural network (FCNN) or just neural network (NN), has
layers that are connected to themselves.
</p>

<p>In an FCNN there are no connections between nodes in a single
layer. For instance, \( (h_1^1 \) is not connected to \( (h_2^1 \). In
addition, the input and output are always of a fixed length.
</p>

<p>In an RNN, however, this is no longer the case. Nodes in the hidden
layers are connected to themselves.
</p>

<!-- !split -->
<h2 id="why-rnns" class="anchor">Why RNNs? </h2>

<p>Recurrent neural networks work very well when working with
sequential data, that is data where the order matters. In a regular
fully connected network, the order of input doesn't really matter.
</p>

<p>Another property of  RNNs is that they can handle variable input
and output. Consider again the simplified breast cancer dataset. If you
have trained a regular FCNN on the dataset with the two features, it
makes no sense to suddenly add a third feature. The network would not
know what to do with it, and would reject such inputs with three
features (or any other number of features that isn't two, for that
matter).
</p>

<!-- !split -->
<h2 id="basic-layout-figures-from-sebastian-rashcka-et-al-machine-learning-with-sickit-learn-and-pytorch-https-sebastianraschka-com-blog-2022-ml-pytorch-book-html" class="anchor">Basic layout,  <a href="https://sebastianraschka.com/blog/2022/ml-pytorch-book.html" target="_self">Figures from Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch</a> </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN1.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="rnns-in-more-detail" class="anchor">RNNs in more detail  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN2.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="rnns-in-more-detail-part-2" class="anchor">RNNs in more detail, part 2  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN3.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="rnns-in-more-detail-part-3" class="anchor">RNNs in more detail, part 3  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN4.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="rnns-in-more-detail-part-4" class="anchor">RNNs in more detail, part 4  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN5.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="rnns-in-more-detail-part-5" class="anchor">RNNs in more detail, part 5  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN6.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="rnns-in-more-detail-part-6" class="anchor">RNNs in more detail, part 6  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN7.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="rnns-in-more-detail-part-7" class="anchor">RNNs in more detail, part 7  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN8.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="backpropagation-through-time" class="anchor">Backpropagation through time </h2>

<div class="panel panel-default">
<div class="panel-body">
<!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
<p>We can think of the recurrent net as a layered, feed-forward
net with shared weights and then train the feed-forward net
with weight constraints.
</p>
</div>
</div>


<p>We can also think of this training algorithm in the time domain:</p>
<ol>
<li> The forward pass builds up a stack of the activities of all the units at each time step.</li>
<li> The backward pass peels activities off the stack to compute the error derivatives at each time step.</li>
<li> After the backward pass we add together the derivatives at all the different times for each weight.</li> 
</ol>
<!-- !split -->
<h2 id="the-backward-pass-is-linear" class="anchor">The backward pass is linear </h2>

<ol>
<li> There is a big difference between the forward and backward passes.</li>
<li> In the forward pass we use squashing functions (like the logistic) to prevent the activity vectors from exploding.</li>
<li> The backward pass, is completely linear. If you double the error derivatives at the final layer, all the error derivatives will double.</li>
</ol>
<p>The forward pass determines the slope of the linear function used for
backpropagating through each neuron
</p>

<!-- !split  -->
<h2 id="the-problem-of-exploding-or-vanishing-gradients" class="anchor">The problem of exploding or vanishing gradients </h2>
<ul>
<li> What happens to the magnitude of the gradients as we backpropagate through many layers?
<ol type="a"></li>
 <li> If the weights are small, the gradients shrink exponentially.</li>
 <li> If the weights are big the gradients grow exponentially.</li>
</ol>
<li> Typical feed-forward neural nets can cope with these exponential effects because they only have a few hidden layers.</li>
<li> In an RNN trained on long sequences (e.g. 100 time steps) the gradients can easily explode or vanish.
<ol type="a"></li>
 <li> We can avoid this by initializing the weights very carefully.</li>
</ol>
<li> Even with good initial weights, its very hard to detect that the current target output depends on an input from many time-steps ago.</li>
</ul>
<p>RNNs have difficulty dealing with long-range dependencies. </p>

<!-- !split -->
<h2 id="the-mathematics-of-rnns-the-basic-architecture" class="anchor">The mathematics of RNNs, the basic architecture  </h2>

<p>See notebook at <a href="https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week7/ipynb/rnnmath.ipynb" target="_self"><tt>https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week7/ipynb/rnnmath.ipynb</tt></a></p>

<!-- !split -->
<h2 id="four-effective-ways-to-learn-an-rnn-and-preparing-for-next-week" class="anchor">Four effective ways to learn an RNN and preparing for next week </h2>
<ol>
<li> Long Short Term Memory Make the RNN out of little modules that are designed to remember values for a long time.</li>
<li> Hessian Free Optimization: Deal with the vanishing gradients problem by using a fancy optimizer that can detect directions with a tiny gradient but even smaller curvature.</li>
<li> Echo State Networks (ESN): Initialize the input a hidden and hidden-hidden and output-hidden connections very carefully so that the hidden state has a huge reservoir of weakly coupled oscillators which can be selectively driven by the input. ESNs only need to learn the hidden-output connections.</li>
<li> Good initialization with momentum Initialize like in Echo State Networks, but then learn all of the connections using momentum</li>
</ol>
<!-- !split -->
<h2 id="long-short-term-memory-lstm" class="anchor">Long Short Term Memory (LSTM) </h2>

<p>LSTM uses a memory cell for 
 modeling long-range dependencies and avoid vanishing gradient
 problems.
</p>

<ol>
<li> Introduced by Hochreiter and Schmidhuber (1997) who solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).</li>
<li> They designed a memory cell using logistic and linear units with multiplicative interactions.</li>
<li> Information gets into the cell whenever its &#8220;write&#8221; gate is on.</li>
<li> The information stays in the cell so long as its <b>keep</b> gate is on.</li>
<li> Information can be read from the cell by turning on its <b>read</b> gate.</li> 
</ol>
<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2025, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</body>
</html>

