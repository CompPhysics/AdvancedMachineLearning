<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week7.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week7-bs --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="February 26-March 1: Advanced machine learning and data analysis for the physical sciences">
<title>February 26-March 1: Advanced machine learning and data analysis for the physical sciences</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html week7.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week7-bs --no_mako -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plans for the week February 26-March 1',
               2,
               None,
               'plans-for-the-week-february-26-march-1'),
              ('From FFNNs and CNNs to recurrent neural networks (RNNs)',
               2,
               None,
               'from-ffnns-and-cnns-to-recurrent-neural-networks-rnns'),
              ('Feedback connections', 2, None, 'feedback-connections'),
              ('Vanishing gradients', 2, None, 'vanishing-gradients'),
              ('Recurrent neural networks (RNNs): Overarching view',
               2,
               None,
               'recurrent-neural-networks-rnns-overarching-view'),
              ('Sequential data only?', 2, None, 'sequential-data-only'),
              ('A simple example', 2, None, 'a-simple-example'),
              ('Memoryless models', 2, None, 'memoryless-models'),
              ('RNNs', 2, None, 'rnns'),
              ('What kinds of behaviour can RNNs exhibit?',
               2,
               None,
               'what-kinds-of-behaviour-can-rnns-exhibit'),
              ('Basic layout,  "Figures from Sebastian Rashcka et al, Machine '
               'learning with Sickit-Learn and '
               'PyTorch":"https://sebastianraschka.com/blog/2022/ml-pytorch-book.html"',
               2,
               None,
               'basic-layout-figures-from-sebastian-rashcka-et-al-machine-learning-with-sickit-learn-and-pytorch-https-sebastianraschka-com-blog-2022-ml-pytorch-book-html'),
              ('We need to specify the initial activity state of all the '
               'hidden and output units',
               2,
               None,
               'we-need-to-specify-the-initial-activity-state-of-all-the-hidden-and-output-units'),
              ('We can specify inputs in several ways',
               2,
               None,
               'we-can-specify-inputs-in-several-ways'),
              ('We can specify targets in several ways',
               2,
               None,
               'we-can-specify-targets-in-several-ways'),
              ('Solving differential equations with RNNs',
               2,
               None,
               'solving-differential-equations-with-rnns'),
              ('RNNs in mode detail', 2, None, 'rnns-in-mode-detail'),
              ('RNNs in mode detail, part 2',
               2,
               None,
               'rnns-in-mode-detail-part-2'),
              ('RNNs in mode detail, part 3',
               2,
               None,
               'rnns-in-mode-detail-part-3'),
              ('RNNs in mode detail, part 4',
               2,
               None,
               'rnns-in-mode-detail-part-4'),
              ('RNNs in mode detail, part 5',
               2,
               None,
               'rnns-in-mode-detail-part-5'),
              ('RNNs in mode detail, part 6',
               2,
               None,
               'rnns-in-mode-detail-part-6'),
              ('RNNs in mode detail, part 7',
               2,
               None,
               'rnns-in-mode-detail-part-7'),
              ('Backpropagation through time',
               2,
               None,
               'backpropagation-through-time'),
              ('The backward pass is linear',
               2,
               None,
               'the-backward-pass-is-linear'),
              ('The problem of exploding or vanishing gradients',
               2,
               None,
               'the-problem-of-exploding-or-vanishing-gradients'),
              ('Mathematical setup', 2, None, 'mathematical-setup'),
              ('Back propagation in time through figures, part 1',
               2,
               None,
               'back-propagation-in-time-through-figures-part-1'),
              ('Back propagation in time, part 2',
               2,
               None,
               'back-propagation-in-time-part-2'),
              ('Back propagation in time, part 3',
               2,
               None,
               'back-propagation-in-time-part-3'),
              ('Back propagation in time, part 4',
               2,
               None,
               'back-propagation-in-time-part-4'),
              ('Back propagation in time in equations',
               2,
               None,
               'back-propagation-in-time-in-equations'),
              ('Chain rule again', 2, None, 'chain-rule-again'),
              ('Gradients of loss functions',
               2,
               None,
               'gradients-of-loss-functions'),
              ('Summary of RNNs', 2, None, 'summary-of-rnns'),
              ('A typical RNN', 2, None, 'a-typical-rnn'),
              ('Solving differential equations with RNNs, code examples',
               2,
               None,
               'solving-differential-equations-with-rnns-code-examples'),
              ('Four effective ways to learn an RNN',
               2,
               None,
               'four-effective-ways-to-learn-an-rnn'),
              ('Long Short Term Memory (LSTM)',
               2,
               None,
               'long-short-term-memory-lstm'),
              ('Implementing a memory cell in a neural network',
               2,
               None,
               'implementing-a-memory-cell-in-a-neural-network'),
              ('Long Short Term Memory (LSTM), part 1',
               2,
               None,
               'long-short-term-memory-lstm-part-1'),
              ('Long Short Term Memory (LSTM), part 2',
               2,
               None,
               'long-short-term-memory-lstm-part-2'),
              ('Long Short Term Memory (LSTM), part 3',
               2,
               None,
               'long-short-term-memory-lstm-part-3'),
              ('Long Short Term Memory (LSTM), part 4',
               2,
               None,
               'long-short-term-memory-lstm-part-4'),
              ('Long Short Term Memory (LSTM), part 5',
               2,
               None,
               'long-short-term-memory-lstm-part-5'),
              ('Long Short Term Memory (LSTM), part 6',
               2,
               None,
               'long-short-term-memory-lstm-part-6'),
              ('Long Short Term Memory (LSTM), part 7',
               2,
               None,
               'long-short-term-memory-lstm-part-7'),
              ('Long Short Term Memory (LSTM), part 8',
               2,
               None,
               'long-short-term-memory-lstm-part-8'),
              ('Long Short Term Memory (LSTM), part 9',
               2,
               None,
               'long-short-term-memory-lstm-part-9'),
              ('Long Short Term Memory (LSTM), part 10',
               2,
               None,
               'long-short-term-memory-lstm-part-10'),
              ('An extrapolation example', 2, None, 'an-extrapolation-example'),
              ('Formatting the Data', 2, None, 'formatting-the-data'),
              ('Predicting New Points With A Trained Recurrent Neural Network',
               2,
               None,
               'predicting-new-points-with-a-trained-recurrent-neural-network'),
              ('Other Things to Try', 2, None, 'other-things-to-try'),
              ('Other Types of Recurrent Neural Networks',
               2,
               None,
               'other-types-of-recurrent-neural-networks'),
              ('Gating mechanism: Long Short Term Memory (LSTM)',
               2,
               None,
               'gating-mechanism-long-short-term-memory-lstm'),
              ('Implementing a memory cell in a neural network',
               2,
               None,
               'implementing-a-memory-cell-in-a-neural-network')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week7-bs.html">February 26-March 1: Advanced machine learning and data analysis for the physical sciences</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="#plans-for-the-week-february-26-march-1" style="font-size: 80%;">Plans for the week February 26-March 1</a></li>
     <!-- navigation toc: --> <li><a href="#from-ffnns-and-cnns-to-recurrent-neural-networks-rnns" style="font-size: 80%;">From FFNNs and CNNs to recurrent neural networks (RNNs)</a></li>
     <!-- navigation toc: --> <li><a href="#feedback-connections" style="font-size: 80%;">Feedback connections</a></li>
     <!-- navigation toc: --> <li><a href="#vanishing-gradients" style="font-size: 80%;">Vanishing gradients</a></li>
     <!-- navigation toc: --> <li><a href="#recurrent-neural-networks-rnns-overarching-view" style="font-size: 80%;">Recurrent neural networks (RNNs): Overarching view</a></li>
     <!-- navigation toc: --> <li><a href="#sequential-data-only" style="font-size: 80%;">Sequential data only?</a></li>
     <!-- navigation toc: --> <li><a href="#a-simple-example" style="font-size: 80%;">A simple example</a></li>
     <!-- navigation toc: --> <li><a href="#memoryless-models" style="font-size: 80%;">Memoryless models</a></li>
     <!-- navigation toc: --> <li><a href="#rnns" style="font-size: 80%;">RNNs</a></li>
     <!-- navigation toc: --> <li><a href="#what-kinds-of-behaviour-can-rnns-exhibit" style="font-size: 80%;">What kinds of behaviour can RNNs exhibit?</a></li>
     <!-- navigation toc: --> <li><a href="#basic-layout-figures-from-sebastian-rashcka-et-al-machine-learning-with-sickit-learn-and-pytorch-https-sebastianraschka-com-blog-2022-ml-pytorch-book-html" style="font-size: 80%;">Basic layout,  "Figures from Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch":"https://sebastianraschka.com/blog/2022/ml-pytorch-book.html"</a></li>
     <!-- navigation toc: --> <li><a href="#we-need-to-specify-the-initial-activity-state-of-all-the-hidden-and-output-units" style="font-size: 80%;">We need to specify the initial activity state of all the hidden and output units</a></li>
     <!-- navigation toc: --> <li><a href="#we-can-specify-inputs-in-several-ways" style="font-size: 80%;">We can specify inputs in several ways</a></li>
     <!-- navigation toc: --> <li><a href="#we-can-specify-targets-in-several-ways" style="font-size: 80%;">We can specify targets in several ways</a></li>
     <!-- navigation toc: --> <li><a href="#solving-differential-equations-with-rnns" style="font-size: 80%;">Solving differential equations with RNNs</a></li>
     <!-- navigation toc: --> <li><a href="#rnns-in-mode-detail" style="font-size: 80%;">RNNs in mode detail</a></li>
     <!-- navigation toc: --> <li><a href="#rnns-in-mode-detail-part-2" style="font-size: 80%;">RNNs in mode detail, part 2</a></li>
     <!-- navigation toc: --> <li><a href="#rnns-in-mode-detail-part-3" style="font-size: 80%;">RNNs in mode detail, part 3</a></li>
     <!-- navigation toc: --> <li><a href="#rnns-in-mode-detail-part-4" style="font-size: 80%;">RNNs in mode detail, part 4</a></li>
     <!-- navigation toc: --> <li><a href="#rnns-in-mode-detail-part-5" style="font-size: 80%;">RNNs in mode detail, part 5</a></li>
     <!-- navigation toc: --> <li><a href="#rnns-in-mode-detail-part-6" style="font-size: 80%;">RNNs in mode detail, part 6</a></li>
     <!-- navigation toc: --> <li><a href="#rnns-in-mode-detail-part-7" style="font-size: 80%;">RNNs in mode detail, part 7</a></li>
     <!-- navigation toc: --> <li><a href="#backpropagation-through-time" style="font-size: 80%;">Backpropagation through time</a></li>
     <!-- navigation toc: --> <li><a href="#the-backward-pass-is-linear" style="font-size: 80%;">The backward pass is linear</a></li>
     <!-- navigation toc: --> <li><a href="#the-problem-of-exploding-or-vanishing-gradients" style="font-size: 80%;">The problem of exploding or vanishing gradients</a></li>
     <!-- navigation toc: --> <li><a href="#mathematical-setup" style="font-size: 80%;">Mathematical setup</a></li>
     <!-- navigation toc: --> <li><a href="#back-propagation-in-time-through-figures-part-1" style="font-size: 80%;">Back propagation in time through figures, part 1</a></li>
     <!-- navigation toc: --> <li><a href="#back-propagation-in-time-part-2" style="font-size: 80%;">Back propagation in time, part 2</a></li>
     <!-- navigation toc: --> <li><a href="#back-propagation-in-time-part-3" style="font-size: 80%;">Back propagation in time, part 3</a></li>
     <!-- navigation toc: --> <li><a href="#back-propagation-in-time-part-4" style="font-size: 80%;">Back propagation in time, part 4</a></li>
     <!-- navigation toc: --> <li><a href="#back-propagation-in-time-in-equations" style="font-size: 80%;">Back propagation in time in equations</a></li>
     <!-- navigation toc: --> <li><a href="#chain-rule-again" style="font-size: 80%;">Chain rule again</a></li>
     <!-- navigation toc: --> <li><a href="#gradients-of-loss-functions" style="font-size: 80%;">Gradients of loss functions</a></li>
     <!-- navigation toc: --> <li><a href="#summary-of-rnns" style="font-size: 80%;">Summary of RNNs</a></li>
     <!-- navigation toc: --> <li><a href="#a-typical-rnn" style="font-size: 80%;">A typical RNN</a></li>
     <!-- navigation toc: --> <li><a href="#solving-differential-equations-with-rnns-code-examples" style="font-size: 80%;">Solving differential equations with RNNs, code examples</a></li>
     <!-- navigation toc: --> <li><a href="#four-effective-ways-to-learn-an-rnn" style="font-size: 80%;">Four effective ways to learn an RNN</a></li>
     <!-- navigation toc: --> <li><a href="#long-short-term-memory-lstm" style="font-size: 80%;">Long Short Term Memory (LSTM)</a></li>
     <!-- navigation toc: --> <li><a href="#implementing-a-memory-cell-in-a-neural-network" style="font-size: 80%;">Implementing a memory cell in a neural network</a></li>
     <!-- navigation toc: --> <li><a href="#long-short-term-memory-lstm-part-1" style="font-size: 80%;">Long Short Term Memory (LSTM), part 1</a></li>
     <!-- navigation toc: --> <li><a href="#long-short-term-memory-lstm-part-2" style="font-size: 80%;">Long Short Term Memory (LSTM), part 2</a></li>
     <!-- navigation toc: --> <li><a href="#long-short-term-memory-lstm-part-3" style="font-size: 80%;">Long Short Term Memory (LSTM), part 3</a></li>
     <!-- navigation toc: --> <li><a href="#long-short-term-memory-lstm-part-4" style="font-size: 80%;">Long Short Term Memory (LSTM), part 4</a></li>
     <!-- navigation toc: --> <li><a href="#long-short-term-memory-lstm-part-5" style="font-size: 80%;">Long Short Term Memory (LSTM), part 5</a></li>
     <!-- navigation toc: --> <li><a href="#long-short-term-memory-lstm-part-6" style="font-size: 80%;">Long Short Term Memory (LSTM), part 6</a></li>
     <!-- navigation toc: --> <li><a href="#long-short-term-memory-lstm-part-7" style="font-size: 80%;">Long Short Term Memory (LSTM), part 7</a></li>
     <!-- navigation toc: --> <li><a href="#long-short-term-memory-lstm-part-8" style="font-size: 80%;">Long Short Term Memory (LSTM), part 8</a></li>
     <!-- navigation toc: --> <li><a href="#long-short-term-memory-lstm-part-9" style="font-size: 80%;">Long Short Term Memory (LSTM), part 9</a></li>
     <!-- navigation toc: --> <li><a href="#long-short-term-memory-lstm-part-10" style="font-size: 80%;">Long Short Term Memory (LSTM), part 10</a></li>
     <!-- navigation toc: --> <li><a href="#an-extrapolation-example" style="font-size: 80%;">An extrapolation example</a></li>
     <!-- navigation toc: --> <li><a href="#formatting-the-data" style="font-size: 80%;">Formatting the Data</a></li>
     <!-- navigation toc: --> <li><a href="#predicting-new-points-with-a-trained-recurrent-neural-network" style="font-size: 80%;">Predicting New Points With A Trained Recurrent Neural Network</a></li>
     <!-- navigation toc: --> <li><a href="#other-things-to-try" style="font-size: 80%;">Other Things to Try</a></li>
     <!-- navigation toc: --> <li><a href="#other-types-of-recurrent-neural-networks" style="font-size: 80%;">Other Types of Recurrent Neural Networks</a></li>
     <!-- navigation toc: --> <li><a href="#gating-mechanism-long-short-term-memory-lstm" style="font-size: 80%;">Gating mechanism: Long Short Term Memory (LSTM)</a></li>
     <!-- navigation toc: --> <li><a href="#implementing-a-memory-cell-in-a-neural-network" style="font-size: 80%;">Implementing a memory cell in a neural network</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<!-- ------------------- main content ---------------------- -->
<div class="jumbotron">
<center>
<h1>February 26-March 1: Advanced machine learning and data analysis for the physical sciences</h1>
</center>  <!-- document title -->

<!-- author(s): Morten Hjorth-Jensen -->
<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>
<!-- institution(s) -->
<center>
[1] <b>Department of Physics and Center for Computing in Science Education, University of Oslo, Norway</b>
</center>
<center>
[2] <b>Department of Physics and Astronomy and Facility for Rare Isotope Beams, Michigan State University, East Lansing, Michigan, USA</b>
</center>
<br>
<center>
<h4>February 26-March 1, 2024</h4>
</center> <!-- date -->
<br>

<!-- potential-jumbotron-button -->
</div> <!-- end jumbotron -->

<!-- !split -->
<h2 id="plans-for-the-week-february-26-march-1" class="anchor">Plans for the week February 26-March 1  </h2>

<div class="panel panel-default">
<div class="panel-body">
<!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
<ol>
<li> Finalizing discussion of Convolutional  Neural Networks (CNNs)</li>
<li> Discussion of recurrent neural networks (RNNs)</li>
<li> Reading recommendations:
<ol type="a"></li>
 <li> Goodfellow, Bengio and Courville's chapter 10 from <a href="https://www.deeplearningbook.org/" target="_self">Deep Learning</a></li>
 <li> <a href="https://sebastianraschka.com/blog/2022/ml-pytorch-book.html" target="_self">Sebastian Rashcka et al, chapter 15, Machine learning with Sickit-Learn and PyTorch</a></li>
 <li> <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html" target="_self">David Foster, Generative Deep Learning with TensorFlow, see chapter 5</a></li>
</ol>
</ol>
</div>
</div>


<p>The last two books have codes for RNNs in PyTorch and TensorFlow/Keras. Next week we will study the solution of differential equations.</p>

<!-- !split -->
<h2 id="from-ffnns-and-cnns-to-recurrent-neural-networks-rnns" class="anchor">From FFNNs and CNNs to recurrent neural networks (RNNs) </h2>

<p>There are limitation of FFNNs, one of which being that FFNNs are not
designed to handle sequential data (data for which the order matters)
effectively because they lack the capabilities of storing information
about previous inputs; each input is being treated indepen-
dently. This is a limitation when dealing with sequential data where
past information can be vital to correctly process current and future
inputs. 
</p>

<!-- !split -->
<h2 id="feedback-connections" class="anchor">Feedback connections </h2>

<p>In contrast to FFNs, recurrent networks introduce feedback
connections, meaning the information is al- lowed to be carried to
subsequent nodes across different time steps. These cyclic or feedback
connections have the objective of providing the network with some kind
of memory, making RNNs particularly suited for time- series data,
natural language processing, speech recog- nition, and several other
problems for which the order of the data is crucial.  RNN
architectures vary greatly in how they manage information flow and
memory in the network.
</p>

<!-- !split -->
<h2 id="vanishing-gradients" class="anchor">Vanishing gradients </h2>

<p>Different architectures often aim at improving
some sub-optimal characteristics of the network. The simplest form of
recurrent network, commonly called simple or vanilla RNN, for example,
is known to suffer from the problem of vanishing gradients. This
problem arises due to the nature of backpropagation in time. Gradients
of the cost/loss function may get exponentially small (or large) if
there are many layers in the network, which is the case of RNN when
the sequence gets long.
</p>

<!-- !split -->
<h2 id="recurrent-neural-networks-rnns-overarching-view" class="anchor">Recurrent neural networks (RNNs): Overarching view </h2>

<p>Till now our focus has been, including convolutional neural networks
as well, on feedforward neural networks. The output or the activations
flow only in one direction, from the input layer to the output layer.
</p>

<p>A recurrent neural network (RNN) looks very much like a feedforward
neural network, except that it also has connections pointing
backward. 
</p>

<p>RNNs are used to analyze time series data such as stock prices, and
tell you when to buy or sell. In autonomous driving systems, they can
anticipate car trajectories and help avoid accidents. More generally,
they can work on sequences of arbitrary lengths, rather than on
fixed-sized inputs like all the nets we have discussed so far. For
example, they can take sentences, documents, or audio samples as
input, making them extremely useful for natural language processing
systems such as automatic translation and speech-to-text.
</p>

<!-- !split -->
<h2 id="sequential-data-only" class="anchor">Sequential data only? </h2>

<p>An important issue is that in many deep learning methods we assume
that the input and output data can be treated as independent and
identically distributed, normally abbreviated to <b>iid</b>.
This means that the data we use can be seen as mutually independent.
</p>

<p>This is however not the case for most data sets used in RNNs since we
are dealing with sequences of data with strong inter-dependencies.
This applies in particular to time series, which are sequential by
contruction.
</p>

<p>As an example, the solutions of ordinary differential equations can be
represented as a time series, similarly, how stock prices evolve as
function of time is another example of a typical time series, or voice
records and many other examples.
</p>

<p>Not all sequential data may however have a time stamp, texts being a
typical example thereof, or DNA sequences.
</p>

<p>The main focus here is on data that can be structured either as time
series or as ordered series of data.  We will not focus on for example
natural language processing or similar data sets.
</p>

<!-- !split -->
<h2 id="a-simple-example" class="anchor">A simple example </h2>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># Start importing packages</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">tf</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras</span> <span style="color: #008000; font-weight: bold">import</span> datasets, layers, models
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.layers</span> <span style="color: #008000; font-weight: bold">import</span> Input
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.models</span> <span style="color: #008000; font-weight: bold">import</span> Model, Sequential 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.layers</span> <span style="color: #008000; font-weight: bold">import</span> Dense, SimpleRNN, LSTM, GRU
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras</span> <span style="color: #008000; font-weight: bold">import</span> optimizers     
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras</span> <span style="color: #008000; font-weight: bold">import</span> regularizers           
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.utils</span> <span style="color: #008000; font-weight: bold">import</span> to_categorical 



<span style="color: #408080; font-style: italic"># convert into dataset matrix</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">convertToMatrix</span>(data, step):
 X, Y <span style="color: #666666">=</span>[], []
 <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">len</span>(data)<span style="color: #666666">-</span>step):
  d<span style="color: #666666">=</span>i<span style="color: #666666">+</span>step  
  X<span style="color: #666666">.</span>append(data[i:d,])
  Y<span style="color: #666666">.</span>append(data[d,])
 <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>array(X), np<span style="color: #666666">.</span>array(Y)

step <span style="color: #666666">=</span> <span style="color: #666666">4</span>
N <span style="color: #666666">=</span> <span style="color: #666666">1000</span>    
Tp <span style="color: #666666">=</span> <span style="color: #666666">800</span>    

t<span style="color: #666666">=</span>np<span style="color: #666666">.</span>arange(<span style="color: #666666">0</span>,N)
x<span style="color: #666666">=</span>np<span style="color: #666666">.</span>sin(<span style="color: #666666">0.02*</span>t)<span style="color: #666666">+2*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(N)
df <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>DataFrame(x)
df<span style="color: #666666">.</span>head()

values<span style="color: #666666">=</span>df<span style="color: #666666">.</span>values
train,test <span style="color: #666666">=</span> values[<span style="color: #666666">0</span>:Tp,:], values[Tp:N,:]

<span style="color: #408080; font-style: italic"># add step elements into train and test</span>
test <span style="color: #666666">=</span> np<span style="color: #666666">.</span>append(test,np<span style="color: #666666">.</span>repeat(test[<span style="color: #666666">-1</span>,],step))
train <span style="color: #666666">=</span> np<span style="color: #666666">.</span>append(train,np<span style="color: #666666">.</span>repeat(train[<span style="color: #666666">-1</span>,],step))
 
trainX,trainY <span style="color: #666666">=</span>convertToMatrix(train,step)
testX,testY <span style="color: #666666">=</span>convertToMatrix(test,step)
trainX <span style="color: #666666">=</span> np<span style="color: #666666">.</span>reshape(trainX, (trainX<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>], <span style="color: #666666">1</span>, trainX<span style="color: #666666">.</span>shape[<span style="color: #666666">1</span>]))
testX <span style="color: #666666">=</span> np<span style="color: #666666">.</span>reshape(testX, (testX<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>], <span style="color: #666666">1</span>, testX<span style="color: #666666">.</span>shape[<span style="color: #666666">1</span>]))

model <span style="color: #666666">=</span> Sequential()
model<span style="color: #666666">.</span>add(SimpleRNN(units<span style="color: #666666">=32</span>, input_shape<span style="color: #666666">=</span>(<span style="color: #666666">1</span>,step), activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>))
model<span style="color: #666666">.</span>add(Dense(<span style="color: #666666">8</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>)) 
model<span style="color: #666666">.</span>add(Dense(<span style="color: #666666">1</span>))
model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&#39;mean_squared_error&#39;</span>, optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&#39;rmsprop&#39;</span>)
model<span style="color: #666666">.</span>summary()

model<span style="color: #666666">.</span>fit(trainX,trainY, epochs<span style="color: #666666">=100</span>, batch_size<span style="color: #666666">=16</span>, verbose<span style="color: #666666">=2</span>)
trainPredict <span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(trainX)
testPredict<span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(testX)
predicted<span style="color: #666666">=</span>np<span style="color: #666666">.</span>concatenate((trainPredict,testPredict),axis<span style="color: #666666">=0</span>)

trainScore <span style="color: #666666">=</span> model<span style="color: #666666">.</span>evaluate(trainX, trainY, verbose<span style="color: #666666">=0</span>)
<span style="color: #008000">print</span>(trainScore)
plt<span style="color: #666666">.</span>plot(df)
plt<span style="color: #666666">.</span>plot(predicted)
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split -->
<h2 id="memoryless-models" class="anchor">Memoryless models </h2>

<p>Autoregressive models predict the next term in a sequence from a fixed number of previous terms. <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html" target="_self">David Foster, Generative Deep Learning with TensorFlow, discusses in chapter 5 autoregressive models</a>. We will come back later to autoregressive models.</p>

<div class="panel panel-default">
<div class="panel-body">
<!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
<p>These generalize autoregressive
models by using one or more
layers of non-linear hidden units. 
</p>
</div>
</div>


<p>If we give our generative model some hidden state, and if we give
this hidden state its own internal dynamics, we get a much more
interesting kind of model.
</p>
<ol>
<li> It can store information in its hidden state for a long time.</li>
<li> If the dynamics is noisy and the way it generates outputs from its hidden state is noisy, we can never know its exact hidden state.</li>
<li> The best we can do is to infer a probability distribution over the</li>
</ol>
<p>space of hidden state vectors.</p>

<p>This inference is only tractable for two types of hidden state models.</p>

<!-- !split -->
<h2 id="rnns" class="anchor">RNNs </h2>

<p>RNNs are very powerful, because they
combine two properties:
</p>
<ol>
<li> Distributed hidden state that allows them to store a lot of information about the past efficiently.</li>
<li> Non-linear dynamics that allows them to update their hidden state in complicated ways.</li>
</ol>
<p>With enough neurons and time, RNNs
can compute anything that can be
computed by your computer.
</p>

<!-- !split -->
<h2 id="what-kinds-of-behaviour-can-rnns-exhibit" class="anchor">What kinds of behaviour can RNNs exhibit? </h2>

<ol>
<li> They can oscillate.</li> 
<li> They can settle to point attractors.</li>
<li> They can behave chaotically.</li>
<li> RNNs could potentially learn to implement lots of small programs that each capture a nugget of knowledge and run in parallel, interacting to produce very complicated effects.</li>
</ol>
<p>But the extensive computational needs  of RNNs makes them very hard to train.</p>

<!-- !split -->
<h2 id="basic-layout-figures-from-sebastian-rashcka-et-al-machine-learning-with-sickit-learn-and-pytorch-https-sebastianraschka-com-blog-2022-ml-pytorch-book-html" class="anchor">Basic layout,  <a href="https://sebastianraschka.com/blog/2022/ml-pytorch-book.html" target="_self">Figures from Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch</a> </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN1.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="we-need-to-specify-the-initial-activity-state-of-all-the-hidden-and-output-units" class="anchor">We need to specify the initial activity state of all the hidden and output units </h2>

<ol>
<li> We could just fix these initial states to have some default value like 0.5.</li>
<li> But it is better to treat the initial states as learned parameters.</li>
<li> We learn them in the same way as we learn the weights.</li>
<li> Start off with an initial random guess for the initial states.
<ol type="a"></li>
 <li> At the end of each training sequence, backpropagate through time all the way to the initial states to get the gradient of the error function with respect to each initial state.</li>
 <li> Adjust the initial states by following the negative gradient.</li> 
</ol>
</ol>
<!-- !split -->
<h2 id="we-can-specify-inputs-in-several-ways" class="anchor">We can specify inputs in several ways </h2>

<ol>
<li> Specify the initial states of all the units.</li>
<li> Specify the initial states of a subset of the units.</li>
<li> Specify the states of the same subset of the units at every time step.</li>
</ol>
<p>This is the natural way to model most sequential data. </p>

<!-- !split -->
<h2 id="we-can-specify-targets-in-several-ways" class="anchor">We can specify targets in several ways </h2>

<ol>
<li> Specify desired final activities of all the units</li>
<li> Specify desired activities of all units for the last few steps</li>
<li> Good for learning attractors</li>
<li> It is easy to add in extra error derivatives as we backpropagate.</li>
<ul>
  <li> Specify the desired activity of a subset of the units.</li>
</ul>
<li> The other units are input or hidden units.</li> 
</ol>
<!-- !split -->
<h2 id="solving-differential-equations-with-rnns" class="anchor">Solving differential equations with RNNs </h2>

<p>More material to be added here.</p>

<!-- !split -->
<h2 id="rnns-in-mode-detail" class="anchor">RNNs in mode detail  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN2.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="rnns-in-mode-detail-part-2" class="anchor">RNNs in mode detail, part 2  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN3.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="rnns-in-mode-detail-part-3" class="anchor">RNNs in mode detail, part 3  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN4.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="rnns-in-mode-detail-part-4" class="anchor">RNNs in mode detail, part 4  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN5.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="rnns-in-mode-detail-part-5" class="anchor">RNNs in mode detail, part 5  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN6.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="rnns-in-mode-detail-part-6" class="anchor">RNNs in mode detail, part 6  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN7.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="rnns-in-mode-detail-part-7" class="anchor">RNNs in mode detail, part 7  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN8.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="backpropagation-through-time" class="anchor">Backpropagation through time </h2>

<div class="panel panel-default">
<div class="panel-body">
<!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
<p>We can think of the recurrent net as a layered, feed-forward
net with shared weights and then train the feed-forward net
with weight constraints.
</p>
</div>
</div>


<p>We can also think of this training algorithm in the time domain:</p>
<ol>
<li> The forward pass builds up a stack of the activities of all the units at each time step.</li>
<li> The backward pass peels activities off the stack to compute the error derivatives at each time step.</li>
<li> After the backward pass we add together the derivatives at all the different times for each weight.</li> 
</ol>
<!-- !split -->
<h2 id="the-backward-pass-is-linear" class="anchor">The backward pass is linear </h2>

<ol>
<li> There is a big difference between the forward and backward passes.</li>
<li> In the forward pass we use squashing functions (like the logistic) to prevent the activity vectors from exploding.</li>
<li> The backward pass, is completely linear. If you double the error derivatives at the final layer, all the error derivatives will double.</li>
</ol>
<p>The forward pass determines the slope of the linear function used for
backpropagating through each neuron
</p>

<!-- !split  -->
<h2 id="the-problem-of-exploding-or-vanishing-gradients" class="anchor">The problem of exploding or vanishing gradients </h2>
<ul>
<li> What happens to the magnitude of the gradients as we backpropagate through many layers?
<ol type="a"></li>
 <li> If the weights are small, the gradients shrink exponentially.</li>
 <li> If the weights are big the gradients grow exponentially.</li>
</ol>
<li> Typical feed-forward neural nets can cope with these exponential effects because they only have a few hidden layers.</li>
<li> In an RNN trained on long sequences (e.g. 100 time steps) the gradients can easily explode or vanish.
<ol type="a"></li>
 <li> We can avoid this by initializing the weights very carefully.</li>
</ol>
<li> Even with good initial weights, its very hard to detect that the current target output depends on an input from many time-steps ago.</li>
</ul>
<p>RNNs have difficulty dealing with long-range dependencies. </p>

<!-- !split -->
<h2 id="mathematical-setup" class="anchor">Mathematical setup </h2>

<p>The expression for the simplest Recurrent network resembles that of a
regular feed-forward neural network, but now with
the concept of temporal dependencies
</p>

$$
\begin{align*}
    \mathbf{a}^{(t)} & = U * \mathbf{x}^{(t)} + W * \mathbf{h}^{(t-1)} + \mathbf{b}, \notag \\
    \mathbf{h}^{(t)} &= \sigma_h(\mathbf{a}^{(t)}), \notag\\
    \mathbf{o}^{(t)} &= V * \mathbf{h}^{(t)} + \mathbf{c}, \notag\\
    \mathbf{\hat{y}}^{(t)} &= \sigma_y(\mathbf{o}^{(t)}).
\end{align*}
$$


<!-- !split -->
<h2 id="back-propagation-in-time-through-figures-part-1" class="anchor">Back propagation in time through figures, part 1   </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN9.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="back-propagation-in-time-part-2" class="anchor">Back propagation in time, part 2   </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN10.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="back-propagation-in-time-part-3" class="anchor">Back propagation in time, part 3   </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN11.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="back-propagation-in-time-part-4" class="anchor">Back propagation in time, part 4   </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN12.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="back-propagation-in-time-in-equations" class="anchor">Back propagation in time in equations </h2>

<p>To derive the expression of the gradients of \( \mathcal{L} \) for
the RNN, we need to start recursively from the nodes closer to the
output layer in the temporal unrolling scheme - such as \( \mathbf{o} \)
and \( \mathbf{h} \) at final time \( t = \tau \),
</p>

$$
\begin{align*}
    (\nabla_{ \mathbf{o}^{(t)}} \mathcal{L})_{i} &= \frac{\partial \mathcal{L}}{\partial L^{(t)}}\frac{\partial L^{(t)}}{\partial o_{i}^{(t)}}, \notag\\
    \nabla_{\mathbf{h}^{(\tau)}} \mathcal{L} &= \mathbf{V}^\mathsf{T}\nabla_{ \mathbf{o}^{(\tau)}} \mathcal{L}.
\end{align*}
$$


<!-- !split -->
<h2 id="chain-rule-again" class="anchor">Chain rule again </h2>
<p>For the following hidden nodes, we have to iterate through time, so by the chain rule, </p>

$$
\begin{align*}
    \nabla_{\mathbf{h}^{(t)}} \mathcal{L} &= \left(\frac{\partial\mathbf{h}^{(t+1)}}{\partial\mathbf{h}^{(t)}}\right)^\mathsf{T}\nabla_{\mathbf{h}^{(t+1)}}\mathcal{L} + \left(\frac{\partial\mathbf{o}^{(t)}}{\partial\mathbf{h}^{(t)}}\right)^\mathsf{T}\nabla_{ \mathbf{o}^{(t)}} \mathcal{L}.
\end{align*}
$$


<!-- !split -->
<h2 id="gradients-of-loss-functions" class="anchor">Gradients of loss functions </h2>
<p>Similarly, the gradients of \( \mathcal{L} \) with respect to the weights and biases follow,</p>

$$
\begin{align*}
    \nabla_{\mathbf{c}} \mathcal{L} &=\sum_{t}\left(\frac{\partial \mathbf{o}^{(t)}}{\partial \mathbf{c}}\right)^\mathsf{T} \nabla_{\mathbf{o}^{(t)}} \mathcal{L} \notag\\
    \nabla_{\mathbf{b}} \mathcal{L} &=\sum_{t}\left(\frac{\partial \mathbf{h}^{(t)}}{\partial \mathbf{b}}\right)^\mathsf{T}        \nabla_{\mathbf{h}^{(t)}} \mathcal{L} \notag\\
    \nabla_{\mathbf{V}} \mathcal{L} &=\sum_{t}\sum_{i}\left(\frac{\partial \mathcal{L}}{\partial o_i^{(t)} }\right)\nabla_{\mathbf{V}^{(t)}}o_i^{(t)} \notag\\
    \nabla_{\mathbf{W}} \mathcal{L} &=\sum_{t}\sum_{i}\left(\frac{\partial \mathcal{L}}{\partial h_i^{(t)}}\right)\nabla_{\mathbf{w}^{(t)}} h_i^{(t)} \notag\\
    \nabla_{\mathbf{U}} \mathcal{L} &=\sum_{t}\sum_{i}\left(\frac{\partial \mathcal{L}}{\partial h_i^{(t)}}\right)\nabla_{\mathbf{U}^{(t)}}h_i^{(t)}.
    \label{eq:rnn_gradients3}
\end{align*}
$$


<!-- !split -->
<h2 id="summary-of-rnns" class="anchor">Summary of RNNs </h2>

<p>Recurrent neural networks (RNNs) have in general no probabilistic component
in a model. With a given fixed input and target from data, the RNNs learn the intermediate
association between various layers.
The inputs, outputs, and internal representation (hidden states) are all
real-valued vectors.
</p>

<p>In a  traditional NN, it is assumed that every input is
independent of each other.  But with sequential data, the input at a given stage \( t \) depends on the input from the previous stage \( t-1 \)
</p>

<!-- !split -->
<h2 id="a-typical-rnn" class="anchor">A typical RNN </h2>

<ol>
<li> Weight matrices \( U \), \( W \) and \( V \) that connect the input layer at a stage \( t \) with the hidden layer \( h_t \), the previous hidden layer \( h_{t-1} \) with \( h_t \) and the hidden layer \( h_t \) connecting with the output layer at the same stage and producing an output \( \tilde{y}_t \), respectively.</li>
<li> The output from the hidden layer \( h_t \) is oftem modulated by a \( \tanh{} \) function \( h_t=f(x_t,h_{t-1})=\tanh{(Ux_t+Wh_{t-1}+b)} \) with \( b \) a bias value</li>
<li> The output from the hidden layer produces \( \tilde{y}_t=g(Vh_t+c) \) where \( c \) is a new bias parameter.</li>
<li> The output from the training at a given stage is in turn compared with the observation \( y_t \) thorugh a chosen cost function.</li>
</ol>
<p>The function \( g \) can any of the standard activation functions, that is a Sigmoid, a Softmax, a ReLU and other.
The parameters are trained through the so-called back-propagation through time (BPTT) algorithm.
</p>

<!-- !split -->
<h2 id="solving-differential-equations-with-rnns-code-examples" class="anchor">Solving differential equations with RNNs, code examples </h2>

<p>Examples to be added for week 8</p>

<!-- !split -->
<h2 id="four-effective-ways-to-learn-an-rnn" class="anchor">Four effective ways to learn an RNN </h2>
<ol>
<li> Long Short Term Memory Make the RNN out of little modules that are designed to remember values for a long time.</li>
<li> Hessian Free Optimization: Deal with the vanishing gradients problem by using a fancy optimizer that can detect directions with a tiny gradient but even smaller curvature.</li>
<li> Echo State Networks: Initialize the input a hidden and hidden-hidden and output-hidden connections very carefully so that the hidden state has a huge reservoir of weakly coupled oscillators which can be selectively driven by the input.</li>
<ul>
  <li> ESNs only need to learn the hidden-output connections.</li>
</ul>
<li> Good initialization with momentum Initialize like in Echo State Networks, but then learn all of the connections using momentum</li>
</ol>
<!-- !split -->
<h2 id="long-short-term-memory-lstm" class="anchor">Long Short Term Memory (LSTM) </h2>

<p>LSTM uses a memory cell for 
 modeling long-range dependencies and avoid vanishing gradient
 problems.
</p>

<ol>
<li> Introduced by Hochreiter and Schmidhuber (1997) who solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).</li>
<li> They designed a memory cell using logistic and linear units with multiplicative interactions.</li>
<li> Information gets into the cell whenever its &#8220;write&#8221; gate is on.</li>
<li> The information stays in the cell so long as its <b>keep</b> gate is on.</li>
<li> Information can be read from the cell by turning on its <b>read</b> gate.</li> 
</ol>
<!-- !split -->
<h2 id="implementing-a-memory-cell-in-a-neural-network" class="anchor">Implementing a memory cell in a neural network </h2>
<p>To preserve information for a long time in
the activities of an RNN, we use a circuit
that implements an analog memory cell.
</p>

<ol>
<li> A linear unit that has a self-link with a weight of 1 will maintain its state.</li>
<li> Information is stored in the cell by activating its write gate.</li>
<li> Information is retrieved by activating the read gate.</li>
<li> We can backpropagate through this circuit because logistics are have nice derivatives.</li> 
</ol>
<!-- !split -->
<h2 id="long-short-term-memory-lstm-part-1" class="anchor">Long Short Term Memory (LSTM), part 1   </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM1.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="long-short-term-memory-lstm-part-2" class="anchor">Long Short Term Memory (LSTM), part 2   </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM2.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="long-short-term-memory-lstm-part-3" class="anchor">Long Short Term Memory (LSTM), part 3   </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM3.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="long-short-term-memory-lstm-part-4" class="anchor">Long Short Term Memory (LSTM), part 4   </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM4.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="long-short-term-memory-lstm-part-5" class="anchor">Long Short Term Memory (LSTM), part 5   </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM5.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="long-short-term-memory-lstm-part-6" class="anchor">Long Short Term Memory (LSTM), part 6   </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM6.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="long-short-term-memory-lstm-part-7" class="anchor">Long Short Term Memory (LSTM), part 7   </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM7.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="long-short-term-memory-lstm-part-8" class="anchor">Long Short Term Memory (LSTM), part 8   </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM8.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="long-short-term-memory-lstm-part-9" class="anchor">Long Short Term Memory (LSTM), part 9   </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM9.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="long-short-term-memory-lstm-part-10" class="anchor">Long Short Term Memory (LSTM), part 10   </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM10.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split -->
<h2 id="an-extrapolation-example" class="anchor">An extrapolation example </h2>

<p>The following code provides an example of how recurrent neural
networks can be used to extrapolate to unknown values of physics data
sets.  Specifically, the data sets used in this program come from
a quantum mechanical many-body calculation of energies as functions of the number of particles.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># For matrices and calculations</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #408080; font-style: italic"># For machine learning (backend for keras)</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">tf</span>
<span style="color: #408080; font-style: italic"># User-friendly machine learning library</span>
<span style="color: #408080; font-style: italic"># Front end for TensorFlow</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras</span>
<span style="color: #408080; font-style: italic"># Different methods from Keras needed to create an RNN</span>
<span style="color: #408080; font-style: italic"># This is not necessary but it shortened function calls </span>
<span style="color: #408080; font-style: italic"># that need to be used in the code.</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras</span> <span style="color: #008000; font-weight: bold">import</span> datasets, layers, models
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.layers</span> <span style="color: #008000; font-weight: bold">import</span> Input
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras</span> <span style="color: #008000; font-weight: bold">import</span> regularizers
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.models</span> <span style="color: #008000; font-weight: bold">import</span> Model, Sequential
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.layers</span> <span style="color: #008000; font-weight: bold">import</span> Dense, SimpleRNN, LSTM, GRU
<span style="color: #408080; font-style: italic"># For timing the code</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">timeit</span> <span style="color: #008000; font-weight: bold">import</span> default_timer <span style="color: #008000; font-weight: bold">as</span> timer
<span style="color: #408080; font-style: italic"># For plotting</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>


<span style="color: #408080; font-style: italic"># The data set</span>
datatype<span style="color: #666666">=</span><span style="color: #BA2121">&#39;VaryDimension&#39;</span>
X_tot <span style="color: #666666">=</span> np<span style="color: #666666">.</span>arange(<span style="color: #666666">2</span>, <span style="color: #666666">42</span>, <span style="color: #666666">2</span>)
y_tot <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([<span style="color: #666666">-0.03077640549</span>, <span style="color: #666666">-0.08336233266</span>, <span style="color: #666666">-0.1446729567</span>, <span style="color: #666666">-0.2116753732</span>, <span style="color: #666666">-0.2830637392</span>, <span style="color: #666666">-0.3581341341</span>, <span style="color: #666666">-0.436462435</span>, <span style="color: #666666">-0.5177783846</span>,
	<span style="color: #666666">-0.6019067271</span>, <span style="color: #666666">-0.6887363571</span>, <span style="color: #666666">-0.7782028952</span>, <span style="color: #666666">-0.8702784034</span>, <span style="color: #666666">-0.9649652536</span>, <span style="color: #666666">-1.062292565</span>, <span style="color: #666666">-1.16231451</span>, 
	<span style="color: #666666">-1.265109911</span>, <span style="color: #666666">-1.370782966</span>, <span style="color: #666666">-1.479465113</span>, <span style="color: #666666">-1.591317992</span>, <span style="color: #666666">-1.70653767</span>])
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split -->
<h2 id="formatting-the-data" class="anchor">Formatting the Data </h2>

<p>The way the recurrent neural networks are trained in this program
differs from how machine learning algorithms are usually trained.
Typically a machine learning algorithm is trained by learning the
relationship between the x data and the y data.  In this program, the
recurrent neural network will be trained to recognize the relationship
in a sequence of y values.  This is type of data formatting is
typically used time series forcasting, but it can also be used in any
extrapolation (time series forecasting is just a specific type of
extrapolation along the time axis).  This method of data formatting
does not use the x data and assumes that the y data are evenly spaced.
</p>

<p>For a standard machine learning algorithm, the training data has the
form of (x,y) so the machine learning algorithm learns to assiciate a
y value with a given x value.  This is useful when the test data has x
values within the same range as the training data.  However, for this
application, the x values of the test data are outside of the x values
of the training data and the traditional method of training a machine
learning algorithm does not work as well.  For this reason, the
recurrent neural network is trained on sequences of y values of the
form ((y1, y2), y3), so that the network is concerned with learning
the pattern of the y data and not the relation between the x and y
data.  As long as the pattern of y data outside of the training region
stays relatively stable compared to what was inside the training
region, this method of training can produce accurate extrapolations to
y values far removed from the training data set.
</p>

<!--  -->
<!-- The idea behind formatting the data in this way comes from [this resource](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/) and [this one](https://fairyonice.github.io/Understand-Keras%27s-RNN-behind-the-scenes-with-a-sin-wave-example.html). -->
<!--  -->
<!-- The following method takes in a y data set and formats it so the "x data" are of the form (y1, y2) and the "y data" are of the form y3, with extra brackets added in to make the resulting arrays compatable with both Keras and Tensorflow. -->
<!--  -->
<!-- Note: Using a sequence length of two is not required for time series forecasting so any lenght of sequence could be used (for example instead of ((y1, y2) y3) you could change the length of sequence to be 4 and the resulting data points would have the form ((y1, y2, y3, y4), y5)).  While the following method can be used to create a data set of any sequence length, the remainder of the code expects the length of sequence to be 2.  This is because the data sets are very small and the higher the lenght of the sequence the less resulting data points. -->


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># FORMAT_DATA</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">format_data</span>(data, length_of_sequence <span style="color: #666666">=</span> <span style="color: #666666">2</span>):  
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            data(a numpy array): the data that will be the inputs to the recurrent neural</span>
<span style="color: #BA2121; font-style: italic">                network</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequence (an int): the number of elements in one iteration of the</span>
<span style="color: #BA2121; font-style: italic">                sequence patter.  For a function approximator use length_of_sequence = 2.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            rnn_input (a 3D numpy array): the input data for the recurrent neural network.  Its</span>
<span style="color: #BA2121; font-style: italic">                dimensions are length of data - length of sequence, length of sequence, </span>
<span style="color: #BA2121; font-style: italic">                dimnsion of data</span>
<span style="color: #BA2121; font-style: italic">            rnn_output (a numpy array): the training data for the neural network</span>
<span style="color: #BA2121; font-style: italic">        Formats data to be used in a recurrent neural network.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>

    X, Y <span style="color: #666666">=</span> [], []
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">len</span>(data)<span style="color: #666666">-</span>length_of_sequence):
        <span style="color: #408080; font-style: italic"># Get the next length_of_sequence elements</span>
        a <span style="color: #666666">=</span> data[i:i<span style="color: #666666">+</span>length_of_sequence]
        <span style="color: #408080; font-style: italic"># Get the element that immediately follows that</span>
        b <span style="color: #666666">=</span> data[i<span style="color: #666666">+</span>length_of_sequence]
        <span style="color: #408080; font-style: italic"># Reshape so that each data point is contained in its own array</span>
        a <span style="color: #666666">=</span> np<span style="color: #666666">.</span>reshape (a, (<span style="color: #008000">len</span>(a), <span style="color: #666666">1</span>))
        X<span style="color: #666666">.</span>append(a)
        Y<span style="color: #666666">.</span>append(b)
    rnn_input <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(X)
    rnn_output <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(Y)

    <span style="color: #008000; font-weight: bold">return</span> rnn_input, rnn_output


<span style="color: #408080; font-style: italic"># ## Defining the Recurrent Neural Network Using Keras</span>
<span style="color: #408080; font-style: italic"># </span>
<span style="color: #408080; font-style: italic"># The following method defines a simple recurrent neural network in keras consisting of one input layer, one hidden layer, and one output layer.</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">rnn</span>(length_of_sequences, batch_size <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">None</span>, stateful <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">False</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequences (an int): the number of y values in &quot;x data&quot;.  This is determined</span>
<span style="color: #BA2121; font-style: italic">                when the data is formatted</span>
<span style="color: #BA2121; font-style: italic">            batch_size (an int): Default value is None.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            model (a Keras model): The recurrent neural network that is built and compiled by this</span>
<span style="color: #BA2121; font-style: italic">                method</span>
<span style="color: #BA2121; font-style: italic">        Builds and compiles a recurrent neural network with one hidden layer and returns the model.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    <span style="color: #408080; font-style: italic"># Number of neurons in the input and output layers</span>
    in_out_neurons <span style="color: #666666">=</span> <span style="color: #666666">1</span>
    <span style="color: #408080; font-style: italic"># Number of neurons in the hidden layer</span>
    hidden_neurons <span style="color: #666666">=</span> <span style="color: #666666">200</span>
    <span style="color: #408080; font-style: italic"># Define the input layer</span>
    inp <span style="color: #666666">=</span> Input(batch_shape<span style="color: #666666">=</span>(batch_size, 
                length_of_sequences, 
                in_out_neurons))  
    <span style="color: #408080; font-style: italic"># Define the hidden layer as a simple RNN layer with a set number of neurons and add it to </span>
    <span style="color: #408080; font-style: italic"># the network immediately after the input layer</span>
    rnn <span style="color: #666666">=</span> SimpleRNN(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN&quot;</span>)(inp)
    <span style="color: #408080; font-style: italic"># Define the output layer as a dense neural network layer (standard neural network layer)</span>
    <span style="color: #408080; font-style: italic">#and add it to the network immediately after the hidden layer.</span>
    dens <span style="color: #666666">=</span> Dense(in_out_neurons,name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;dense&quot;</span>)(rnn)
    <span style="color: #408080; font-style: italic"># Create the machine learning model starting with the input layer and ending with the </span>
    <span style="color: #408080; font-style: italic"># output layer</span>
    model <span style="color: #666666">=</span> Model(inputs<span style="color: #666666">=</span>[inp],outputs<span style="color: #666666">=</span>[dens])
    <span style="color: #408080; font-style: italic"># Compile the machine learning model using the mean squared error function as the loss </span>
    <span style="color: #408080; font-style: italic"># function and an Adams optimizer.</span>
    model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&quot;mean_squared_error&quot;</span>, optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&quot;adam&quot;</span>)  
    <span style="color: #008000; font-weight: bold">return</span> model
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split -->
<h2 id="predicting-new-points-with-a-trained-recurrent-neural-network" class="anchor">Predicting New Points With A Trained Recurrent Neural Network </h2>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">test_rnn</span> (x1, y_test, plot_min, plot_max):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            x1 (a list or numpy array): The complete x component of the data set</span>
<span style="color: #BA2121; font-style: italic">            y_test (a list or numpy array): The complete y component of the data set</span>
<span style="color: #BA2121; font-style: italic">            plot_min (an int or float): the smallest x value used in the training data</span>
<span style="color: #BA2121; font-style: italic">            plot_max (an int or float): the largest x valye used in the training data</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            None.</span>
<span style="color: #BA2121; font-style: italic">        Uses a trained recurrent neural network model to predict future points in the </span>
<span style="color: #BA2121; font-style: italic">        series.  Computes the MSE of the predicted data set from the true data set, saves</span>
<span style="color: #BA2121; font-style: italic">        the predicted data set to a csv file, and plots the predicted and true data sets w</span>
<span style="color: #BA2121; font-style: italic">        while also displaying the data range used for training.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    <span style="color: #408080; font-style: italic"># Add the training data as the first dim points in the predicted data array as these</span>
    <span style="color: #408080; font-style: italic"># are known values.</span>
    y_pred <span style="color: #666666">=</span> y_test[:dim]<span style="color: #666666">.</span>tolist()
    <span style="color: #408080; font-style: italic"># Generate the first input to the trained recurrent neural network using the last two </span>
    <span style="color: #408080; font-style: italic"># points of the training data.  Based on how the network was trained this means that it</span>
    <span style="color: #408080; font-style: italic"># will predict the first point in the data set after the training data.  All of the </span>
    <span style="color: #408080; font-style: italic"># brackets are necessary for Tensorflow.</span>
    next_input <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([[[y_test[dim<span style="color: #666666">-2</span>]], [y_test[dim<span style="color: #666666">-1</span>]]]])
    <span style="color: #408080; font-style: italic"># Save the very last point in the training data set.  This will be used later.</span>
    last <span style="color: #666666">=</span> [y_test[dim<span style="color: #666666">-1</span>]]

    <span style="color: #408080; font-style: italic"># Iterate until the complete data set is created.</span>
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span> (dim, <span style="color: #008000">len</span>(y_test)):
        <span style="color: #408080; font-style: italic"># Predict the next point in the data set using the previous two points.</span>
        <span style="color: #008000">next</span> <span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(next_input)
        <span style="color: #408080; font-style: italic"># Append just the number of the predicted data set</span>
        y_pred<span style="color: #666666">.</span>append(<span style="color: #008000">next</span>[<span style="color: #666666">0</span>][<span style="color: #666666">0</span>])
        <span style="color: #408080; font-style: italic"># Create the input that will be used to predict the next data point in the data set.</span>
        next_input <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([[last, <span style="color: #008000">next</span>[<span style="color: #666666">0</span>]]], dtype<span style="color: #666666">=</span>np<span style="color: #666666">.</span>float64)
        last <span style="color: #666666">=</span> <span style="color: #008000">next</span>

    <span style="color: #408080; font-style: italic"># Print the mean squared error between the known data set and the predicted data set.</span>
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;MSE: &#39;</span>, np<span style="color: #666666">.</span>square(np<span style="color: #666666">.</span>subtract(y_test, y_pred))<span style="color: #666666">.</span>mean())
    <span style="color: #408080; font-style: italic"># Save the predicted data set as a csv file for later use</span>
    name <span style="color: #666666">=</span> datatype <span style="color: #666666">+</span> <span style="color: #BA2121">&#39;Predicted&#39;</span><span style="color: #666666">+</span><span style="color: #008000">str</span>(dim)<span style="color: #666666">+</span><span style="color: #BA2121">&#39;.csv&#39;</span>
    np<span style="color: #666666">.</span>savetxt(name, y_pred, delimiter<span style="color: #666666">=</span><span style="color: #BA2121">&#39;,&#39;</span>)
    <span style="color: #408080; font-style: italic"># Plot the known data set and the predicted data set.  The red box represents the region that was used</span>
    <span style="color: #408080; font-style: italic"># for the training data.</span>
    fig, ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots()
    ax<span style="color: #666666">.</span>plot(x1, y_test, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;true&quot;</span>, linewidth<span style="color: #666666">=3</span>)
    ax<span style="color: #666666">.</span>plot(x1, y_pred, <span style="color: #BA2121">&#39;g-.&#39;</span>,label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;predicted&quot;</span>, linewidth<span style="color: #666666">=4</span>)
    ax<span style="color: #666666">.</span>legend()
    <span style="color: #408080; font-style: italic"># Created a red region to represent the points used in the training data.</span>
    ax<span style="color: #666666">.</span>axvspan(plot_min, plot_max, alpha<span style="color: #666666">=0.25</span>, color<span style="color: #666666">=</span><span style="color: #BA2121">&#39;red&#39;</span>)
    plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># Check to make sure the data set is complete</span>
<span style="color: #008000; font-weight: bold">assert</span> <span style="color: #008000">len</span>(X_tot) <span style="color: #666666">==</span> <span style="color: #008000">len</span>(y_tot)

<span style="color: #408080; font-style: italic"># This is the number of points that will be used in as the training data</span>
dim<span style="color: #666666">=12</span>

<span style="color: #408080; font-style: italic"># Separate the training data from the whole data set</span>
X_train <span style="color: #666666">=</span> X_tot[:dim]
y_train <span style="color: #666666">=</span> y_tot[:dim]


<span style="color: #408080; font-style: italic"># Generate the training data for the RNN, using a sequence of 2</span>
rnn_input, rnn_training <span style="color: #666666">=</span> format_data(y_train, <span style="color: #666666">2</span>)


<span style="color: #408080; font-style: italic"># Create a recurrent neural network in Keras and produce a summary of the </span>
<span style="color: #408080; font-style: italic"># machine learning model</span>
model <span style="color: #666666">=</span> rnn(length_of_sequences <span style="color: #666666">=</span> rnn_input<span style="color: #666666">.</span>shape[<span style="color: #666666">1</span>])
model<span style="color: #666666">.</span>summary()

<span style="color: #408080; font-style: italic"># Start the timer.  Want to time training+testing</span>
start <span style="color: #666666">=</span> timer()
<span style="color: #408080; font-style: italic"># Fit the model using the training data genenerated above using 150 training iterations and a 5%</span>
<span style="color: #408080; font-style: italic"># validation split.  Setting verbose to True prints information about each training iteration.</span>
hist <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(rnn_input, rnn_training, batch_size<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>, epochs<span style="color: #666666">=150</span>, 
                 verbose<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,validation_split<span style="color: #666666">=0.05</span>)

<span style="color: #008000; font-weight: bold">for</span> label <span style="color: #AA22FF; font-weight: bold">in</span> [<span style="color: #BA2121">&quot;loss&quot;</span>,<span style="color: #BA2121">&quot;val_loss&quot;</span>]:
    plt<span style="color: #666666">.</span>plot(hist<span style="color: #666666">.</span>history[label],label<span style="color: #666666">=</span>label)

plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;loss&quot;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;epoch&quot;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;The final validation loss: </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(hist<span style="color: #666666">.</span>history[<span style="color: #BA2121">&quot;val_loss&quot;</span>][<span style="color: #666666">-1</span>]))
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># Use the trained neural network to predict more points of the data set</span>
test_rnn(X_tot, y_tot, X_tot[<span style="color: #666666">0</span>], X_tot[dim<span style="color: #666666">-1</span>])
<span style="color: #408080; font-style: italic"># Stop the timer and calculate the total time needed.</span>
end <span style="color: #666666">=</span> timer()
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Time: &#39;</span>, end<span style="color: #666666">-</span>start)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split -->
<h2 id="other-things-to-try" class="anchor">Other Things to Try </h2>

<p>Changing the size of the recurrent neural network and its parameters
can drastically change the results you get from the model.  The below
code takes the simple recurrent neural network from above and adds a
second hidden layer, changes the number of neurons in the hidden
layer, and explicitly declares the activation function of the hidden
layers to be a sigmoid function.  The loss function and optimizer can
also be changed but are kept the same as the above network.  These
parameters can be tuned to provide the optimal result from the
network.  For some ideas on how to improve the performance of a
<a href="https://danijar.com/tips-for-training-recurrent-neural-networks" target="_self">recurrent neural network</a>.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">rnn_2layers</span>(length_of_sequences, batch_size <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">None</span>, stateful <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">False</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequences (an int): the number of y values in &quot;x data&quot;.  This is determined</span>
<span style="color: #BA2121; font-style: italic">                when the data is formatted</span>
<span style="color: #BA2121; font-style: italic">            batch_size (an int): Default value is None.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            model (a Keras model): The recurrent neural network that is built and compiled by this</span>
<span style="color: #BA2121; font-style: italic">                method</span>
<span style="color: #BA2121; font-style: italic">        Builds and compiles a recurrent neural network with two hidden layers and returns the model.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    <span style="color: #408080; font-style: italic"># Number of neurons in the input and output layers</span>
    in_out_neurons <span style="color: #666666">=</span> <span style="color: #666666">1</span>
    <span style="color: #408080; font-style: italic"># Number of neurons in the hidden layer, increased from the first network</span>
    hidden_neurons <span style="color: #666666">=</span> <span style="color: #666666">500</span>
    <span style="color: #408080; font-style: italic"># Define the input layer</span>
    inp <span style="color: #666666">=</span> Input(batch_shape<span style="color: #666666">=</span>(batch_size, 
                length_of_sequences, 
                in_out_neurons))  
    <span style="color: #408080; font-style: italic"># Create two hidden layers instead of one hidden layer.  Explicitly set the activation</span>
    <span style="color: #408080; font-style: italic"># function to be the sigmoid function (the default value is hyperbolic tangent)</span>
    rnn1 <span style="color: #666666">=</span> SimpleRNN(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,  <span style="color: #408080; font-style: italic"># This needs to be True if another hidden layer is to follow</span>
                    stateful <span style="color: #666666">=</span> stateful, activation <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;sigmoid&#39;</span>,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN1&quot;</span>)(inp)
    rnn2 <span style="color: #666666">=</span> SimpleRNN(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>, activation <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;sigmoid&#39;</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN2&quot;</span>)(rnn1)
    <span style="color: #408080; font-style: italic"># Define the output layer as a dense neural network layer (standard neural network layer)</span>
    <span style="color: #408080; font-style: italic">#and add it to the network immediately after the hidden layer.</span>
    dens <span style="color: #666666">=</span> Dense(in_out_neurons,name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;dense&quot;</span>)(rnn2)
    <span style="color: #408080; font-style: italic"># Create the machine learning model starting with the input layer and ending with the </span>
    <span style="color: #408080; font-style: italic"># output layer</span>
    model <span style="color: #666666">=</span> Model(inputs<span style="color: #666666">=</span>[inp],outputs<span style="color: #666666">=</span>[dens])
    <span style="color: #408080; font-style: italic"># Compile the machine learning model using the mean squared error function as the loss </span>
    <span style="color: #408080; font-style: italic"># function and an Adams optimizer.</span>
    model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&quot;mean_squared_error&quot;</span>, optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&quot;adam&quot;</span>)  
    <span style="color: #008000; font-weight: bold">return</span> model

<span style="color: #408080; font-style: italic"># Check to make sure the data set is complete</span>
<span style="color: #008000; font-weight: bold">assert</span> <span style="color: #008000">len</span>(X_tot) <span style="color: #666666">==</span> <span style="color: #008000">len</span>(y_tot)

<span style="color: #408080; font-style: italic"># This is the number of points that will be used in as the training data</span>
dim<span style="color: #666666">=12</span>

<span style="color: #408080; font-style: italic"># Separate the training data from the whole data set</span>
X_train <span style="color: #666666">=</span> X_tot[:dim]
y_train <span style="color: #666666">=</span> y_tot[:dim]


<span style="color: #408080; font-style: italic"># Generate the training data for the RNN, using a sequence of 2</span>
rnn_input, rnn_training <span style="color: #666666">=</span> format_data(y_train, <span style="color: #666666">2</span>)


<span style="color: #408080; font-style: italic"># Create a recurrent neural network in Keras and produce a summary of the </span>
<span style="color: #408080; font-style: italic"># machine learning model</span>
model <span style="color: #666666">=</span> rnn_2layers(length_of_sequences <span style="color: #666666">=</span> <span style="color: #666666">2</span>)
model<span style="color: #666666">.</span>summary()

<span style="color: #408080; font-style: italic"># Start the timer.  Want to time training+testing</span>
start <span style="color: #666666">=</span> timer()
<span style="color: #408080; font-style: italic"># Fit the model using the training data genenerated above using 150 training iterations and a 5%</span>
<span style="color: #408080; font-style: italic"># validation split.  Setting verbose to True prints information about each training iteration.</span>
hist <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(rnn_input, rnn_training, batch_size<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>, epochs<span style="color: #666666">=150</span>, 
                 verbose<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,validation_split<span style="color: #666666">=0.05</span>)


<span style="color: #408080; font-style: italic"># This section plots the training loss and the validation loss as a function of training iteration.</span>
<span style="color: #408080; font-style: italic"># This is not required for analyzing the couple cluster data but can help determine if the network is</span>
<span style="color: #408080; font-style: italic"># being overtrained.</span>
<span style="color: #008000; font-weight: bold">for</span> label <span style="color: #AA22FF; font-weight: bold">in</span> [<span style="color: #BA2121">&quot;loss&quot;</span>,<span style="color: #BA2121">&quot;val_loss&quot;</span>]:
    plt<span style="color: #666666">.</span>plot(hist<span style="color: #666666">.</span>history[label],label<span style="color: #666666">=</span>label)

plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;loss&quot;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;epoch&quot;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;The final validation loss: </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(hist<span style="color: #666666">.</span>history[<span style="color: #BA2121">&quot;val_loss&quot;</span>][<span style="color: #666666">-1</span>]))
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># Use the trained neural network to predict more points of the data set</span>
test_rnn(X_tot, y_tot, X_tot[<span style="color: #666666">0</span>], X_tot[dim<span style="color: #666666">-1</span>])
<span style="color: #408080; font-style: italic"># Stop the timer and calculate the total time needed.</span>
end <span style="color: #666666">=</span> timer()
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Time: &#39;</span>, end<span style="color: #666666">-</span>start)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split -->
<h2 id="other-types-of-recurrent-neural-networks" class="anchor">Other Types of Recurrent Neural Networks </h2>

<p>Besides a simple recurrent neural network layer, there are two other
commonly used types of recurrent neural network layers: Long Short
Term Memory (LSTM) and Gated Recurrent Unit (GRU).  For a short
introduction to these layers see <a href="https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b" target="_self"><tt>https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b</tt></a>
and <a href="https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b" target="_self"><tt>https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b</tt></a>.
</p>

<p>The first network created below is similar to the previous network,
but it replaces the SimpleRNN layers with LSTM layers.  The second
network below has two hidden layers made up of GRUs, which are
preceeded by two dense (feeddorward) neural network layers.  These
dense layers "preprocess" the data before it reaches the recurrent
layers.  This architecture has been shown to improve the performance
of recurrent neural networks (see the link above and also
<a href="https://arxiv.org/pdf/1807.02857.pdf" target="_self"><tt>https://arxiv.org/pdf/1807.02857.pdf</tt></a>.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">lstm_2layers</span>(length_of_sequences, batch_size <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">None</span>, stateful <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">False</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequences (an int): the number of y values in &quot;x data&quot;.  This is determined</span>
<span style="color: #BA2121; font-style: italic">                when the data is formatted</span>
<span style="color: #BA2121; font-style: italic">            batch_size (an int): Default value is None.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            model (a Keras model): The recurrent neural network that is built and compiled by this</span>
<span style="color: #BA2121; font-style: italic">                method</span>
<span style="color: #BA2121; font-style: italic">        Builds and compiles a recurrent neural network with two LSTM hidden layers and returns the model.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    <span style="color: #408080; font-style: italic"># Number of neurons on the input/output layer and the number of neurons in the hidden layer</span>
    in_out_neurons <span style="color: #666666">=</span> <span style="color: #666666">1</span>
    hidden_neurons <span style="color: #666666">=</span> <span style="color: #666666">250</span>
    <span style="color: #408080; font-style: italic"># Input Layer</span>
    inp <span style="color: #666666">=</span> Input(batch_shape<span style="color: #666666">=</span>(batch_size, 
                length_of_sequences, 
                in_out_neurons)) 
    <span style="color: #408080; font-style: italic"># Hidden layers (in this case they are LSTM layers instead if SimpleRNN layers)</span>
    rnn<span style="color: #666666">=</span> LSTM(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN&quot;</span>, use_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;tanh&#39;</span>)(inp)
    rnn1 <span style="color: #666666">=</span> LSTM(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN1&quot;</span>, use_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;tanh&#39;</span>)(rnn)
    <span style="color: #408080; font-style: italic"># Output layer</span>
    dens <span style="color: #666666">=</span> Dense(in_out_neurons,name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;dense&quot;</span>)(rnn1)
    <span style="color: #408080; font-style: italic"># Define the midel</span>
    model <span style="color: #666666">=</span> Model(inputs<span style="color: #666666">=</span>[inp],outputs<span style="color: #666666">=</span>[dens])
    <span style="color: #408080; font-style: italic"># Compile the model</span>
    model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&#39;mean_squared_error&#39;</span>, optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&#39;adam&#39;</span>)  
    <span style="color: #408080; font-style: italic"># Return the model</span>
    <span style="color: #008000; font-weight: bold">return</span> model

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">dnn2_gru2</span>(length_of_sequences, batch_size <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">None</span>, stateful <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">False</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequences (an int): the number of y values in &quot;x data&quot;.  This is determined</span>
<span style="color: #BA2121; font-style: italic">                when the data is formatted</span>
<span style="color: #BA2121; font-style: italic">            batch_size (an int): Default value is None.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            model (a Keras model): The recurrent neural network that is built and compiled by this</span>
<span style="color: #BA2121; font-style: italic">                method</span>
<span style="color: #BA2121; font-style: italic">        Builds and compiles a recurrent neural network with four hidden layers (two dense followed by</span>
<span style="color: #BA2121; font-style: italic">        two GRU layers) and returns the model.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>    
    <span style="color: #408080; font-style: italic"># Number of neurons on the input/output layers and hidden layers</span>
    in_out_neurons <span style="color: #666666">=</span> <span style="color: #666666">1</span>
    hidden_neurons <span style="color: #666666">=</span> <span style="color: #666666">250</span>
    <span style="color: #408080; font-style: italic"># Input layer</span>
    inp <span style="color: #666666">=</span> Input(batch_shape<span style="color: #666666">=</span>(batch_size, 
                length_of_sequences, 
                in_out_neurons)) 
    <span style="color: #408080; font-style: italic"># Hidden Dense (feedforward) layers</span>
    dnn <span style="color: #666666">=</span> Dense(hidden_neurons<span style="color: #666666">/2</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;relu&#39;</span>, name<span style="color: #666666">=</span><span style="color: #BA2121">&#39;dnn&#39;</span>)(inp)
    dnn1 <span style="color: #666666">=</span> Dense(hidden_neurons<span style="color: #666666">/2</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;relu&#39;</span>, name<span style="color: #666666">=</span><span style="color: #BA2121">&#39;dnn1&#39;</span>)(dnn)
    <span style="color: #408080; font-style: italic"># Hidden GRU layers</span>
    rnn1 <span style="color: #666666">=</span> GRU(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN1&quot;</span>, use_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)(dnn1)
    rnn <span style="color: #666666">=</span> GRU(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN&quot;</span>, use_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)(rnn1)
    <span style="color: #408080; font-style: italic"># Output layer</span>
    dens <span style="color: #666666">=</span> Dense(in_out_neurons,name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;dense&quot;</span>)(rnn)
    <span style="color: #408080; font-style: italic"># Define the model</span>
    model <span style="color: #666666">=</span> Model(inputs<span style="color: #666666">=</span>[inp],outputs<span style="color: #666666">=</span>[dens])
    <span style="color: #408080; font-style: italic"># Compile the mdoel</span>
    model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&#39;mean_squared_error&#39;</span>, optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&#39;adam&#39;</span>)  
    <span style="color: #408080; font-style: italic"># Return the model</span>
    <span style="color: #008000; font-weight: bold">return</span> model

<span style="color: #408080; font-style: italic"># Check to make sure the data set is complete</span>
<span style="color: #008000; font-weight: bold">assert</span> <span style="color: #008000">len</span>(X_tot) <span style="color: #666666">==</span> <span style="color: #008000">len</span>(y_tot)

<span style="color: #408080; font-style: italic"># This is the number of points that will be used in as the training data</span>
dim<span style="color: #666666">=12</span>

<span style="color: #408080; font-style: italic"># Separate the training data from the whole data set</span>
X_train <span style="color: #666666">=</span> X_tot[:dim]
y_train <span style="color: #666666">=</span> y_tot[:dim]


<span style="color: #408080; font-style: italic"># Generate the training data for the RNN, using a sequence of 2</span>
rnn_input, rnn_training <span style="color: #666666">=</span> format_data(y_train, <span style="color: #666666">2</span>)


<span style="color: #408080; font-style: italic"># Create a recurrent neural network in Keras and produce a summary of the </span>
<span style="color: #408080; font-style: italic"># machine learning model</span>
<span style="color: #408080; font-style: italic"># Change the method name to reflect which network you want to use</span>
model <span style="color: #666666">=</span> dnn2_gru2(length_of_sequences <span style="color: #666666">=</span> <span style="color: #666666">2</span>)
model<span style="color: #666666">.</span>summary()

<span style="color: #408080; font-style: italic"># Start the timer.  Want to time training+testing</span>
start <span style="color: #666666">=</span> timer()
<span style="color: #408080; font-style: italic"># Fit the model using the training data genenerated above using 150 training iterations and a 5%</span>
<span style="color: #408080; font-style: italic"># validation split.  Setting verbose to True prints information about each training iteration.</span>
hist <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(rnn_input, rnn_training, batch_size<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>, epochs<span style="color: #666666">=150</span>, 
                 verbose<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,validation_split<span style="color: #666666">=0.05</span>)


<span style="color: #408080; font-style: italic"># This section plots the training loss and the validation loss as a function of training iteration.</span>
<span style="color: #408080; font-style: italic"># This is not required for analyzing the couple cluster data but can help determine if the network is</span>
<span style="color: #408080; font-style: italic"># being overtrained.</span>
<span style="color: #008000; font-weight: bold">for</span> label <span style="color: #AA22FF; font-weight: bold">in</span> [<span style="color: #BA2121">&quot;loss&quot;</span>,<span style="color: #BA2121">&quot;val_loss&quot;</span>]:
    plt<span style="color: #666666">.</span>plot(hist<span style="color: #666666">.</span>history[label],label<span style="color: #666666">=</span>label)

plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;loss&quot;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;epoch&quot;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;The final validation loss: </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(hist<span style="color: #666666">.</span>history[<span style="color: #BA2121">&quot;val_loss&quot;</span>][<span style="color: #666666">-1</span>]))
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># Use the trained neural network to predict more points of the data set</span>
test_rnn(X_tot, y_tot, X_tot[<span style="color: #666666">0</span>], X_tot[dim<span style="color: #666666">-1</span>])
<span style="color: #408080; font-style: italic"># Stop the timer and calculate the total time needed.</span>
end <span style="color: #666666">=</span> timer()
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Time: &#39;</span>, end<span style="color: #666666">-</span>start)


<span style="color: #408080; font-style: italic"># ### Training Recurrent Neural Networks in the Standard Way (i.e. learning the relationship between the X and Y data)</span>
<span style="color: #408080; font-style: italic"># </span>
<span style="color: #408080; font-style: italic"># Finally, comparing the performace of a recurrent neural network using the standard data formatting to the performance of the network with time sequence data formatting shows the benefit of this type of data formatting with extrapolation.</span>

<span style="color: #408080; font-style: italic"># Check to make sure the data set is complete</span>
<span style="color: #008000; font-weight: bold">assert</span> <span style="color: #008000">len</span>(X_tot) <span style="color: #666666">==</span> <span style="color: #008000">len</span>(y_tot)

<span style="color: #408080; font-style: italic"># This is the number of points that will be used in as the training data</span>
dim<span style="color: #666666">=12</span>

<span style="color: #408080; font-style: italic"># Separate the training data from the whole data set</span>
X_train <span style="color: #666666">=</span> X_tot[:dim]
y_train <span style="color: #666666">=</span> y_tot[:dim]

<span style="color: #408080; font-style: italic"># Reshape the data for Keras specifications</span>
X_train <span style="color: #666666">=</span> X_train<span style="color: #666666">.</span>reshape((dim, <span style="color: #666666">1</span>))
y_train <span style="color: #666666">=</span> y_train<span style="color: #666666">.</span>reshape((dim, <span style="color: #666666">1</span>))


<span style="color: #408080; font-style: italic"># Create a recurrent neural network in Keras and produce a summary of the </span>
<span style="color: #408080; font-style: italic"># machine learning model</span>
<span style="color: #408080; font-style: italic"># Set the sequence length to 1 for regular data formatting </span>
model <span style="color: #666666">=</span> rnn(length_of_sequences <span style="color: #666666">=</span> <span style="color: #666666">1</span>)
model<span style="color: #666666">.</span>summary()

<span style="color: #408080; font-style: italic"># Start the timer.  Want to time training+testing</span>
start <span style="color: #666666">=</span> timer()
<span style="color: #408080; font-style: italic"># Fit the model using the training data genenerated above using 150 training iterations and a 5%</span>
<span style="color: #408080; font-style: italic"># validation split.  Setting verbose to True prints information about each training iteration.</span>
hist <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(X_train, y_train, batch_size<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>, epochs<span style="color: #666666">=150</span>, 
                 verbose<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,validation_split<span style="color: #666666">=0.05</span>)


<span style="color: #408080; font-style: italic"># This section plots the training loss and the validation loss as a function of training iteration.</span>
<span style="color: #408080; font-style: italic"># This is not required for analyzing the couple cluster data but can help determine if the network is</span>
<span style="color: #408080; font-style: italic"># being overtrained.</span>
<span style="color: #008000; font-weight: bold">for</span> label <span style="color: #AA22FF; font-weight: bold">in</span> [<span style="color: #BA2121">&quot;loss&quot;</span>,<span style="color: #BA2121">&quot;val_loss&quot;</span>]:
    plt<span style="color: #666666">.</span>plot(hist<span style="color: #666666">.</span>history[label],label<span style="color: #666666">=</span>label)

plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;loss&quot;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;epoch&quot;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;The final validation loss: </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(hist<span style="color: #666666">.</span>history[<span style="color: #BA2121">&quot;val_loss&quot;</span>][<span style="color: #666666">-1</span>]))
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># Use the trained neural network to predict the remaining data points</span>
X_pred <span style="color: #666666">=</span> X_tot[dim:]
X_pred <span style="color: #666666">=</span> X_pred<span style="color: #666666">.</span>reshape((<span style="color: #008000">len</span>(X_pred), <span style="color: #666666">1</span>))
y_model <span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(X_pred)
y_pred <span style="color: #666666">=</span> np<span style="color: #666666">.</span>concatenate((y_tot[:dim], y_model<span style="color: #666666">.</span>flatten()))

<span style="color: #408080; font-style: italic"># Plot the known data set and the predicted data set.  The red box represents the region that was used</span>
<span style="color: #408080; font-style: italic"># for the training data.</span>
fig, ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots()
ax<span style="color: #666666">.</span>plot(X_tot, y_tot, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;true&quot;</span>, linewidth<span style="color: #666666">=3</span>)
ax<span style="color: #666666">.</span>plot(X_tot, y_pred, <span style="color: #BA2121">&#39;g-.&#39;</span>,label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;predicted&quot;</span>, linewidth<span style="color: #666666">=4</span>)
ax<span style="color: #666666">.</span>legend()
<span style="color: #408080; font-style: italic"># Created a red region to represent the points used in the training data.</span>
ax<span style="color: #666666">.</span>axvspan(X_tot[<span style="color: #666666">0</span>], X_tot[dim], alpha<span style="color: #666666">=0.25</span>, color<span style="color: #666666">=</span><span style="color: #BA2121">&#39;red&#39;</span>)
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># Stop the timer and calculate the total time needed.</span>
end <span style="color: #666666">=</span> timer()
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Time: &#39;</span>, end<span style="color: #666666">-</span>start)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split -->
<h2 id="gating-mechanism-long-short-term-memory-lstm" class="anchor">Gating mechanism: Long Short Term Memory (LSTM) </h2>

<p>Besides a simple recurrent neural network layer, as discussed above, there are two other
commonly used types of recurrent neural network layers: Long Short
Term Memory (LSTM) and Gated Recurrent Unit (GRU).  For a short
introduction to these layers see <a href="https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b" target="_self"><tt>https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b</tt></a>
and <a href="https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b" target="_self"><tt>https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b</tt></a>.
</p>

<p>LSTM uses a memory cell for 
modeling long-range dependencies and avoid vanishing gradient
 problems.
Capable of modeling longer term dependencies by having
memory cells and gates that controls the information flow along
with the memory cells.
</p>

<ol>
<li> Introduced by Hochreiter and Schmidhuber (1997) who solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).</li>
<li> They designed a memory cell using logistic and linear units with multiplicative interactions.</li>
<li> Information gets into the cell whenever its &#8220;write&#8221; gate is on.</li>
<li> The information stays in the cell so long as its <b>keep</b> gate is on.</li>
<li> Information can be read from the cell by turning on its <b>read</b> gate.</li> 
</ol>
<!-- !split -->
<h2 id="implementing-a-memory-cell-in-a-neural-network" class="anchor">Implementing a memory cell in a neural network </h2>

<p>To preserve information for a long time in
the activities of an RNN, we use a circuit
that implements an analog memory cell.
</p>

<ol>
<li> A linear unit that has a self-link with a weight of 1 will maintain its state.</li>
<li> Information is stored in the cell by activating its write gate.</li>
<li> Information is retrieved by activating the read gate.</li>
<li> We can backpropagate through this circuit because logistics are have nice derivatives.</li> 
</ol>
<p>In contrast to FFNs, recurrent networks introduce feedback
connections, meaning the information is allowed to be carried to
subsequent nodes across different time steps. These cyclic or feedback
connections have the objective of providing the network with some kind
of memory, making RNNs particularly suited for time-series data,
natural language processing, speech recognition, and several other
problems for which the order of the data is crucial.
</p>

<p>RNN architectures vary greatly in how they manage information flow and
memory in the network. Different architectures often aim at improving
some sub-optimal characteristics of the network. The simplest form of
recurrent network, commonly called simple or vanilla RNN, for example,
is known to suffer from the problem of vanishing gradients.  This
problem arises due to the nature of backpropagation in time. Gradients
of the cost/loss function may get exponentially small (or large) if
there are many layers in the network, which is the case of RNN when
the sequence gets long. Consequently, the convergence to correctly
predicted values in the training process happens very slowly. This is
particularly the case when using activation functions like the Sigmoid
or hyperbolic tangent, (whose outputs are between 0 and 1 for Sigmoid,
and -1 and 1 for \( \tanh \)), with equally small gradients by design
(\( < 1 \)).
</p>

<p>To address that, one natural alternative is to use the Long Short-Term
Memory (LSTM) variation. While LSTMs are generally used, they also
present the downside of being inefficient for large-scale models. For
such problems, Gated recurrent units (GRUs) can be employed as they
have a simpler architecture, thus being computationally faster. We
will hereafter focus simply on the Vanilla and LSTM variants for their
simplicity and popularity.
</p>

<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2024, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</body>
</html>

