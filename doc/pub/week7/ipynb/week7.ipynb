{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d39768",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week7.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: February 26-March 1: Advanced machine learning and data analysis for the physical sciences -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eec6f6",
   "metadata": {
    "editable": true
   },
   "source": [
    "# February 26-March 1: Advanced machine learning and data analysis for the physical sciences\n",
    "**Morten Hjorth-Jensen**, Department of Physics and Center for Computing in Science Education, University of Oslo, Norway and Department of Physics and Astronomy and Facility for Rare Isotope Beams, Michigan State University, East Lansing, Michigan, USA\n",
    "\n",
    "Date: **February 26-March 1, 2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030c8e0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Plans for the week February 26-March 1\n",
    "\n",
    "1. Finalizing discussion of Convolutional  Neural Networks (CNNs)\n",
    "\n",
    "2. Discussion of recurrent neural networks (RNNs)\n",
    "\n",
    "3. Reading recommendations:\n",
    "\n",
    "a. Goodfellow, Bengio and Courville's chapter 10 from [Deep Learning](https://www.deeplearningbook.org/)\n",
    "\n",
    "b. [Sebastian Rashcka et al, chapter 15, Machine learning with Sickit-Learn and PyTorch](https://sebastianraschka.com/blog/2022/ml-pytorch-book.html)\n",
    "\n",
    "c. [David Foster, Generative Deep Learning with TensorFlow, see chapter 5](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html)\n",
    "\n",
    "The last two books have codes for RNNs in PyTorch and TensorFlow/Keras. Next week we will study the solution of differential equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00398fe9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## From FFNNs and CNNs to recurrent neural networks (RNNs)\n",
    "\n",
    "There are limitation of FFNNs, one of which being that FFNNs are not\n",
    "designed to handle sequential data (data for which the order matters)\n",
    "effectively because they lack the capabilities of storing information\n",
    "about previous inputs; each input is being treated indepen-\n",
    "dently. This is a limitation when dealing with sequential data where\n",
    "past information can be vital to correctly process current and future\n",
    "inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec1ddf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Feedback connections\n",
    "\n",
    "In contrast to FFNs, recurrent networks introduce feedback\n",
    "connections, meaning the information is al- lowed to be carried to\n",
    "subsequent nodes across different time steps. These cyclic or feedback\n",
    "connections have the objective of providing the network with some kind\n",
    "of memory, making RNNs particularly suited for time- series data,\n",
    "natural language processing, speech recog- nition, and several other\n",
    "problems for which the order of the data is crucial.  RNN\n",
    "architectures vary greatly in how they manage information flow and\n",
    "memory in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8eaac4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Vanishing gradients\n",
    "\n",
    "Different architectures often aim at improving\n",
    "some sub-optimal characteristics of the network. The simplest form of\n",
    "recurrent network, commonly called simple or vanilla RNN, for example,\n",
    "is known to suffer from the problem of vanishing gradients. This\n",
    "problem arises due to the nature of backpropagation in time. Gradients\n",
    "of the cost/loss function may get exponentially small (or large) if\n",
    "there are many layers in the network, which is the case of RNN when\n",
    "the sequence gets long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80bcf22",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Recurrent neural networks (RNNs): Overarching view\n",
    "\n",
    "Till now our focus has been, including convolutional neural networks\n",
    "as well, on feedforward neural networks. The output or the activations\n",
    "flow only in one direction, from the input layer to the output layer.\n",
    "\n",
    "A recurrent neural network (RNN) looks very much like a feedforward\n",
    "neural network, except that it also has connections pointing\n",
    "backward. \n",
    "\n",
    "RNNs are used to analyze time series data such as stock prices, and\n",
    "tell you when to buy or sell. In autonomous driving systems, they can\n",
    "anticipate car trajectories and help avoid accidents. More generally,\n",
    "they can work on sequences of arbitrary lengths, rather than on\n",
    "fixed-sized inputs like all the nets we have discussed so far. For\n",
    "example, they can take sentences, documents, or audio samples as\n",
    "input, making them extremely useful for natural language processing\n",
    "systems such as automatic translation and speech-to-text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ae7bf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Sequential data only?\n",
    "\n",
    "An important issue is that in many deep learning methods we assume\n",
    "that the input and output data can be treated as independent and\n",
    "identically distributed, normally abbreviated to **iid**.\n",
    "This means that the data we use can be seen as mutually independent.\n",
    "\n",
    "This is however not the case for most data sets used in RNNs since we\n",
    "are dealing with sequences of data with strong inter-dependencies.\n",
    "This applies in particular to time series, which are sequential by\n",
    "contruction.\n",
    "\n",
    "As an example, the solutions of ordinary differential equations can be\n",
    "represented as a time series, similarly, how stock prices evolve as\n",
    "function of time is another example of a typical time series, or voice\n",
    "records and many other examples.\n",
    "\n",
    "Not all sequential data may however have a time stamp, texts being a\n",
    "typical example thereof, or DNA sequences.\n",
    "\n",
    "The main focus here is on data that can be structured either as time\n",
    "series or as ordered series of data.  We will not focus on for example\n",
    "natural language processing or similar data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a94680",
   "metadata": {
    "editable": true
   },
   "source": [
    "## A simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa74b1a",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Start importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model, Sequential \n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU\n",
    "from tensorflow.keras import optimizers     \n",
    "from tensorflow.keras import regularizers           \n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "\n",
    "\n",
    "# convert into dataset matrix\n",
    "def convertToMatrix(data, step):\n",
    " X, Y =[], []\n",
    " for i in range(len(data)-step):\n",
    "  d=i+step  \n",
    "  X.append(data[i:d,])\n",
    "  Y.append(data[d,])\n",
    " return np.array(X), np.array(Y)\n",
    "\n",
    "step = 4\n",
    "N = 1000    \n",
    "Tp = 800    \n",
    "\n",
    "t=np.arange(0,N)\n",
    "x=np.sin(0.02*t)+2*np.random.rand(N)\n",
    "df = pd.DataFrame(x)\n",
    "df.head()\n",
    "\n",
    "values=df.values\n",
    "train,test = values[0:Tp,:], values[Tp:N,:]\n",
    "\n",
    "# add step elements into train and test\n",
    "test = np.append(test,np.repeat(test[-1,],step))\n",
    "train = np.append(train,np.repeat(train[-1,],step))\n",
    " \n",
    "trainX,trainY =convertToMatrix(train,step)\n",
    "testX,testY =convertToMatrix(test,step)\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=32, input_shape=(1,step), activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\")) \n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "model.summary()\n",
    "\n",
    "model.fit(trainX,trainY, epochs=100, batch_size=16, verbose=2)\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict= model.predict(testX)\n",
    "predicted=np.concatenate((trainPredict,testPredict),axis=0)\n",
    "\n",
    "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
    "print(trainScore)\n",
    "plt.plot(df)\n",
    "plt.plot(predicted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab49721",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Memoryless models\n",
    "\n",
    "Autoregressive models predict the next term in a sequence from a fixed number of previous terms. [David Foster, Generative Deep Learning with TensorFlow, discusses in chapter 5 autoregressive models](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html). We will come back later to autoregressive models.\n",
    "\n",
    "**Feed-forward neural networks.**\n",
    "\n",
    "These generalize autoregressive\n",
    "models by using one or more\n",
    "layers of non-linear hidden units.\n",
    "\n",
    "If we give our generative model some hidden state, and if we give\n",
    "this hidden state its own internal dynamics, we get a much more\n",
    "interesting kind of model.\n",
    "1. It can store information in its hidden state for a long time.\n",
    "\n",
    "2. If the dynamics is noisy and the way it generates outputs from its hidden state is noisy, we can never know its exact hidden state.\n",
    "\n",
    "3. The best we can do is to infer a probability distribution over the\n",
    "\n",
    "space of hidden state vectors.\n",
    "\n",
    "This inference is only tractable for two types of hidden state models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88982eeb",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs\n",
    "\n",
    "RNNs are very powerful, because they\n",
    "combine two properties:\n",
    "1. Distributed hidden state that allows them to store a lot of information about the past efficiently.\n",
    "\n",
    "2. Non-linear dynamics that allows them to update their hidden state in complicated ways.\n",
    "\n",
    "With enough neurons and time, RNNs\n",
    "can compute anything that can be\n",
    "computed by your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd9db93",
   "metadata": {
    "editable": true
   },
   "source": [
    "## What kinds of behaviour can RNNs exhibit?\n",
    "\n",
    "1. They can oscillate. \n",
    "\n",
    "2. They can settle to point attractors.\n",
    "\n",
    "3. They can behave chaotically.\n",
    "\n",
    "4. RNNs could potentially learn to implement lots of small programs that each capture a nugget of knowledge and run in parallel, interacting to produce very complicated effects.\n",
    "\n",
    "But the extensive computational needs  of RNNs makes them very hard to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f188416",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Basic layout,  [Figures from Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch](https://sebastianraschka.com/blog/2022/ml-pytorch-book.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN1.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN1.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b234df6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Solving differential equations with RNNs\n",
    "\n",
    "More material to be added here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45149c7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN2.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN2.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c52eea",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 2\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN3.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN3.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446cd3d7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 3\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN4.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN4.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d4bb53",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 4\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN5.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN5.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348754a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 5\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN6.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN6.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d6ec87",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 6\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN7.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN7.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254aee30",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 7\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN8.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN8.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8222918b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Backpropagation through time\n",
    "\n",
    "We can think of the recurrent net as a layered, feed-forward\n",
    "net with shared weights and then train the feed-forward net\n",
    "with weight constraints.\n",
    "\n",
    "We can also think of this training algorithm in the time domain:\n",
    "1. The forward pass builds up a stack of the activities of all the units at each time step.\n",
    "\n",
    "2. The backward pass peels activities off the stack to compute the error derivatives at each time step.\n",
    "\n",
    "3. After the backward pass we add together the derivatives at all the different times for each weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8b50f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The backward pass is linear\n",
    "\n",
    "1. There is a big difference between the forward and backward passes.\n",
    "\n",
    "2. In the forward pass we use squashing functions (like the logistic) to prevent the activity vectors from exploding.\n",
    "\n",
    "3. The backward pass, is completely linear. If you double the error derivatives at the final layer, all the error derivatives will double.\n",
    "\n",
    "The forward pass determines the slope of the linear function used for\n",
    "backpropagating through each neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b12c0e9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The problem of exploding or vanishing gradients\n",
    "* What happens to the magnitude of the gradients as we backpropagate through many layers?\n",
    "\n",
    "a. If the weights are small, the gradients shrink exponentially.\n",
    "\n",
    "b. If the weights are big the gradients grow exponentially.\n",
    "\n",
    "* Typical feed-forward neural nets can cope with these exponential effects because they only have a few hidden layers.\n",
    "\n",
    "* In an RNN trained on long sequences (e.g. 100 time steps) the gradients can easily explode or vanish.\n",
    "\n",
    "a. We can avoid this by initializing the weights very carefully.\n",
    "\n",
    "* Even with good initial weights, its very hard to detect that the current target output depends on an input from many time-steps ago.\n",
    "\n",
    "RNNs have difficulty dealing with long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3536adce",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematical setup\n",
    "\n",
    "The expression for the simplest Recurrent network resembles that of a\n",
    "regular feed-forward neural network, but now with\n",
    "the concept of temporal dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ccf8e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{a}^{(t)} & = U * \\mathbf{x}^{(t)} + W * \\mathbf{h}^{(t-1)} + \\mathbf{b}, \\notag \\\\\n",
    "    \\mathbf{h}^{(t)} &= \\sigma_h(\\mathbf{a}^{(t)}), \\notag\\\\\n",
    "    \\mathbf{o}^{(t)} &= V * \\mathbf{h}^{(t)} + \\mathbf{c}, \\notag\\\\\n",
    "    \\mathbf{\\hat{y}}^{(t)} &= \\sigma_y(\\mathbf{o}^{(t)}).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c40f1c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time through figures, part 1\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN9.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN9.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e4ac0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time, part 2\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN10.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN10.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568507c7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time, part 3\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN11.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN11.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967308d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time, part 4\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN12.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN12.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641db30",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time in equations\n",
    "\n",
    "To derive the expression of the gradients of $\\mathcal{L}$ for\n",
    "the RNN, we need to start recursively from the nodes closer to the\n",
    "output layer in the temporal unrolling scheme - such as $\\mathbf{o}$\n",
    "and $\\mathbf{h}$ at final time $t = \\tau$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f714230",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    (\\nabla_{ \\mathbf{o}^{(t)}} \\mathcal{L})_{i} &= \\frac{\\partial \\mathcal{L}}{\\partial L^{(t)}}\\frac{\\partial L^{(t)}}{\\partial o_{i}^{(t)}}, \\notag\\\\\n",
    "    \\nabla_{\\mathbf{h}^{(\\tau)}} \\mathcal{L} &= \\mathbf{V}^\\mathsf{T}\\nabla_{ \\mathbf{o}^{(\\tau)}} \\mathcal{L}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8990531",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Chain rule again\n",
    "For the following hidden nodes, we have to iterate through time, so by the chain rule,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87a970",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla_{\\mathbf{h}^{(t)}} \\mathcal{L} &= \\left(\\frac{\\partial\\mathbf{h}^{(t+1)}}{\\partial\\mathbf{h}^{(t)}}\\right)^\\mathsf{T}\\nabla_{\\mathbf{h}^{(t+1)}}\\mathcal{L} + \\left(\\frac{\\partial\\mathbf{o}^{(t)}}{\\partial\\mathbf{h}^{(t)}}\\right)^\\mathsf{T}\\nabla_{ \\mathbf{o}^{(t)}} \\mathcal{L}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c946605e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gradients of loss functions\n",
    "Similarly, the gradients of $\\mathcal{L}$ with respect to the weights and biases follow,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e049df",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:rnn_gradients3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla_{\\mathbf{c}} \\mathcal{L} &=\\sum_{t}\\left(\\frac{\\partial \\mathbf{o}^{(t)}}{\\partial \\mathbf{c}}\\right)^\\mathsf{T} \\nabla_{\\mathbf{o}^{(t)}} \\mathcal{L} \\notag\\\\\n",
    "    \\nabla_{\\mathbf{b}} \\mathcal{L} &=\\sum_{t}\\left(\\frac{\\partial \\mathbf{h}^{(t)}}{\\partial \\mathbf{b}}\\right)^\\mathsf{T}        \\nabla_{\\mathbf{h}^{(t)}} \\mathcal{L} \\notag\\\\\n",
    "    \\nabla_{\\mathbf{V}} \\mathcal{L} &=\\sum_{t}\\sum_{i}\\left(\\frac{\\partial \\mathcal{L}}{\\partial o_i^{(t)} }\\right)\\nabla_{\\mathbf{V}^{(t)}}o_i^{(t)} \\notag\\\\\n",
    "    \\nabla_{\\mathbf{W}} \\mathcal{L} &=\\sum_{t}\\sum_{i}\\left(\\frac{\\partial \\mathcal{L}}{\\partial h_i^{(t)}}\\right)\\nabla_{\\mathbf{w}^{(t)}} h_i^{(t)} \\notag\\\\\n",
    "    \\nabla_{\\mathbf{U}} \\mathcal{L} &=\\sum_{t}\\sum_{i}\\left(\\frac{\\partial \\mathcal{L}}{\\partial h_i^{(t)}}\\right)\\nabla_{\\mathbf{U}^{(t)}}h_i^{(t)}.\n",
    "    \\label{eq:rnn_gradients3} \\tag{1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a4241b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Summary of RNNs\n",
    "\n",
    "Recurrent neural networks (RNNs) have in general no probabilistic component\n",
    "in a model. With a given fixed input and target from data, the RNNs learn the intermediate\n",
    "association between various layers.\n",
    "The inputs, outputs, and internal representation (hidden states) are all\n",
    "real-valued vectors.\n",
    "\n",
    "In a  traditional NN, it is assumed that every input is\n",
    "independent of each other.  But with sequential data, the input at a given stage $t$ depends on the input from the previous stage $t-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c0a36",
   "metadata": {
    "editable": true
   },
   "source": [
    "## A typical RNN\n",
    "\n",
    "1. Weight matrices $U$, $W$ and $V$ that connect the input layer at a stage $t$ with the hidden layer $h_t$, the previous hidden layer $h_{t-1}$ with $h_t$ and the hidden layer $h_t$ connecting with the output layer at the same stage and producing an output $\\tilde{y}_t$, respectively.\n",
    "\n",
    "2. The output from the hidden layer $h_t$ is oftem modulated by a $\\tanh{}$ function $h_t=f(x_t,h_{t-1})=\\tanh{(Ux_t+Wh_{t-1}+b)}$ with $b$ a bias value\n",
    "\n",
    "3. The output from the hidden layer produces $\\tilde{y}_t=g(Vh_t+c)$ where $c$ is a new bias parameter.\n",
    "\n",
    "4. The output from the training at a given stage is in turn compared with the observation $y_t$ thorugh a chosen cost function.\n",
    "\n",
    "The function $g$ can any of the standard activation functions, that is a Sigmoid, a Softmax, a ReLU and other.\n",
    "The parameters are trained through the so-called back-propagation through time (BPTT) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af018cc",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Four effective ways to learn an RNN\n",
    "1. Long Short Term Memory Make the RNN out of little modules that are designed to remember values for a long time.\n",
    "\n",
    "2. Hessian Free Optimization: Deal with the vanishing gradients problem by using a fancy optimizer that can detect directions with a tiny gradient but even smaller curvature.\n",
    "\n",
    "3. Echo State Networks: Initialize the input a hidden and hidden-hidden and output-hidden connections very carefully so that the hidden state has a huge reservoir of weakly coupled oscillators which can be selectively driven by the input.\n",
    "\n",
    "  * ESNs only need to learn the hidden-output connections.\n",
    "\n",
    "4. Good initialization with momentum Initialize like in Echo State Networks, but then learn all of the connections using momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad5aa8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Long Short Term Memory (LSTM)\n",
    "\n",
    "LSTM uses a memory cell for \n",
    " modeling long-range dependencies and avoid vanishing gradient\n",
    " problems.\n",
    "\n",
    "1. Introduced by Hochreiter and Schmidhuber (1997) who solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).\n",
    "\n",
    "2. They designed a memory cell using logistic and linear units with multiplicative interactions.\n",
    "\n",
    "3. Information gets into the cell whenever its “write” gate is on.\n",
    "\n",
    "4. The information stays in the cell so long as its **keep** gate is on.\n",
    "\n",
    "5. Information can be read from the cell by turning on its **read** gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c8bf6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Implementing a memory cell in a neural network\n",
    "To preserve information for a long time in\n",
    "the activities of an RNN, we use a circuit\n",
    "that implements an analog memory cell.\n",
    "\n",
    "1. A linear unit that has a self-link with a weight of 1 will maintain its state.\n",
    "\n",
    "2. Information is stored in the cell by activating its write gate.\n",
    "\n",
    "3. Information is retrieved by activating the read gate.\n",
    "\n",
    "4. We can backpropagate through this circuit because logistics are have nice derivatives."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
