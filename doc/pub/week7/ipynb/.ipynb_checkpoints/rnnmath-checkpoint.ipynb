{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network (RNN), as opposed to a regular fully connected neural network (FCNN), has layers that are connected to themselves. This difference might be clearer by first looking at an FCNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/fcnn.svg\" width=\"500px\" alt=\"FCNN\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an FCNN there are no connections between nodes in a single layer. For instance, $h_1^1$ is not connected to $h_2^1$. In addition, the input and output are always of a fixed length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an RNN, however, this is no longer the case. Nodes in the hidden layers are connected to themselves, represented by the curved lines in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/rnn.svg\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the output $\\mathbf{h}$ from the hidden layer is fed back into the hidden layer. This recurrence makes RNNs useful when working with sequential data, as we can have input of variable length. This is more clear if we unfold the recurrent part of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/rnn_unfold.svg\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the nodes don't represent the same thing in the FCNN and RNN diagrams above. In the FCNN, each node gives a *scalar* value, representing a certain feature of the layer, while the nodes in the RNN represent *vectors* containing all features at that layer and time step. Thus the connections between the nodes in the RNN are in fact dense connections as seen in th FCNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/feature_zoom.svg\" width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and improvements of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest limitations of RNNs is the problem of exploding/vanishing gradients. We want to backpropagate through the RNN to tune the weights, but if the input data is a very long sequence, we have to backpropagate through very many nodes. If the weight connecting time steps is very small, the backpropagation through many time steps causes the gradient to decrease very rapidly (they *vanish*). If the weight is very large, gradients increase very rapidly (they *explode*). Thus, a simple RNN like the one we will create in this notebook will perform poorly on datasets of long sequences.\n",
    "\n",
    "There are ways to make RNNs perform better with long sequences, the perhaps most prominent being *gated RNNs* such as the *long short-term memory* (LSTM) and the *gated recurrent unit* (GRU). In this notebook we will only develop a simple RNN, which will have its limitations on what data we can look at and how good our results will be, but feel free to look up LSTMs, GRUs, and other methods of improving the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The mathematics of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what RNNs are, and why they are useful, let's get into some of the math that builds up the network. We will start by looking at the architecture of the RNN and go through the notation I use in this notebook. After that, we will derive the equations needed for forward- and backpropagation through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider some sequential input $X$ with $n$ features. Note that $X$ here is an array with two axes, since it contains $n$ features at each time step in the sequence. We will denote the input at a specific time step $t$ as\n",
    "$$\\mathbf{X}^{(t)} = \\begin{bmatrix}\n",
    "X^{(t)}_1 \\\\ \\vdots \\\\ X^{(t)}_n\n",
    "\\end{bmatrix},$$\n",
    "which is then an $n$-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, consider an RNN with $L$ hidden layers, and an output layer with $m$ features. We will denote the output of the $l$'th hidden layer at time step $t$ as\n",
    "$$\\mathbf{h}_l^{(t)} = \\begin{bmatrix}\n",
    "h_{l, 1}^{(t)} \\\\ \\vdots \\\\ h_{l, n_l}^{(t)}\n",
    "\\end{bmatrix},$$\n",
    "with $n_l$ being the number of features in the $l$'th hidden layer. The output of the RNN at time step $t$ is denoted\n",
    "$$\\hat{\\vec{y}}^{(t)} = \\begin{bmatrix}\n",
    "\\hat{y}_1 \\\\ \\vdots \\\\ \\hat{y}_m\n",
    "\\end{bmatrix},$$\n",
    "where the hat is there to distinguish the RNN output $\\hat{\\vec{y}}^{(t)}$ from the target value, which is denoted $\\vec{y}^{(t)}$.\n",
    "The RNN will then look like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/large_rnn.svg\" width=\"720px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to propagate forward through the network we need some weights and biases to connect the nodes. To simplify the notation going forward, we will consider the input layer to be the *zeroth layer*, and the output layer to be the *$L+1$'th layer*. We need each node to propagate to the node at the next layer (keeping the time step constant), and the next time step (keeping the layer constant), except for the input and output layers which do not connect to each other (as illustrated in the diagram above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $W^{l,l+1}$ be the weight matrix and $\\vec{b}^{l,l+1}$ the bias vector, both connecting nodes at the $l$'th layer to the $l+1$'th layer, keeping the time step constant. Next, let $W^{ll}$ be the weight matrix and $\\vec{b}^{ll}$ the bias vector, both connecting nodes at subsequent time steps in the same layer. Also, let $\\sigma_l$ be the activation function in the $l$'th layer. Lastly, define the weighted sum $\\vec{z}_l^{(t)}$ at layer $l$ and time step $t$ such that the output of the node is the activation of that weighted sum, that is, such that $\\vec{h}_l^{(t)} = \\sigma_l (\\vec{z}_l^{(t)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these definitions the output from the first hidden layer at the first time step is then\n",
    "$$ \\vec{h}_1^{(1)} = \\sigma_1 \\left( \\vec{z}_1^{(1)} \\right), $$\n",
    "with\n",
    "$$ \\vec{z}_1^{(1)} = W^{01} \\vec{X}^{(1)} + \\vec{b}^{01}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At later time steps we will also need to consider the contribution from the previous time step. Hence for $t \\geq 2$ we will define\n",
    "$$\\left( \\vec{z}_1^{(t)} \\right)_\\text{layer} = W^{01} X^{(t)} + \\vec{b}^{01}$$\n",
    "$$\\left( \\vec{z}_1^{(t)} \\right)_\\text{time} = W^{11} \\vec{h}_1^{(t-1)} + \\vec{b}^{11},$$\n",
    "such that $\\left( \\vec{z}_1^{(t)} \\right)_\\text{layer}$ is the contribution from the previous layer, and $\\left( \\vec{z}_1^{(t)} \\right)_\\text{time}$ is the contribution from the previous time step. We then have\n",
    "$$\\vec{z}_1^{(t)} = \\left( \\vec{z}_1^{(t)} \\right)_\\text{layer} + \\left( \\vec{z}_1^{(t)} \\right)_\\text{time},$$\n",
    "and\n",
    "$$\\vec{h}_1^{(t)} = \\sigma_1 \\left( \\vec{z}_1^{(t)} \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression is exactly the same for any hidden node, but for $l \\geq 2$ we substitute $\\vec{X}^{(t)}$ with $\\vec{h}_{l-1}^{(t)}$. Thus for the $l$'th layer and $t$'th time step we have\n",
    "$$ \\left( \\vec{z}_l^{(t)} \\right)_{layer} = W^{l-1,l} \\vec{h}_{l-1}^{(t)} + \\vec{b}^{l-1,l} $$\n",
    "and\n",
    "$$ \\left( \\vec{z}_l^{(t)} \\right)_{time} = W^{ll} \\vec{h}_{l}^{(t-1)} + \\vec{b}^{ll}, $$\n",
    "that combine to give\n",
    "$$ \\vec{z}_l^{(t)} = \\left( \\vec{z}_l^{(t)} \\right)_{layer} + \\left( \\vec{z}_l^{(t)} \\right)_{time}, $$\n",
    "which in turn results in\n",
    "$$ \\vec{h}_l^{(t)} = \\sigma_l \\left( \\vec{z}_l^{(t)} \\right). $$\n",
    "This is also valid at the first time step by setting $\\left( \\vec{z}_l^{(1)} \\right)_\\text{time} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression for the output layer is exactly the same as above, but with $\\left( \\vec{z}_l^{(t)} \\right)_\\text{time} = 0$. Thus we have\n",
    "$$ \\vec{z}_{L+1}^{(t)} = \\left( \\vec{z}_{L+1}^{(t)} \\right)_\\text{layer} = W^{L,L+1} \\vec{h}_L^{(t)} + \\vec{b}^{L,L+1} $$\n",
    "and\n",
    "$$ \\hat{\\vec{y}}^{(t)} = \\sigma_{L+1} \\left( \\vec{z}_{L+1}^{(t)} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations given for the forward propagation can seem a bit messy, so it is nice to have a more visual aid of what is going on. Here is a diagram of the complete RNN including the weights and biases relating the different nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/feed_forward.svg\" width=\"720px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a weights and biases connected to a single arbitrary node. The green arrows represent input to the node, and the red arrows represent the output from the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/feed_forward_node.svg\" width=300px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the connections resulting in $\\vec{h}_l^{(t)}$ in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/activation.svg\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation through time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation in an RNN works by comparing the output of the network to some target output (just as in the regular neural network), and propagating backwards through both the layers and the *time sequence*. It is therefore commonly referred to as *backpropagation through time* (BPTT). We will now derive the necessary equations to perform BPTT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that we have propagated forward through the network, and have produced some output $\\hat{\\vec{y}}^{(t)}$. We want to compare this with some target output value $\\vec{y}^{(t)}$, and will do so through a cost function $C \\left(\\hat{\\vec{y}}, \\vec{y} \\right)$. We will denote the cost at a specific time step $t$ by $C^{(t)} = C^{(t)} \\left(\\hat{\\vec{y}}^{(t)}, \\vec{y}^{(t)} \\right)$, and the overall cost of the network as $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cost function at each time step, we want to compute the gradient with respect to each weight and bias, that is, we want to compute\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial W^{l_1 l_2}} \\; \\text{ and } \\; \\frac{\\partial C}{\\partial \\vec{b}^{l_1 l_2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do this one layer at a time, starting at the output layer, and propagating backwards through time in each layer. We assume that we know the gradient of the cost function with respect to the output $\\frac{\\partial C^{(t)}}{\\partial \\hat{\\vec{y}}^{(t)}}$, and start by finding the gradient with respect to the output weights and biases $W^{L,L+1}$ and $\\vec{b}^{L,L+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation through the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to find the gradient with respect to $\\vec{z}_{L+1}^{(t)}$. The derivative of $C$ with respect to some element $z_{L+1, i}^{(t)}$ of the weighted sum is given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z_{L+1,i}^{(t)}} &= \\frac{\\partial C^{(t)}}{\\partial z_{L+1,i}^{(t)}}\n",
    "\\\\[4ex]\n",
    "&= \\sum_{j=1}^m \\frac{\\partial C^{(t)}}{\\partial \\hat{y}_j^{(t)}}  \\frac{\\partial \\hat{y}_j^{(t)}}{\\partial z_{L+1,i}^{(t)}}\n",
    "\\\\[4ex]\n",
    "&= \\sum_{j=1}^m \\frac{\\partial C^{(t)}}{\\partial \\hat{y}_j^{(t)}}  \\sigma_{L+1}^\\prime \\left( z_{L+1,i}^{(t)} \\right) \\delta_{ij}\n",
    "\\\\[4ex]\n",
    "&= \\frac{\\partial C^{(t)}}{\\partial \\hat{y}_i^{(t)}}  \\sigma_{L+1}^\\prime \\left( z_{L+1,i}^{(t)} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\delta_{ij}$ is the Kronecker delta\n",
    "$\\delta_{ij} = \\begin{cases}\n",
    "0, & i \\neq j\\\\\n",
    "1, & i = j\n",
    "\\end{cases}$, and $\\sigma_{L+1}^\\prime$ denotes the derivative of the activation function, which we will assume to be known.\n",
    "we can write this expression more compactly in vector form as\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial \\vec{z}_{L+1}^{(t)}} = \\frac{\\partial C^{(t)}}{\\partial \\hat{\\vec{y}}^{(t)}} \\odot \\sigma_{L+1}^\\prime \\left( \\vec{z}_{L+1}^{(t)} \\right),\n",
    "$$\n",
    "where $\\odot$ denotes the *Hadamard product*, an elementwise multiplication of two vectors/matrices of same size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<u>**Note:**</u> Sometimes the derivatives are real numbers like $\\frac{\\partial C^{(t)}}{\\partial z_{L+1,i}^{(t)}}$, sometimes they are vectors such as $\\frac{\\partial C^{(t)}}{\\partial \\vec{z}_{L+1}^{(t)}}$, and sometimes they are matrices. I have not included any explicit notation to explain when they are what, but will assume that this is understood implicitly. A general rule would be to look at whether the expression contains indices like $i,j,k,\\ldots$ or not.\n",
    "\n",
    "<u>**Another note:**</u> There are a lot of indices to keep track of, so to make the notation simpler to follow I will try to follow these rules consistently:\n",
    "- $l$ = layer index (with $L$ being the final hidden layer). If I need several layer indices I will use $l_1,l_2,\\ldots$.\n",
    "- $(t)$ = time step index.\n",
    "- $i,j,k$ = vector/matrix elements.\n",
    "- $n$ = number of input features (length of $\\vec{X}$).\n",
    "- $m$ = number of output features (length of $\\hat{\\vec{y}}$).\n",
    "- $n_1,n_2,\\ldots$ = number of features in hidden layer number $1,2,\\ldots$.\n",
    "\n",
    "<u>**Third note:**</u> I will not always write the upper bound of summations explicitly, but will assume that this is understood implicitly. For instance, $\\sum_j W^{l-1,l}_{ij} h_{l-1,j}$ should be understood to mean $\\sum_{j=1}^{n_{l-1}} W^{l-1,l}_{ij} h_{l-1,j}$, such that it sums over all elements of $\\vec{h}_{l-1}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative with respect to the weighted sum will be used a lot during backpropagation, so we will give it its own notation\n",
    "$$ \\vec{\\delta}_{L+1}^{(t)} \\equiv \\frac{\\partial C^{(t)}}{\\partial \\vec{z}_{L+1}^{(t)}} = \\frac{\\partial C^{(t)}}{\\partial \\hat{\\vec{y}}^{(t)}} \\odot \\sigma_{L+1}^\\prime \\left( \\vec{z}_{L+1}^{(t)} \\right).$$\n",
    "$\\delta_{L+1}^{(t)}$ has one index downstairs (denoting layer), and one index upstairs in parentheses (denoting time step), so don't mix it up with the Kronecker delta $\\delta_{ij}$, which I will consistently write with two indices downstairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the delta we can find the cost gradient with respect to the output bias.\n",
    "Note that the same weights and biases occur several times in the RNN, so we have to sum over each contribution. The cost gradients with respect to the weights and biases in layer $l$ are denoted $\\frac{\\partial C}{\\partial W^{l-1,l}}$, $\\frac{\\partial C}{\\partial W^{ll}}$, $\\frac{\\partial C}{\\partial \\vec{b}^{l-1,l}}$ and $\\frac{\\partial C}{\\partial \\vec{b}^{ll}}$, and we will denote the contribution at time step $t$ as $\\left(\\frac{\\partial C}{\\partial W^{l-1,l}} \\right)^{(t)}$, $\\left( \\frac{\\partial C}{\\partial W^{ll}} \\right)^{(t)}$, $\\left( \\frac{\\partial C}{\\partial \\vec{b}^{l-1,l}} \\right)^{(t)}$ and $\\left( \\frac{\\partial C}{\\partial \\vec{b}^{ll}} \\right)^{(t)}$ such that $\\frac{\\partial C}{\\partial W^{l-1,l}} = \\sum_t \\left( \\frac{\\partial C}{\\partial W^{l-1,l}}\\right)^{(t)}$ and so on.\n",
    "Using this notation, the gradient with respect to the output bias becomes\n",
    "\n",
    "\\begin{align*}\n",
    "\\left( \\frac{\\partial C}{\\partial b^{L,L+1}_i} \\right)^{(t)} &= \\sum_{j=1}^m \\frac{\\partial C}{\\partial z_{L+1,j}^{(t)}} \\frac{\\partial z_{L+1,j}^{(t)}}{\\partial b^{L,L+1}_i}\n",
    "\\\\[4ex]\n",
    "&= \\sum_{j=1}^m \\frac{\\partial C}{\\partial z_{L+1,j}^{(t)}} \\frac{\\partial}{\\partial b^{L,L+1}_i} \\left( \\sum_k W^{L,L+1}_{jk} h_{L,k}^{(t)} + b^{L,L+1}_j \\right)\n",
    "\\\\[4ex]\n",
    "&= \\sum_{j=1}^m \\frac{\\partial C}{\\partial z_{L+1,j}^{(t)}} \\delta_{ij}\n",
    "\\\\[4ex]\n",
    "&= \\frac{\\partial C}{\\partial z_{L+1,i}^{(t)}}\n",
    "\\\\[4ex]\n",
    "&= \\delta_{L+1,i}^{(t)}.\n",
    "\\end{align*}\n",
    "\n",
    "Thus on vector form we have\n",
    "$$ \\left( \\frac{\\partial C}{\\partial \\vec{b}^{L,L+1}} \\right)^{(t)} = \\vec{\\delta}_{L+1}^{(t)},$$\n",
    "and finally\n",
    "$$\\frac{\\partial C}{\\partial \\vec{b}^{L,L+1}} = \\sum_t \\left( \\frac{\\partial C}{\\partial \\vec{b}^{L,L+1}} \\right)^{(t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the gradient with respect to the output weights\n",
    "\n",
    "\\begin{align*}\n",
    "\\left( \\frac{\\partial C}{W^{L,L+1}_{ij}} \\right)^{(t)} &= \\sum_{k_1=1}^m \\frac{\\partial C}{\\partial z_{L+1,k_1}^{(t)}} \\frac{\\partial z_{L+1,k_1}^{(t)}}{\\partial W^{L,L+1}_{ij}}\n",
    "\\\\[4ex]\n",
    "&= \\sum_{k_1=1}^m \\delta_{L+1,k_1}^{(t)} \\frac{\\partial}{\\partial W^{L,L+1}_{ij}}\n",
    "\\left( \\sum_{k_2} W^{L,L+1}_{k_1 k_2} h_{L,k_2}^{(t)} + b^{L,L+1}_{k_1} \\right)\n",
    "\\\\[4ex]\n",
    "&= \\sum_{k_1=1}^m \\delta_{L+1,k_1}^{(t)} \\sum_{k_2} h_{L,k_2}^{(t)} \\delta_{i k_1} \\delta_{j k_2}\n",
    "\\\\[4ex]\n",
    "&= \\delta_{L+1,i}^{(t)} h_{L,j}^{(t)}\n",
    "\\\\[4ex]\n",
    "&= \\left[ \\vec{\\delta}_{L+1}^{(t)} \\left(\\vec{h}_{L}^{(t)}\\right)^T \\right]_{ij}.\n",
    "\\end{align*}\n",
    "\n",
    "Thus on vector form we have\n",
    "\n",
    "$$ \\left( \\frac{\\partial C}{W^{L,L+1}} \\right)^{(t)} = \\vec{\\delta}_{L+1}^{(t)} \\left(\\vec{h}_{L}^{(t)}\\right)^T, $$\n",
    "and\n",
    "$$\\frac{\\partial C}{W^{L,L+1}} = \\sum_t \\left( \\frac{\\partial C}{W^{L,L+1}} \\right)^{(t)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we here have an outer product between two vectors, which results in a matrix:\n",
    "\n",
    "$$\n",
    "\\vec{\\delta}_{L+1}^{(t)} \\left(\\vec{h}_{L}^{(t)}\\right)^T\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\delta_{L+1,1}^{(t)} \\\\ \\vdots \\\\ \\delta_{L+1,m}^{(t)}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "h_{L,1}^{(t)} & \\cdots & h_{L,n_L}^{(t)}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\delta_{L+1,1}^{(t)} h_{L,1}^{(t)} & \\cdots & \\delta_{L+1,1}^{(t)} h_{L,n_L}^{(t)}\n",
    "\\\\\n",
    "\\vdots & \\ddots & \\vdots\n",
    "\\\\\n",
    "\\delta_{L+1,m}^{(t)} h_{L,1}^{(t)} & \\cdots & \\delta_{L+1,m}^{(t)} h_{L,n_L}^{(t)}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to compute the gradient with respect to the output from the previous layer $\\frac{\\partial C}{\\partial \\vec{h}_L^{(t)}}$, in order to continue backpropagating through previous layers. We find this in much the same way as we found the other gradients above.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial h_{L,i}^{(t)}} &= \\sum_j \\frac{\\partial C}{z_{L+1,j}^{(t)}} \\frac{\\partial z_{L+1,j}^{(t)}}{\\partial h_{L,i}^{(t)}}\n",
    "\\\\[4ex]\n",
    "&= \\sum_j \\delta_{L+1,j}^{(t)} \\frac{\\partial}{\\partial h_{L,i}^{(t)}} \\left( \\sum_k W^{L,L+1}_{jk} h_{L,k}^{(t)} + b_j^{L,L+1} \\right)\n",
    "\\\\[4ex]\n",
    "&= \\sum_j \\delta_{L+1,j}^{(t)} \\sum_k W^{L,L+1}_{jk} \\delta_{ik}\n",
    "\\\\[4ex]\n",
    "&= \\sum_j \\delta_{L+1,j}^{(t)} W^{L,L+1}_{ji}\n",
    "\\\\[4ex]\n",
    "&= \\sum_j \\left[ \\left( W^{L,L+1} \\right)^T \\right]_{ij} \\delta_{L+1,j}^{(t)}\n",
    "\\\\[4ex]\n",
    "&= \\left[ \\left(W^{L,L+1} \\right)^T \\vec{\\delta}_{L+1}^{(t)} \\right]_i\n",
    "\\end{align*}\n",
    "\n",
    "And thus on vector form we have\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial \\vec{h}_L^{(t)}} = \\left( W^{L,L+1} \\right)^T \\vec{\\delta}_{L+1}^{(t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram showing the backpropagation through the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/backprop_output.svg\" width=720px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation through arbitrary node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider some arbitrary node in the RNN with output $\\vec{h}_l^{(t)}$. Assume you know the total gradient of the cost with respect to this output from the two suceeding nodes\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial \\vec{h}_l^{(t)}} = \\left( \\frac{\\partial C}{\\partial \\vec{h}_l^{(t)}} \\right)_\\text{layer} + \\left( \\frac{\\partial C}{\\partial \\vec{h}_l^{(t)}} \\right)_\\text{time}.\n",
    "$$\n",
    "We now want to compute the gradients with respect to the weights and biases connecting the two previous nodes to this node, so that we can update these weights and biases when training the network, as well as the gradient with respect to the two previous nodes, so that we can continue backpropagation through the other nodes. The situation is illustrated in the diagram below. The blue arrows show the input gradient from the succeeding nodes, and the red arrows show the gradients we want to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/backprop_node.svg\" width=720px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The necessary gradients are derived in the same way as for the output layer, so I will simply state the results here. We get the following set of equations for backpropagating through a general node in the RNN.\n",
    "\\begin{align}\n",
    "\\delta_l^{(t)} &= \\frac{\\partial C}{\\partial \\vec{h}_l^{(t)}} \\odot \\sigma_l^\\prime \\left(\\vec{z}_l^{(t)} \\right)\n",
    "\\\\[4ex]\n",
    "\\left( \\frac{\\partial C}{\\partial \\vec{b}^{l-1,l}} \\right)^{(t)} = \\left( \\frac{\\partial C}{\\partial \\vec{b}^{ll}} \\right)^{(t)} &= \\delta_l^{(t)}\n",
    "\\\\[4ex]\n",
    "\\left( \\frac{\\partial C}{\\partial W^{l-1,l}} \\right)^{(t)} &= \\delta_l^{(t)} \\left( \\vec{h}_{l-1}^{(t)} \\right)^T\n",
    "\\\\[4ex]\n",
    "\\left( \\frac{\\partial C}{\\partial W^{ll}} \\right)^{(t)} &= \\delta_l^{(t)} \\left( \\vec{h}_l^{(t-1)} \\right)^T\n",
    "\\\\[4ex]\n",
    "\\frac{\\partial C}{\\partial \\vec{h}_{l-1}^{(t)}} &= \\left[ \\left( W^{l-1,l} \\right)^{(t)} \\right]^T \\delta_l^{(t)}\n",
    "\\\\[4ex]\n",
    "\\frac{\\partial C}{\\partial \\vec{h}_{l}^{(t-1)}} &= \\left[ \\left( W^{ll} \\right)^{(t-1)} \\right]^T \\delta_l^{(t)},\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial \\vec{b}^{l-1,l}} &= \\sum_t \\left( \\frac{\\partial C}{\\partial \\vec{b}^{l-1,l}} \\right)^{(t)}\n",
    "\\\\[4ex]\n",
    "\\frac{\\partial C}{\\partial \\vec{b}^{ll}} &= \\sum_t \\left( \\frac{\\partial C}{\\partial \\vec{b}^{ll}} \\right)^{(t)}\n",
    "\\\\[4ex]\n",
    "\\frac{\\partial C}{\\partial W^{l-1,l}} &= \\sum_t \\left( \\frac{\\partial C}{\\partial W^{l-1,l}} \\right)^{(t)}\n",
    "\\\\[4ex]\n",
    "\\frac{\\partial C}{\\partial W^{ll}} &= \\sum_t \\left( \\frac{\\partial C}{\\partial W^{ll}} \\right)^{(t)}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this method we can start with the nodes in the output layer, and propagate backwards. The necessary input to one node is the output from backpropagating through the previous node. Thus we can use the equations above recursively, layer by layer, to backpropagate through the entire network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
