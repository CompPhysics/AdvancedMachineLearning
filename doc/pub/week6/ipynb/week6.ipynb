{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baab1c34",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week6.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Advanced machine learning and data analysis for the physical sciences -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e75418e",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Advanced machine learning and data analysis for the physical sciences\n",
    "**Morten Hjorth-Jensen**, Department of Physics and Center for Computing in Science Education, University of Oslo, Norway\n",
    "\n",
    "Date: **February 26, 2026**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba57fd6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Plans for the week February 23-27, 2026\n",
    "\n",
    "1. Intro to and mathematics of  Recurrent Neural Networks (RNNs)\n",
    "<!-- o More material will be added -->\n",
    "<!-- o \"Video of lecture\":\"\" -->\n",
    "<!-- o [Whiteboard notes](https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/HandwrittenNotes/2026/Lectureweek6.pdf) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb51bb",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reading recommendations\n",
    "\n",
    "1. For RNNs, see Goodfellow et al chapter 10, see <https://www.deeplearningbook.org/contents/rnn.html>.\n",
    "\n",
    "2. Reading suggestions for implementation of RNNs in PyTorch: see Rashcka et al.'s chapter 15 and GitHub site at <https://github.com/rasbt/machine-learning-book/tree/main/ch15>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6da87",
   "metadata": {
    "editable": true
   },
   "source": [
    "## TensorFlow examples\n",
    "For TensorFlow (using Keras) implementations, we recommend\n",
    "1. David Foster, Generative Deep Learning with TensorFlow, see chapter 5 at <https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html>\n",
    "\n",
    "2. Joseph Babcock and Raghav Bali Generative AI with Python and their GitHub link, chapters 2 and  3 at <https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a174141",
   "metadata": {
    "editable": true
   },
   "source": [
    "## From NNs and CNNs to recurrent neural networks (RNNs)\n",
    "\n",
    "There are limitation of NNs, one of which being that FFNNs are not\n",
    "designed to handle sequential data (data for which the order matters)\n",
    "effectively because they lack the capabilities of storing information\n",
    "about previous inputs; each input is being treated indepen-\n",
    "dently. This is a limitation when dealing with sequential data where\n",
    "past information can be vital to correctly process current and future\n",
    "inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2ab2a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## What is a recurrent NN?\n",
    "\n",
    "A recurrent neural network (RNN), as opposed to a regular fully\n",
    "connected neural network (FCNN) or just neural network (NN), has\n",
    "layers that are connected to themselves.\n",
    "\n",
    "In an FCNN there are no connections between nodes in a single\n",
    "layer. For instance, $(h_1^1$ is not connected to $(h_2^1$. In\n",
    "addition, the input and output are always of a fixed length.\n",
    "\n",
    "In an RNN, however, this is no longer the case. Nodes in the hidden\n",
    "layers are connected to themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8008e493",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Why RNNs?\n",
    "\n",
    "Recurrent neural networks work very well when working with\n",
    "sequential data, that is data where the order matters. In a regular\n",
    "fully connected network, the order of input doesn't really matter.\n",
    "\n",
    "Another property of  RNNs is that they can handle variable input\n",
    "and output. Consider again the simplified breast cancer dataset. If you\n",
    "have trained a regular FCNN on the dataset with the two features, it\n",
    "makes no sense to suddenly add a third feature. The network would not\n",
    "know what to do with it, and would reject such inputs with three\n",
    "features (or any other number of features that isn't two, for that\n",
    "matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9337470",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Feedback connections\n",
    "\n",
    "In contrast to NNs, recurrent networks introduce feedback\n",
    "connections, meaning the information is allowed to be carried to\n",
    "subsequent nodes across different time steps. These cyclic or feedback\n",
    "connections have the objective of providing the network with some kind\n",
    "of memory, making RNNs particularly suited for time- series data,\n",
    "natural language processing, speech recognition, and several other\n",
    "problems for which the order of the data is crucial.  The RNN\n",
    "architectures vary greatly in how they manage information flow and\n",
    "memory in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38215c78",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Vanishing gradients\n",
    "\n",
    "Different architectures often aim at improving\n",
    "some sub-optimal characteristics of the network. The simplest form of\n",
    "recurrent network, commonly called simple or vanilla RNN, for example,\n",
    "is known to suffer from the problem of vanishing gradients. This\n",
    "problem arises due to the nature of backpropagation in time. Gradients\n",
    "of the cost/loss function may get exponentially small (or large) if\n",
    "there are many layers in the network, which is the case of RNN when\n",
    "the sequence gets long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff03ce5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Recurrent neural networks (RNNs): Overarching view\n",
    "\n",
    "Till now our focus has been, including convolutional neural networks\n",
    "as well, on feedforward neural networks. The output or the activations\n",
    "flow only in one direction, from the input layer to the output layer.\n",
    "\n",
    "A recurrent neural network (RNN) looks very much like a feedforward\n",
    "neural network, except that it also has connections pointing\n",
    "backward. \n",
    "\n",
    "RNNs are used to analyze time series data such as stock prices, and\n",
    "tell you when to buy or sell. In autonomous driving systems, they can\n",
    "anticipate car trajectories and help avoid accidents. More generally,\n",
    "they can work on sequences of arbitrary lengths, rather than on\n",
    "fixed-sized inputs like all the nets we have discussed so far. For\n",
    "example, they can take sentences, documents, or audio samples as\n",
    "input, making them extremely useful for natural language processing\n",
    "systems such as automatic translation and speech-to-text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27164f68",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Sequential data only?\n",
    "\n",
    "An important issue is that in many deep learning methods we assume\n",
    "that the input and output data can be treated as independent and\n",
    "identically distributed, normally abbreviated to **iid**.\n",
    "This means that the data we use can be seen as mutually independent.\n",
    "\n",
    "This is however not the case for most data sets used in RNNs since we\n",
    "are dealing with sequences of data with strong inter-dependencies.\n",
    "This applies in particular to time series, which are sequential by\n",
    "contruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80cb127",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Differential equations\n",
    "\n",
    "As an example, the solutions of ordinary differential equations can be\n",
    "represented as a time series, similarly, how stock prices evolve as\n",
    "function of time is another example of a typical time series, or voice\n",
    "records and many other examples.\n",
    "\n",
    "Not all sequential data may however have a time stamp, texts being a\n",
    "typical example thereof, or DNA sequences.\n",
    "\n",
    "The main focus here is on data that can be structured either as time\n",
    "series or as ordered series of data.  We will not focus on for example\n",
    "natural language processing or similar data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f72b8b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## A simple regression example using Pytorch\n",
    "\n",
    "The structure of the code here is as follows\n",
    "1. Generate a sine function  and splits it into training and validation sets\n",
    "\n",
    "2. Create a custom data set for sequence generation\n",
    "\n",
    "3. Define an RNN model with one RNN layer and a final plain linear layer\n",
    "\n",
    "4. Train the model using the mean-squared error as cost function and the Adam optimizer\n",
    "\n",
    "5. Generate predictions using recursive forecasting\n",
    "\n",
    "6. Plot the results and training/validation loss curves\n",
    "\n",
    "The model takes sequences of previous values to predict the next\n",
    "value of the sine function. The recursive prediction uses the model's own\n",
    "predictions to generate future values, showing how well it maintains\n",
    "the sine wave pattern over time.\n",
    "\n",
    "The final plots show the the predicted values vs. the actual sine wave for the validation period\n",
    "and the training and validation cost function curves to monitor for overfitting.\n",
    "\n",
    "Here we create a dataset for the sine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18158cae",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Sine wave with noise\n",
    "t = linspace(0, 10π, 1000 points)\n",
    "y = sin(t) + noise\n",
    "\n",
    "# Creates sequences:\n",
    "Input: 50 time steps → Output: 1 future step\n",
    "Total sequences: 949\n",
    "Train: 711 (75%)\n",
    "Test: 238 (25%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe2cd3",
   "metadata": {
    "editable": true
   },
   "source": [
    "The code we have implemented is given here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b019175c",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\"\"\"\n",
    "Recurrent Neural Network for Learning Oscillatory Functions\n",
    "Using PyTorch\n",
    "\n",
    "Features:\n",
    "- Simple RNN (we add also LSTM, and GRU implementations, their properties will be discussed later)\n",
    "- Training on sine/cosine functions\n",
    "- 70-80% train/test split\n",
    "- Comprehensive visualization\n",
    "- Quality metrics and analysis\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART I: DATA GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_oscillatory_data(func_type='sine', n_points=1000, noise_level=0.0):\n",
    "    \"\"\"\n",
    "    Generate oscillatory time series data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    func_type : str\n",
    "        'sine', 'cosine', 'combined', or 'damped'\n",
    "    n_points : int\n",
    "        Number of time points\n",
    "    noise_level : float\n",
    "        Standard deviation of Gaussian noise\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    t : ndarray\n",
    "        Time values\n",
    "    y : ndarray\n",
    "        Function values\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, 10 * np.pi, n_points)\n",
    "    \n",
    "    if func_type == 'sine':\n",
    "        y = np.sin(t)\n",
    "    elif func_type == 'cosine':\n",
    "        y = np.cos(t)\n",
    "    elif func_type == 'combined':\n",
    "        # Sum of two frequencies\n",
    "        y = np.sin(t) + 0.5 * np.sin(2 * t)\n",
    "    elif func_type == 'damped':\n",
    "        # Damped oscillation\n",
    "        y = np.exp(-t / 10) * np.sin(t)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown function type: {func_type}\")\n",
    "    \n",
    "    # Add noise\n",
    "    if noise_level > 0:\n",
    "        y += np.random.normal(0, noise_level, size=y.shape)\n",
    "    \n",
    "    return t, y\n",
    "\n",
    "def create_sequences(data, seq_length, pred_length=1):\n",
    "    \"\"\"\n",
    "    Create input-output sequences for RNN training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : ndarray\n",
    "        Time series data\n",
    "    seq_length : int\n",
    "        Length of input sequence\n",
    "    pred_length : int\n",
    "        Length of prediction (future steps)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray (n_sequences, seq_length)\n",
    "        Input sequences\n",
    "    y : ndarray (n_sequences, pred_length)\n",
    "        Target sequences\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - seq_length - pred_length + 1):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length:i + seq_length + pred_length])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Generate data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING OSCILLATORY DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "t, y = generate_oscillatory_data(func_type='sine', n_points=1000, noise_level=0.05)\n",
    "\n",
    "print(f\"\\nData generated:\")\n",
    "print(f\"  Function: sine wave\")\n",
    "print(f\"  Points: {len(t)}\")\n",
    "print(f\"  Time range: [{t[0]:.2f}, {t[-1]:.2f}]\")\n",
    "print(f\"  Value range: [{y.min():.2f}, {y.max():.2f}]\")\n",
    "\n",
    "# Create sequences\n",
    "seq_length = 50  # Use 50 past points to predict\n",
    "pred_length = 1  # Predict 1 point ahead\n",
    "\n",
    "X, y_seq = create_sequences(y, seq_length, pred_length)\n",
    "\n",
    "print(f\"\\nSequences created:\")\n",
    "print(f\"  Input sequence length: {seq_length}\")\n",
    "print(f\"  Prediction length: {pred_length}\")\n",
    "print(f\"  Number of sequences: {len(X)}\")\n",
    "print(f\"  Input shape: {X.shape}\")\n",
    "print(f\"  Target shape: {y_seq.shape}\")\n",
    "\n",
    "# Train/test split (75% train, 25% test)\n",
    "train_size = int(0.75 * len(X))\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y_seq[:train_size], y_seq[train_size:]\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Training samples: {len(X_train)} ({100*len(X_train)/len(X):.1f}%)\")\n",
    "print(f\"  Testing samples: {len(X_test)} ({100*len(X_test)/len(X):.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART II: PYTORCH DATASET\n",
    "# ============================================================================\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for time series.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        # Convert to PyTorch tensors\n",
    "        self.X = torch.FloatTensor(X).unsqueeze(-1)  # Add feature dimension\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART III: RNN MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    \"\"\"Simple RNN model.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Take the last time step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"LSTM model.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    \"\"\"GRU model.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.gru(x, h0)\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# ============================================================================\n",
    "# PART IV: TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs=100, lr=0.001, device='cpu'):\n",
    "    \"\"\"Complete training loop.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    print(f\"\\nTraining {model.__class__.__name__}...\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Learning rate: {lr}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss = evaluate(model, test_loader, criterion, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"  Epoch {epoch+1:3d}/{epochs}: Train Loss = {train_loss:.6f}, Test Loss = {test_loss:.6f}\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nTraining complete in {training_time:.2f} seconds\")\n",
    "    print(f\"Final Train Loss: {train_losses[-1]:.6f}\")\n",
    "    print(f\"Final Test Loss: {test_losses[-1]:.6f}\")\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "# ============================================================================\n",
    "# PART V: TRAIN MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Model hyperparameters\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Train Simple RNN\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"SIMPLE RNN\")\n",
    "print(\"-\"*70)\n",
    "rnn_model = SimpleRNN(input_size=1, hidden_size=hidden_size, num_layers=num_layers, output_size=1)\n",
    "rnn_train_losses, rnn_test_losses = train_model(\n",
    "    rnn_model, train_loader, test_loader, epochs=epochs, lr=learning_rate, device=device\n",
    ")\n",
    "\n",
    "# Train LSTM\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"LSTM\")\n",
    "print(\"-\"*70)\n",
    "lstm_model = LSTMModel(input_size=1, hidden_size=hidden_size, num_layers=num_layers, output_size=1)\n",
    "lstm_train_losses, lstm_test_losses = train_model(\n",
    "    lstm_model, train_loader, test_loader, epochs=epochs, lr=learning_rate, device=device\n",
    ")\n",
    "\n",
    "# Train GRU\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"GRU\")\n",
    "print(\"-\"*70)\n",
    "gru_model = GRUModel(input_size=1, hidden_size=hidden_size, num_layers=num_layers, output_size=1)\n",
    "gru_train_losses, gru_test_losses = train_model(\n",
    "    gru_model, train_loader, test_loader, epochs=epochs, lr=learning_rate, device=device\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# PART VI: GENERATE PREDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_predictions(model, X_data, device):\n",
    "    \"\"\"Generate predictions for given data.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X_data)):\n",
    "            x = torch.FloatTensor(X_data[i]).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "            pred = model(x)\n",
    "            predictions.append(pred.cpu().numpy()[0, 0])\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate predictions for all models\n",
    "rnn_preds_train = generate_predictions(rnn_model, X_train, device)\n",
    "rnn_preds_test = generate_predictions(rnn_model, X_test, device)\n",
    "\n",
    "lstm_preds_train = generate_predictions(lstm_model, X_train, device)\n",
    "lstm_preds_test = generate_predictions(lstm_model, X_test, device)\n",
    "\n",
    "gru_preds_train = generate_predictions(gru_model, X_train, device)\n",
    "gru_preds_test = generate_predictions(gru_model, X_test, device)\n",
    "\n",
    "# Calculate metrics\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # R² score\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R²': r2}\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "rnn_metrics = compute_metrics(y_test.flatten(), rnn_preds_test)\n",
    "print(f\"\\nSimple RNN:\")\n",
    "for key, val in rnn_metrics.items():\n",
    "    print(f\"  {key:6s} = {val:.6f}\")\n",
    "\n",
    "lstm_metrics = compute_metrics(y_test.flatten(), lstm_preds_test)\n",
    "print(f\"\\nLSTM:\")\n",
    "for key, val in lstm_metrics.items():\n",
    "    print(f\"  {key:6s} = {val:.6f}\")\n",
    "\n",
    "gru_metrics = compute_metrics(y_test.flatten(), gru_preds_test)\n",
    "print(f\"\\nGRU:\")\n",
    "for key, val in gru_metrics.items():\n",
    "    print(f\"  {key:6s} = {val:.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART VII: VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Original data\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "ax1.plot(t, y, 'b-', linewidth=1, alpha=0.7, label='Sine wave')\n",
    "ax1.axvline(x=t[train_size + seq_length], color='r', linestyle='--', \n",
    "            linewidth=2, label='Train/Test split')\n",
    "ax1.set_xlabel('Time', fontsize=10)\n",
    "ax1.set_ylabel('Value', fontsize=10)\n",
    "ax1.set_title('Original Time Series', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training curves - RNN\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "ax2.plot(rnn_train_losses, 'b-', linewidth=2, label='Train')\n",
    "ax2.plot(rnn_test_losses, 'r-', linewidth=2, label='Test')\n",
    "ax2.set_xlabel('Epoch', fontsize=10)\n",
    "ax2.set_ylabel('Loss (MSE)', fontsize=10)\n",
    "ax2.set_title('Simple RNN: Training Curves', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# Plot 3: Training curves - LSTM\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "ax3.plot(lstm_train_losses, 'b-', linewidth=2, label='Train')\n",
    "ax3.plot(lstm_test_losses, 'r-', linewidth=2, label='Test')\n",
    "ax3.set_xlabel('Epoch', fontsize=10)\n",
    "ax3.set_ylabel('Loss (MSE)', fontsize=10)\n",
    "ax3.set_title('LSTM: Training Curves', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "# Plot 4: Predictions - Simple RNN\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "train_indices = np.arange(seq_length, seq_length + len(rnn_preds_train))\n",
    "test_indices = np.arange(seq_length + len(rnn_preds_train), \n",
    "                         seq_length + len(rnn_preds_train) + len(rnn_preds_test))\n",
    "ax4.plot(train_indices, y_train.flatten(), 'b-', linewidth=1, alpha=0.5, label='Train True')\n",
    "ax4.plot(train_indices, rnn_preds_train, 'g-', linewidth=1, label='Train Pred')\n",
    "ax4.plot(test_indices, y_test.flatten(), 'r-', linewidth=1, alpha=0.5, label='Test True')\n",
    "ax4.plot(test_indices, rnn_preds_test, 'orange', linewidth=1, label='Test Pred')\n",
    "ax4.set_xlabel('Time Step', fontsize=10)\n",
    "ax4.set_ylabel('Value', fontsize=10)\n",
    "ax4.set_title('Simple RNN: Predictions', fontsize=12, fontweight='bold')\n",
    "ax4.legend(fontsize=8)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Predictions - LSTM\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "ax5.plot(train_indices, y_train.flatten(), 'b-', linewidth=1, alpha=0.5, label='Train True')\n",
    "ax5.plot(train_indices, lstm_preds_train, 'g-', linewidth=1, label='Train Pred')\n",
    "ax5.plot(test_indices, y_test.flatten(), 'r-', linewidth=1, alpha=0.5, label='Test True')\n",
    "ax5.plot(test_indices, lstm_preds_test, 'orange', linewidth=1, label='Test Pred')\n",
    "ax5.set_xlabel('Time Step', fontsize=10)\n",
    "ax5.set_ylabel('Value', fontsize=10)\n",
    "ax5.set_title('LSTM: Predictions', fontsize=12, fontweight='bold')\n",
    "ax5.legend(fontsize=8)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Predictions - GRU\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "ax6.plot(train_indices, y_train.flatten(), 'b-', linewidth=1, alpha=0.5, label='Train True')\n",
    "ax6.plot(train_indices, gru_preds_train, 'g-', linewidth=1, label='Train Pred')\n",
    "ax6.plot(test_indices, y_test.flatten(), 'r-', linewidth=1, alpha=0.5, label='Test True')\n",
    "ax6.plot(test_indices, gru_preds_test, 'orange', linewidth=1, label='Test Pred')\n",
    "ax6.set_xlabel('Time Step', fontsize=10)\n",
    "ax6.set_ylabel('Value', fontsize=10)\n",
    "ax6.set_title('GRU: Predictions', fontsize=12, fontweight='bold')\n",
    "ax6.legend(fontsize=8)\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 7: Test error distribution - RNN\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "rnn_errors = rnn_preds_test - y_test.flatten()\n",
    "ax7.hist(rnn_errors, bins=30, alpha=0.7, edgecolor='black')\n",
    "ax7.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "ax7.set_xlabel('Prediction Error', fontsize=10)\n",
    "ax7.set_ylabel('Frequency', fontsize=10)\n",
    "ax7.set_title(f'RNN Error (MAE={rnn_metrics[\"MAE\"]:.4f})', fontsize=12, fontweight='bold')\n",
    "ax7.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 8: Test error distribution - LSTM\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "lstm_errors = lstm_preds_test - y_test.flatten()\n",
    "ax8.hist(lstm_errors, bins=30, alpha=0.7, edgecolor='black')\n",
    "ax8.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "ax8.set_xlabel('Prediction Error', fontsize=10)\n",
    "ax8.set_ylabel('Frequency', fontsize=10)\n",
    "ax8.set_title(f'LSTM Error (MAE={lstm_metrics[\"MAE\"]:.4f})', fontsize=12, fontweight='bold')\n",
    "ax8.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 9: Model comparison\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "models = ['RNN', 'LSTM', 'GRU']\n",
    "test_losses_final = [rnn_test_losses[-1], lstm_test_losses[-1], gru_test_losses[-1]]\n",
    "r2_scores = [rnn_metrics['R²'], lstm_metrics['R²'], gru_metrics['R²']]\n",
    "\n",
    "x_pos = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax9.bar(x_pos - width/2, test_losses_final, width, label='Test Loss', alpha=0.7)\n",
    "ax9_twin = ax9.twinx()\n",
    "bars2 = ax9_twin.bar(x_pos + width/2, r2_scores, width, label='R² Score', \n",
    "                     alpha=0.7, color='orange')\n",
    "\n",
    "ax9.set_xlabel('Model', fontsize=10)\n",
    "ax9.set_ylabel('Test Loss (MSE)', fontsize=10, color='blue')\n",
    "ax9_twin.set_ylabel('R² Score', fontsize=10, color='orange')\n",
    "ax9.set_title('Model Comparison', fontsize=12, fontweight='bold')\n",
    "ax9.set_xticks(x_pos)\n",
    "ax9.set_xticklabels(models)\n",
    "ax9.tick_params(axis='y', labelcolor='blue')\n",
    "ax9_twin.tick_params(axis='y', labelcolor='orange')\n",
    "ax9.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('rnn_oscillatory_results.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n✓ Plot saved to: rnn_oscillatory_results.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Total samples: {len(X)}\")\n",
    "print(f\"  Train: {len(X_train)} (75%)\")\n",
    "print(f\"  Test: {len(X_test)} (25%)\")\n",
    "print(f\"  Sequence length: {seq_length}\")\n",
    "\n",
    "print(f\"\\nModels trained:\")\n",
    "print(f\"  Hidden size: {hidden_size}\")\n",
    "print(f\"  Num layers: {num_layers}\")\n",
    "print(f\"  Epochs: {epochs}\")\n",
    "\n",
    "print(f\"\\nPerformance (Test Set):\")\n",
    "print(f\"  {'Model':<10s} {'MSE':<12s} {'RMSE':<12s} {'MAE':<12s} {'R²':<10s}\")\n",
    "print(f\"  {'-'*58}\")\n",
    "print(f\"  {'RNN':<10s} {rnn_metrics['MSE']:<12.6f} {rnn_metrics['RMSE']:<12.6f} {rnn_metrics['MAE']:<12.6f} {rnn_metrics['R²']:<10.6f}\")\n",
    "print(f\"  {'LSTM':<10s} {lstm_metrics['MSE']:<12.6f} {lstm_metrics['RMSE']:<12.6f} {lstm_metrics['MAE']:<12.6f} {lstm_metrics['R²']:<10.6f}\")\n",
    "print(f\"  {'GRU':<10s} {gru_metrics['MSE']:<12.6f} {gru_metrics['RMSE']:<12.6f} {gru_metrics['MAE']:<12.6f} {gru_metrics['R²']:<10.6f}\")\n",
    "\n",
    "print(\"\\n✓ All models successfully trained on oscillatory functions!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d81e89",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Why RNNs in the physical sciences?\n",
    "Many data sets are **ordered** and **correlated**:\n",
    "* detector readout streams, time-of-flight, waveform traces\n",
    "\n",
    "* climate/geo time series; turbulence probes; seismology\n",
    "\n",
    "* molecular dynamics trajectories; spectroscopy scans\n",
    "\n",
    "* state estimation in control/feedback; sensor fusion\n",
    "\n",
    "We want models for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adbf1ae",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(x_{1:T}) \\quad\\text{or}\\quad p(y_{1:T}\\mid x_{1:T}) \\quad\\text{or}\\quad p(y\\mid x_{1:T}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4b57c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Sequence modeling as conditional probability\n",
    "Two canonical tasks:\n",
    "\n",
    "* **Sequence-to-sequence**: $x_{1:T}\\mapsto y_{1:T}$ (filtering, forecasting).\n",
    "\n",
    "* **Sequence-to-label**: $x_{1:T}\\mapsto y$ (classification, regime detection).\n",
    "\n",
    "A Markovian latent-state view:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a79ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "h_t = f_\\theta(h_{t-1},x_t),\\qquad y_t = g_\\theta(h_t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fddc16c",
   "metadata": {
    "editable": true
   },
   "source": [
    "RNNs provide a parameterized $f_\\theta,g_\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a537bc8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Autonomous RNN as a discrete-time dynamical system\n",
    "\n",
    "With $x_t=0$ (or constant), the update is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e6b6f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "h_t = F(h_{t-1}) := \\sigma(W_h h_{t-1}+b).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1eead7",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can analyze:\n",
    "\n",
    "* fixed points $h^\\star = F(h^\\star)$,\n",
    "\n",
    "* stability via Jacobian,\n",
    "\n",
    "* attractors / limit cycles (in high dimensions).\n",
    "\n",
    "* does the system forget initial conditions?\n",
    "\n",
    "* how sensitive is $h_t$ to old inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c09be1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs\n",
    "\n",
    "RNNs are very powerful, because they\n",
    "combine two properties:\n",
    "1. Distributed hidden state that allows them to store a lot of information about the past efficiently.\n",
    "\n",
    "2. Non-linear dynamics that allows them to update their hidden state in complicated ways.\n",
    "\n",
    "With enough neurons and time, RNNs can compute anything that can be computed by your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49292574",
   "metadata": {
    "editable": true
   },
   "source": [
    "## What kinds of behaviour can RNNs exhibit?\n",
    "\n",
    "1. They can oscillate. \n",
    "\n",
    "2. They can settle to point attractors.\n",
    "\n",
    "3. They can behave chaotically.\n",
    "\n",
    "4. RNNs could potentially learn to implement lots of small programs that each capture a nugget of knowledge and run in parallel, interacting to produce very complicated effects.\n",
    "\n",
    "But the extensive computational needs  of RNNs makes them very hard to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a796bf74",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Basic layout,  [Figures from Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch](https://sebastianraschka.com/blog/2022/ml-pytorch-book.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN1.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN1.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144a7822",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Solving differential equations with RNNs\n",
    "\n",
    "To gain some intuition on how we can use RNNs for time series, let us\n",
    "tailor the representation of the solution of a differential equation\n",
    "as a time series.\n",
    "\n",
    "Consider the famous differential equation (Newton's equation of motion for damped harmonic oscillations, scaled in terms of dimensionless time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071e96b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{d^2x}{dt^2}+\\eta\\frac{dx}{dt}+x(t)=F(t),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f506bce",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\eta$ is a constant used in scaling time into a dimensionless variable and $F(t)$ is an external force acting on the system.\n",
    "The constant $\\eta$ is a so-called damping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0f36e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Two first-order differential equations\n",
    "\n",
    "In solving the above second-order equation, it is common to rewrite it in terms of two coupled first-order equations\n",
    "with the velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e75ac98",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v(t)=\\frac{dx}{dt},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1c687",
   "metadata": {
    "editable": true
   },
   "source": [
    "and the acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f05409",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{dv}{dt}=F(t)-\\eta v(t)-x(t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e3584",
   "metadata": {
    "editable": true
   },
   "source": [
    "With the initial conditions $v_0=v(t_0)$ and $x_0=x(t_0)$ defined, we can integrate these equations and find their respective solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1b106",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Velocity only\n",
    "\n",
    "Let us focus on the velocity only. Discretizing and using the simplest\n",
    "possible approximation for the derivative, we have Euler's forward\n",
    "method for the updated velocity at a time step $i+1$ given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870bfb57",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v_{i+1}=v_i+\\Delta t \\frac{dv}{dt}_{\\vert_{v=v_i}}=v_i+\\Delta t\\left(F_i-\\eta v_i-x_i\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d26d4",
   "metadata": {
    "editable": true
   },
   "source": [
    "Defining a function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0472d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "h_i(x_i,v_i,F_i)=v_i+\\Delta t\\left(F_i-\\eta v_i-x_i\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda6a698",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0960621",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v_{i+1}=h_i(x_i,v_i,F_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df2137",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linking with RNNs\n",
    "\n",
    "The equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1ccf6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v_{i+1}=h_i(x_i,v_i,F_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba08bb",
   "metadata": {
    "editable": true
   },
   "source": [
    "can be used to train a feed-forward neural network with inputs $v_i$ and outputs $v_{i+1}$ at a time $t_i$. But we can think of this also as a recurrent neural network\n",
    "with inputs $v_i$, $x_i$ and $F_i$ at each time step $t_i$, and producing an output $v_{i+1}$.\n",
    "\n",
    "Noting that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba3cb75",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v_{i}=v_{i-1}+\\Delta t\\left(F_{i-1}-\\eta v_{i-1}-x_{i-1}\\right)=h_{i-1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6af49a",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de49f1ce",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v_{i}=h_{i-1}(x_{i-1},v_{i-1},F_{i-1}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d1300",
   "metadata": {
    "editable": true
   },
   "source": [
    "and we can rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a31e5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v_{i+1}=h_i(x_i,h_{i-1},F_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5af870",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Minor rewrite\n",
    "\n",
    "We can thus set up a recurring series which depends on the inputs $x_i$ and $F_i$ and the previous values $h_{i-1}$.\n",
    "We assume now that the inputs at each step (or time $t_i$) is given by $x_i$ only and we denote the outputs for $\\tilde{y}_i$ instead of $v_{i_1}$, we have then the compact equation for our outputs at each step $t_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b4c10",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "y_{i}=h_i(x_i,h_{i-1}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d839f7e",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can think of this as an element in a recurrent network where our\n",
    "network (our model) produces an output $y_i$ which is then compared\n",
    "with a target value through a given cost/loss function that we\n",
    "optimize. The target values at a given step $t_i$ could be the results\n",
    "of a measurement or simply the analytical results of a differential\n",
    "equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e47a06",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN2.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN2.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc79cfaf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 2\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN3.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN3.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7af0d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 3\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN4.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN4.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d4ff1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 4\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN5.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN5.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5693ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 5\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN6.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN6.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483e0bd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 6\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN7.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN7.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c04574",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 7\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN8.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN8.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b207644",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Backpropagation through time\n",
    "\n",
    "We can think of the recurrent net as a layered, feed-forward\n",
    "net with shared weights and then train the feed-forward net\n",
    "with weight constraints.\n",
    "\n",
    "We can also think of this training algorithm in the time domain:\n",
    "1. The forward pass builds up a stack of the activities of all the units at each time step.\n",
    "\n",
    "2. The backward pass peels activities off the stack to compute the error derivatives at each time step.\n",
    "\n",
    "3. After the backward pass we add together the derivatives at all the different times for each weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344f952",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The backward pass is linear\n",
    "\n",
    "1. There is a big difference between the forward and backward passes.\n",
    "\n",
    "2. In the forward pass we use squashing functions (like the logistic) to prevent the activity vectors from exploding.\n",
    "\n",
    "3. The backward pass, is completely linear. If you double the error derivatives at the final layer, all the error derivatives will double.\n",
    "\n",
    "The forward pass determines the slope of the linear function used for\n",
    "backpropagating through each neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c27a9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The problem of exploding or vanishing gradients\n",
    "* What happens to the magnitude of the gradients as we backpropagate through many layers?\n",
    "\n",
    "a. If the weights are small, the gradients shrink exponentially.\n",
    "\n",
    "b. If the weights are big the gradients grow exponentially.\n",
    "\n",
    "* Typical feed-forward neural nets can cope with these exponential effects because they only have a few hidden layers.\n",
    "\n",
    "* In an RNN trained on long sequences (e.g. 100 time steps) the gradients can easily explode or vanish.\n",
    "\n",
    "a. We can avoid this by initializing the weights very carefully.\n",
    "\n",
    "* Even with good initial weights, its very hard to detect that the current target output depends on an input from many time-steps ago.\n",
    "\n",
    "RNNs have difficulty dealing with long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b4ed9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematical setup\n",
    "\n",
    "The expression for the simplest Recurrent network resembles that of a\n",
    "regular feed-forward neural network, but now with\n",
    "the concept of temporal dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90577f7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{a}^{(t)} & = U * \\mathbf{x}^{(t)} + W * \\mathbf{h}^{(t-1)} + \\mathbf{b}, \\notag \\\\\n",
    "    \\mathbf{h}^{(t)} &= \\sigma_h(\\mathbf{a}^{(t)}), \\notag\\\\\n",
    "    \\mathbf{y}^{(t)} &= V * \\mathbf{h}^{(t)} + \\mathbf{c}, \\notag\\\\\n",
    "    \\mathbf{\\hat{y}}^{(t)} &= \\sigma_y(\\mathbf{y}^{(t)}).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6827198",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time through figures, part 1\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN9.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN9.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69485c8e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time, part 2\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN10.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN10.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0128732",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time, part 3\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN11.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN11.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e718d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time, part 4\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN12.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN12.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89f8b76",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time in equations\n",
    "\n",
    "To derive the expression of the gradients of $\\mathcal{L}$ for\n",
    "the RNN, we need to start recursively from the nodes closer to the\n",
    "output layer in the temporal unrolling scheme - such as $\\mathbf{y}$\n",
    "and $\\mathbf{h}$ at final time $t = \\tau$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57dce9d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    (\\nabla_{ \\mathbf{y}^{(t)}} \\mathcal{L})_{i} &= \\frac{\\partial \\mathcal{L}}{\\partial L^{(t)}}\\frac{\\partial L^{(t)}}{\\partial y_{i}^{(t)}}, \\notag\\\\\n",
    "    \\nabla_{\\mathbf{h}^{(\\tau)}} \\mathcal{L} &= \\mathbf{V}^\\mathsf{T}\\nabla_{ \\mathbf{y}^{(\\tau)}} \\mathcal{L}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7911a19",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Chain rule again\n",
    "For the following hidden nodes, we have to iterate through time, so by the chain rule,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdfa76d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla_{\\mathbf{h}^{(t)}} \\mathcal{L} &= \\left(\\frac{\\partial\\mathbf{h}^{(t+1)}}{\\partial\\mathbf{h}^{(t)}}\\right)^\\mathsf{T}\\nabla_{\\mathbf{h}^{(t+1)}}\\mathcal{L} + \\left(\\frac{\\partial\\mathbf{y}^{(t)}}{\\partial\\mathbf{h}^{(t)}}\\right)^\\mathsf{T}\\nabla_{ \\mathbf{y}^{(t)}} \\mathcal{L}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f405e049",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gradients of loss functions\n",
    "Similarly, the gradients of $\\mathcal{L}$ with respect to the weights and biases follow,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9255b54",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:rnn_gradients3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla_{\\mathbf{c}} \\mathcal{L} &=\\sum_{t}\\left(\\frac{\\partial \\mathbf{y}^{(t)}}{\\partial \\mathbf{c}}\\right)^\\mathsf{T} \\nabla_{\\mathbf{y}^{(t)}} \\mathcal{L} \\notag\\\\\n",
    "    \\nabla_{\\mathbf{b}} \\mathcal{L} &=\\sum_{t}\\left(\\frac{\\partial \\mathbf{h}^{(t)}}{\\partial \\mathbf{b}}\\right)^\\mathsf{T}        \\nabla_{\\mathbf{h}^{(t)}} \\mathcal{L} \\notag\\\\\n",
    "    \\nabla_{\\mathbf{V}} \\mathcal{L} &=\\sum_{t}\\sum_{i}\\left(\\frac{\\partial \\mathcal{L}}{\\partial y_i^{(t)} }\\right)\\nabla_{\\mathbf{V}^{(t)}}y_i^{(t)} \\notag\\\\\n",
    "    \\nabla_{\\mathbf{W}} \\mathcal{L} &=\\sum_{t}\\sum_{i}\\left(\\frac{\\partial \\mathcal{L}}{\\partial h_i^{(t)}}\\right)\\nabla_{\\mathbf{w}^{(t)}} h_i^{(t)} \\notag\\\\\n",
    "    \\nabla_{\\mathbf{U}} \\mathcal{L} &=\\sum_{t}\\sum_{i}\\left(\\frac{\\partial \\mathcal{L}}{\\partial h_i^{(t)}}\\right)\\nabla_{\\mathbf{U}^{(t)}}h_i^{(t)}.\n",
    "    \\label{eq:rnn_gradients3} \\tag{1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b427c4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Summary of RNNs\n",
    "\n",
    "Recurrent neural networks (RNNs) have in general no probabilistic component\n",
    "in a model. With a given fixed input and target from data, the RNNs learn the intermediate\n",
    "association between various layers.\n",
    "The inputs, outputs, and internal representation (hidden states) are all\n",
    "real-valued vectors.\n",
    "\n",
    "In a  traditional NN, it is assumed that every input is\n",
    "independent of each other.  But with sequential data, the input at a given stage $t$ depends on the input from the previous stage $t-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c1605b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Summary of a  typical simple RNN\n",
    "\n",
    "1. Weight matrices $U$, $W$ and $V$ that connect the input layer at a stage $t$ with the hidden layer $h_t$, the previous hidden layer $h_{t-1}$ with $h_t$ and the hidden layer $h_t$ connecting with the output layer at the same stage and producing an output $\\tilde{y}_t$, respectively.\n",
    "\n",
    "2. The output from the hidden layer $h_t$ is oftem modulated by a $\\tanh{}$ function $h_t=\\sigma_h(x_t,h_{t-1})=\\tanh{(Ux_t+Wh_{t-1}+b)}$ with $b$ a bias value\n",
    "\n",
    "3. The output from the hidden layer produces $\\tilde{y}_t=\\sigma_y(Vh_t+c)$ where $c$ is a new bias parameter.\n",
    "\n",
    "4. The output from the training at a given stage is in turn compared with the observation $y_t$ thorugh a chosen cost function.\n",
    "\n",
    "The function $g$ can any of the standard activation functions, that is a Sigmoid, a Softmax, a ReLU and other.\n",
    "The parameters are trained through the so-called back-propagation through time (BPTT) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86d149e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Our next example, using data from an ordinary differential equation solver\n",
    "\n",
    "Here we will use data produced by the solution of an ordinary\n",
    "differential equation solver using the Runge-Kutta to fourth order for\n",
    "solving forced oscillatory motion.\n",
    "The code we will use is listed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a2ed33b",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Where to save the figures and data files\n",
    "PROJECT_ROOT_DIR = \"Results\"\n",
    "FIGURE_ID = \"Results/FigureFiles\"\n",
    "DATA_ID = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(PROJECT_ROOT_DIR):\n",
    "    os.mkdir(PROJECT_ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(FIGURE_ID):\n",
    "    os.makedirs(FIGURE_ID)\n",
    "\n",
    "if not os.path.exists(DATA_ID):\n",
    "    os.makedirs(DATA_ID)\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(FIGURE_ID, fig_id)\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "def save_fig(fig_id):\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png')\n",
    "\n",
    "\n",
    "def SpringForce(v,x,t):\n",
    "#   note here that we have divided by mass and we return the acceleration\n",
    "    return  -2*gamma*v-x+Ftilde*cos(t*Omegatilde)\n",
    "\n",
    "\n",
    "def RK4(v,x,t,n,Force):\n",
    "    for i in range(n-1):\n",
    "# Setting up k1\n",
    "        k1x = DeltaT*v[i]\n",
    "        k1v = DeltaT*Force(v[i],x[i],t[i])\n",
    "# Setting up k2\n",
    "        vv = v[i]+k1v*0.5\n",
    "        xx = x[i]+k1x*0.5\n",
    "        k2x = DeltaT*vv\n",
    "        k2v = DeltaT*Force(vv,xx,t[i]+DeltaT*0.5)\n",
    "# Setting up k3\n",
    "        vv = v[i]+k2v*0.5\n",
    "        xx = x[i]+k2x*0.5\n",
    "        k3x = DeltaT*vv\n",
    "        k3v = DeltaT*Force(vv,xx,t[i]+DeltaT*0.5)\n",
    "# Setting up k4\n",
    "        vv = v[i]+k3v\n",
    "        xx = x[i]+k3x\n",
    "        k4x = DeltaT*vv\n",
    "        k4v = DeltaT*Force(vv,xx,t[i]+DeltaT)\n",
    "# Final result\n",
    "        x[i+1] = x[i]+(k1x+2*k2x+2*k3x+k4x)/6.\n",
    "        v[i+1] = v[i]+(k1v+2*k2v+2*k3v+k4v)/6.\n",
    "        t[i+1] = t[i] + DeltaT\n",
    "\n",
    "\n",
    "# Main part begins here\n",
    "\n",
    "DeltaT = 0.001\n",
    "#set up arrays \n",
    "tfinal = 20 # in dimensionless time\n",
    "n = ceil(tfinal/DeltaT)\n",
    "# set up arrays for t, v, and x\n",
    "t = np.zeros(n)\n",
    "v = np.zeros(n)\n",
    "x = np.zeros(n)\n",
    "# Initial conditions (can change to more than one dim)\n",
    "x0 =  1.0 \n",
    "v0 = 0.0\n",
    "x[0] = x0\n",
    "v[0] = v0\n",
    "gamma = 0.2\n",
    "Omegatilde = 0.5\n",
    "Ftilde = 1.0\n",
    "# Start integrating using Euler's method\n",
    "# Note that we define the force function as a SpringForce\n",
    "RK4(v,x,t,n,SpringForce)\n",
    "\n",
    "# Plot position as function of time    \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylabel('x[m]')\n",
    "ax.set_xlabel('t[s]')\n",
    "ax.plot(t, x)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa2bd3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Including this code in RNNs\n",
    "\n",
    "Here we show how we can use the simulated data to train a RNNs. We include LSTMs and GRUs as well, although they will be discussed in more details next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b7f8f1",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "RNN for Learning ODE Solutions\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "from math import ceil, cos\n",
    "import sys\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Force CPU to avoid GPU hanging issues\n",
    "device = torch.device('cpu')\n",
    "print(f\"Using device: {device} (CPU mode to avoid hanging)\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART I: ODE SOLVER (OPTIMIZED)\n",
    "# ============================================================================\n",
    "\n",
    "def SpringForce(v, x, t, gamma=0.2, Omega=0.5, F=1.0):\n",
    "    \"\"\"Force function for driven damped harmonic oscillator.\"\"\"\n",
    "    return -2*gamma*v - x + F*cos(t*Omega)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SOLVING ODE (REDUCED SIZE FOR SPEED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# REDUCED parameters to avoid hanging\n",
    "DeltaT = 0.002  # Larger timestep\n",
    "tfinal = 10.0   # Shorter simulation\n",
    "n = ceil(tfinal/DeltaT)\n",
    "\n",
    "print(f\"\\nODE Parameters:\")\n",
    "print(f\"  Time step: {DeltaT}\")\n",
    "print(f\"  Final time: {tfinal}\")\n",
    "print(f\"  Number of points: {n}\")\n",
    "\n",
    "# Solve ODE\n",
    "t = np.zeros(n)\n",
    "x = np.zeros(n)\n",
    "v = np.zeros(n)\n",
    "\n",
    "x[0] = 1.0\n",
    "v[0] = 0.0\n",
    "gamma = 0.2\n",
    "Omega = 0.5\n",
    "F = 1.0\n",
    "\n",
    "print(\"\\nSolving ODE with RK4...\")\n",
    "for i in range(n-1):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"  Progress: {100*i/n:.1f}%\", end='\\r')\n",
    "    \n",
    "    # RK4 step\n",
    "    k1x = DeltaT * v[i]\n",
    "    k1v = DeltaT * SpringForce(v[i], x[i], t[i], gamma, Omega, F)\n",
    "    \n",
    "    vv = v[i] + k1v*0.5\n",
    "    xx = x[i] + k1x*0.5\n",
    "    tt = t[i] + DeltaT*0.5\n",
    "    k2x = DeltaT * vv\n",
    "    k2v = DeltaT * SpringForce(vv, xx, tt, gamma, Omega, F)\n",
    "    \n",
    "    vv = v[i] + k2v*0.5\n",
    "    xx = x[i] + k2x*0.5\n",
    "    k3x = DeltaT * vv\n",
    "    k3v = DeltaT * SpringForce(vv, xx, tt, gamma, Omega, F)\n",
    "    \n",
    "    vv = v[i] + k3v\n",
    "    xx = x[i] + k3x\n",
    "    tt = t[i] + DeltaT\n",
    "    k4x = DeltaT * vv\n",
    "    k4v = DeltaT * SpringForce(vv, xx, tt, gamma, Omega, F)\n",
    "    \n",
    "    x[i+1] = x[i] + (k1x + 2*k2x + 2*k3x + k4x)/6.0\n",
    "    v[i+1] = v[i] + (k1v + 2*k2v + 2*k3v + k4v)/6.0\n",
    "    t[i+1] = t[i] + DeltaT\n",
    "\n",
    "print(f\"  Progress: 100.0% - Complete!\")\n",
    "print(f\"\\nODE solved: {len(x)} points\")\n",
    "print(f\"  Position range: [{x.min():.4f}, {x.max():.4f}]\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART II: PREPARE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPARING TRAINING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "seq_length = 50  # Shorter sequences\n",
    "X_list, y_list = [], []\n",
    "\n",
    "print(f\"\\nCreating sequences (length={seq_length})...\")\n",
    "for i in range(len(x) - seq_length - 1):\n",
    "    X_list.append(x[i:i + seq_length])\n",
    "    y_list.append(x[i + seq_length])\n",
    "\n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list).reshape(-1, 1)\n",
    "\n",
    "print(f\"  Created {len(X)} sequences\")\n",
    "\n",
    "# 75/25 split\n",
    "train_size = int(0.75 * len(X))\n",
    "X_train = X[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "print(f\"  Train: {len(X_train)} ({100*len(X_train)/len(X):.1f}%)\")\n",
    "print(f\"  Test: {len(X_test)} ({100*len(X_test)/len(X):.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART III: PYTORCH DATASET\n",
    "# ============================================================================\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X).unsqueeze(-1)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "# CRITICAL: num_workers=0 to avoid multiprocessing hanging\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nDataLoaders ready:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART IV: LSTM MODEL (SINGLE MODEL FOR SPEED)\n",
    "# ============================================================================\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_layers=2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# ============================================================================\n",
    "# PART V: TRAINING WITH PROGRESS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING LSTM MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = LSTMModel(hidden_size=64, num_layers=2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 50  # Reduced for speed\n",
    "print(f\"\\nStarting training ({epochs} epochs)...\")\n",
    "print(f\"  Hidden size: 64\")\n",
    "print(f\"  Num layers: 2\")\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    batch_num = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        batch_num += 1\n",
    "        \n",
    "        # Forward\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_test_loss += loss.item()\n",
    "    \n",
    "    test_loss = total_test_loss / len(test_loader)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  Epoch {epoch+1:3d}/{epochs}: Train={train_loss:.6f}, Test={test_loss:.6f}, Time={elapsed:.1f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTraining complete in {total_time:.2f} seconds!\")\n",
    "print(f\"Final: Train Loss = {train_losses[-1]:.6f}, Test Loss = {test_losses[-1]:.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART VI: PREDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model.eval()\n",
    "train_preds = []\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(X_train)):\n",
    "        x_in = torch.FloatTensor(X_train[i]).unsqueeze(0).unsqueeze(-1)\n",
    "        pred = model(x_in).item()\n",
    "        train_preds.append(pred)\n",
    "    \n",
    "    for i in range(len(X_test)):\n",
    "        x_in = torch.FloatTensor(X_test[i]).unsqueeze(0).unsqueeze(-1)\n",
    "        pred = model(x_in).item()\n",
    "        test_preds.append(pred)\n",
    "\n",
    "train_preds = np.array(train_preds)\n",
    "test_preds = np.array(test_preds)\n",
    "\n",
    "# Metrics\n",
    "mse = np.mean((y_test.flatten() - test_preds)**2)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(y_test.flatten() - test_preds))\n",
    "r2 = 1 - (np.sum((y_test.flatten() - test_preds)**2) / \n",
    "          np.sum((y_test.flatten() - np.mean(y_test))**2))\n",
    "\n",
    "print(f\"\\nTest Metrics:\")\n",
    "print(f\"  MSE  = {mse:.6f}\")\n",
    "print(f\"  RMSE = {rmse:.6f}\")\n",
    "print(f\"  MAE  = {mae:.6f}\")\n",
    "print(f\"  R²   = {r2:.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART VII: VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Plot 1: ODE solution\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "ax1.plot(t, x, 'b-', linewidth=1, alpha=0.7)\n",
    "split_point = train_size + seq_length\n",
    "if split_point < len(t):\n",
    "    ax1.axvline(x=t[split_point], color='r', linestyle='--', linewidth=2, label='Train/Test')\n",
    "ax1.set_xlabel('Time [s]')\n",
    "ax1.set_ylabel('Position x [m]')\n",
    "ax1.set_title('ODE Solution', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Phase space\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "ax2.plot(x, v, 'b-', linewidth=0.5, alpha=0.5)\n",
    "ax2.set_xlabel('Position x')\n",
    "ax2.set_ylabel('Velocity v')\n",
    "ax2.set_title('Phase Space', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Training curves\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "ax3.plot(train_losses, 'b-', linewidth=2, label='Train')\n",
    "ax3.plot(test_losses, 'r-', linewidth=2, label='Test')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss (MSE)')\n",
    "ax3.set_title('Training Curves', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "# Plot 4: Predictions\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "train_idx = np.arange(seq_length, seq_length + len(train_preds))\n",
    "test_idx = np.arange(seq_length + len(train_preds), \n",
    "                     seq_length + len(train_preds) + len(test_preds))\n",
    "ax4.plot(train_idx, y_train.flatten(), 'b-', linewidth=1, alpha=0.5, label='Train True')\n",
    "ax4.plot(train_idx, train_preds, 'g-', linewidth=1, label='Train Pred')\n",
    "ax4.plot(test_idx, y_test.flatten(), 'r-', linewidth=1, alpha=0.5, label='Test True')\n",
    "ax4.plot(test_idx, test_preds, 'orange', linewidth=1, label='Test Pred')\n",
    "ax4.set_xlabel('Time Step')\n",
    "ax4.set_ylabel('Position')\n",
    "ax4.set_title('Predictions', fontweight='bold')\n",
    "ax4.legend(fontsize=8)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Error distribution\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "errors = test_preds - y_test.flatten()\n",
    "ax5.hist(errors, bins=30, alpha=0.7, edgecolor='black')\n",
    "ax5.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "ax5.set_xlabel('Prediction Error')\n",
    "ax5.set_ylabel('Frequency')\n",
    "ax5.set_title(f'Error Distribution (MAE={mae:.4f})', fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 6: Summary stats\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "ax6.axis('off')\n",
    "summary_text = f\"\"\"\n",
    "TRAINING SUMMARY\n",
    "\n",
    "Dataset:\n",
    "  ODE points: {len(x)}\n",
    "  Sequences: {len(X)}\n",
    "  Train: {len(X_train)} (75%)\n",
    "  Test: {len(X_test)} (25%)\n",
    "\n",
    "Model: LSTM\n",
    "  Hidden: 64\n",
    "  Layers: 2\n",
    "  Epochs: {epochs}\n",
    "  \n",
    "Results:\n",
    "  MSE:  {mse:.6f}\n",
    "  RMSE: {rmse:.6f}\n",
    "  MAE:  {mae:.6f}\n",
    "  R²:   {r2:.6f}\n",
    "  \n",
    "Time: {total_time:.1f}s\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.5, summary_text, fontsize=11, family='monospace',\n",
    "         verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#plt.savefig('/mnt/user-data/outputs/rnn_ode_optimized.png', dpi=150)\n",
    "print(\"\\n✓ Plot saved: rnn_ode_optimized.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n✓ Successfully trained LSTM on ODE data\")\n",
    "print(f\"✓ Test R² score: {r2:.4f}\")\n",
    "print(f\"✓ No hanging issues!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e934c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Tensorflow (Keras) RNN Time Series Example\n",
    "\n",
    "We present here examples on how we can use Tensorflow/Keras for the examples discussed above.\n",
    "We build a\n",
    "tf.keras.Sequential model with a SimpleRNN layer (the most basic\n",
    "recurrent layer) ￼ followed by a Dense output. The workflow is\n",
    "similar: create the same synthetic sine data and split it into\n",
    "train/test sets; then define, train, and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf98d4fb",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. Data preparation: same sine wave data and sequences as above\n",
    "time_steps = np.linspace(0, 100, 500)\n",
    "data = np.sin(time_steps)                     # (500,)\n",
    "seq_length = 20\n",
    "X, y = [], []\n",
    "for i in range(len(data) - seq_length):\n",
    "    X.append(data[i:i+seq_length])\n",
    "    y.append(data[i+seq_length])\n",
    "X = np.array(X)                               # (480, seq_length)\n",
    "y = np.array(y)                               # (480,)\n",
    "# reshape for RNN: (samples, timesteps, features)\n",
    "X = X.reshape(-1, seq_length, 1)             # (480, 20, 1)\n",
    "y = y.reshape(-1, 1)                         # (480, 1)\n",
    "\n",
    "# Split into train/test (80/20)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa68533",
   "metadata": {
    "editable": true
   },
   "source": [
    "Data: We use the same sine-wave sequence and sliding-window split as\n",
    "in the PyTorch example ￼. The arrays are reshaped to (batch,\n",
    "timesteps, features) for Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "844d856a",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 2. Model definition: Keras SimpleRNN and Dense\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(16, input_shape=(seq_length, 1)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')   # MSE loss and Adam optimizer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1349596",
   "metadata": {
    "editable": true
   },
   "source": [
    "Explanation: Here SimpleRNN(16) creates 16 recurrent units. The model\n",
    "summary shows the shapes and number of parameters. (Keras handles the\n",
    "sequence dimension internally.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ab897c",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,    # use 20% of train data for validation\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236151a0",
   "metadata": {
    "editable": true
   },
   "source": [
    "Training: We train for 50 epochs. The fit call also reports validation\n",
    "loss (using a 20$%$ split of the training data) to monitor\n",
    "generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "748fe896",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 4. Evaluation on test set\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# (Optional) Predictions\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Actual:\", y_test.flatten()[:5])\n",
    "print(\"Pred : \", predictions.flatten()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4593465b",
   "metadata": {
    "editable": true
   },
   "source": [
    "Evaluation: After training, we call model.evaluate on the test set. A\n",
    "low test loss indicates good forecasting accuracy. We also predict and\n",
    "compare a few samples of actual vs. predicted values. This completes\n",
    "the simple RNN forecasting example in TensorFlow.\n",
    "\n",
    "Both examples use only basic RNN cells (no LSTM/GRU) and include data\n",
    "preparation, model definition, training loop, and evaluation. The\n",
    "PyTorch code uses nn.RNN as and the Keras\n",
    "code uses SimpleRNN layer. Each code block above is self-contained\n",
    "and can be run independently with standard libraries (NumPy, PyTorch\n",
    "or TensorFlow)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fdbd71",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using the above data to train an RNN\n",
    "\n",
    "In the code here we have reworked the previous example in order to\n",
    "generate data that can be handled by recurrent neural networks in\n",
    "order to train our model. Here we present for the sake of completeness  a code written using Tensorflow/keras..\n",
    "We use again the Runge Kutta to fourth order as a way to generate the data. We have implemented a simple RNN only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ecc09b",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train_rnn_from_rk4.py\n",
    "\n",
    "import runpy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# ---------- Load RK4-generated data from your script ----------\n",
    "# This runs rungekutta.py and collects its globals. It must populate 't' and 'x' arrays.\n",
    "g = runpy.run_path('rungekutta.py')\n",
    "\n",
    "if not all(k in g for k in ('t','x','v')):\n",
    "    raise RuntimeError(\"rungekutta.py did not expose required variables 't', 'x', 'v' in its globals.\")\n",
    "\n",
    "t = np.array(g['t']).ravel()\n",
    "x = np.array(g['x']).ravel()\n",
    "v = np.array(g['v']).ravel()\n",
    "\n",
    "print(\"Loaded shapes:\", t.shape, x.shape, v.shape)\n",
    "\n",
    "# Simple plot of the original trajectory\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(t, x)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('x')\n",
    "plt.title('True trajectory from RK4')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------- Prepare datasets ----------\n",
    "def make_dataset(series, input_len):\n",
    "    X, y = [], []\n",
    "    N = len(series)\n",
    "    for i in range(N - input_len):\n",
    "        X.append(series[i:i+input_len])\n",
    "        y.append(series[i+input_len])\n",
    "    X = np.array(X).reshape(-1, input_len, 1)  # (samples, timesteps, 1)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "# normalize using global mean/std\n",
    "mean_x, std_x = x.mean(), x.std()\n",
    "x_norm = (x - mean_x) / std_x\n",
    "\n",
    "print(f\"Normalization: mean={mean_x:.6f}, std={std_x:.6f}\")\n",
    "\n",
    "# Model A: input_len = 1 (x_t -> x_{t+1})\n",
    "input_len_A = 1\n",
    "X_A, y_A = make_dataset(x_norm, input_len_A)\n",
    "\n",
    "# Model B: input_len = 10 (used for autoregressive generation)\n",
    "input_len_B = 10\n",
    "X_B, y_B = make_dataset(x_norm, input_len_B)\n",
    "\n",
    "# train/test split\n",
    "test_size = 0.2\n",
    "random_seed = 42\n",
    "Xa_train, Xa_test, ya_train, ya_test = train_test_split(X_A, y_A, test_size=test_size, random_state=random_seed)\n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(X_B, y_B, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "print(\"Model A shapes:\", Xa_train.shape, ya_train.shape, \"Model B shapes:\", Xb_train.shape, yb_train.shape)\n",
    "\n",
    "# ---------- Build models ----------\n",
    "def build_simple_rnn(input_len, hidden_size=32):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(input_len,1)),\n",
    "        tf.keras.layers.SimpleRNN(hidden_size, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss='mse',\n",
    "                  metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "model_A = build_simple_rnn(input_len_A, hidden_size=32)\n",
    "model_B = build_simple_rnn(input_len_B, hidden_size=64)\n",
    "\n",
    "print(\"Model A summary:\")\n",
    "model_A.summary()\n",
    "print(\"\\nModel B summary:\")\n",
    "model_B.summary()\n",
    "\n",
    "# ---------- Train ----------\n",
    "epochs_A = 30\n",
    "epochs_B = 40\n",
    "\n",
    "hist_A = model_A.fit(Xa_train, ya_train, validation_data=(Xa_test, ya_test),\n",
    "                     epochs=epochs_A, batch_size=32, verbose=1)\n",
    "\n",
    "hist_B = model_B.fit(Xb_train, yb_train, validation_data=(Xb_test, yb_test),\n",
    "                     epochs=epochs_B, batch_size=32, verbose=1)\n",
    "\n",
    "# ---------- Plot training curves ----------\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(hist_A.history['loss'], label='train')\n",
    "plt.plot(hist_A.history['val_loss'], label='val')\n",
    "plt.title('Model A loss')\n",
    "plt.xlabel('epoch'); plt.ylabel('mse'); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(hist_B.history['loss'], label='train')\n",
    "plt.plot(hist_B.history['val_loss'], label='val')\n",
    "plt.title('Model B loss')\n",
    "plt.xlabel('epoch'); plt.ylabel('mse'); plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------- Evaluate one-step predictions ----------\n",
    "preds_A = model_A.predict(Xa_test)\n",
    "preds_A_un = preds_A.flatten() * std_x + mean_x\n",
    "ya_test_un = ya_test.flatten() * std_x + mean_x\n",
    "\n",
    "print(\"Model A one-step MSE (unnormalized):\", np.mean((preds_A_un - ya_test_un)**2))\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "nplot = min(100, len(ya_test_un))\n",
    "plt.plot(ya_test_un[:nplot], label='true next x')\n",
    "plt.plot(preds_A_un[:nplot], label='predicted next x (Model A)')\n",
    "plt.title(\"Model A: one-step predictions (segment)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---------- Autoregressive generation using Model B ----------\n",
    "# Start from the first input_len_B true values, then generate the remainder autoregressively\n",
    "initial_window = x_norm[:input_len_B].reshape(1,input_len_B,1)\n",
    "gen_steps = len(x_norm) - input_len_B\n",
    "generated = []\n",
    "current_window = initial_window.copy()\n",
    "\n",
    "for i in range(gen_steps):\n",
    "    pred_norm = model_B.predict(current_window, verbose=0)  # shape (1,1)\n",
    "    generated.append(pred_norm.flatten()[0])\n",
    "    # roll the window and append prediction\n",
    "    current_window = np.concatenate([current_window[:,1:,:], pred_norm.reshape(1,1,1)], axis=1)\n",
    "\n",
    "generated_un = np.array(generated) * std_x + mean_x\n",
    "true_remainder = x[input_len_B:]\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(true_remainder, label='true remainder')\n",
    "plt.plot(generated_un, label='generated (Model B)')\n",
    "plt.title('Model B autoregressive generation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---------- Save models ----------\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "path_A = os.path.join('saved_models','model_A_rnn.h5')\n",
    "path_B = os.path.join('saved_models','model_B_rnn.h5')\n",
    "model_A.save(path_A)\n",
    "model_B.save(path_B)\n",
    "print(\"Saved models to:\", path_A, path_B)\n",
    "\n",
    "# ---------- Final numeric summaries ----------\n",
    "preds_B = model_B.predict(Xb_test)\n",
    "preds_B_un = preds_B.flatten() * std_x + mean_x\n",
    "yb_test_un = yb_test.flatten() * std_x + mean_x\n",
    "mse_A = np.mean((preds_A_un - ya_test_un)**2)\n",
    "mse_B = np.mean((preds_B_un - yb_test_un)**2)\n",
    "print(f\"One-step MSE (Model A): {mse_A:.6e}\")\n",
    "print(f\"One-step MSE (Model B): {mse_B:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e4e2a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Four effective ways to learn an RNN and preparing for next week\n",
    "1. Long Short Term Memory Make the RNN out of little modules that are designed to remember values for a long time.\n",
    "\n",
    "2. Hessian Free Optimization: Deal with the vanishing gradients problem by using a fancy optimizer that can detect directions with a tiny gradient but even smaller curvature.\n",
    "\n",
    "3. Echo State Networks (ESN): Initialize the input a hidden and hidden-hidden and output-hidden connections very carefully so that the hidden state has a huge reservoir of weakly coupled oscillators which can be selectively driven by the input. ESNs only need to learn the hidden-output connections.\n",
    "\n",
    "4. Good initialization with momentum Initialize like in Echo State Networks, but then learn all of the connections using momentum\n",
    "\n",
    "5. Next week we discuss in more details the mathematics of RNNs"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
