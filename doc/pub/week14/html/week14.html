<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week14.do.txt --pygments_html_style=default --html_style=bloodish --html_links_in_new_window --html_output=week14 --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Advanced machine learning and data analysis for the physical sciences">
<title>Advanced machine learning and data analysis for the physical sciences</title>
<style type="text/css">
/* bloodish style */
body {
  font-family: Helvetica, Verdana, Arial, Sans-serif;
  color: #404040;
  background: #ffffff;
}
h1 { font-size: 1.8em; color: #8A0808; }
h2 { font-size: 1.6em; color: #8A0808; }
h3 { font-size: 1.4em; color: #8A0808; }
h4 { font-size: 1.2em; color: #8A0808; }
a { color: #8A0808; text-decoration:none; }
tt { font-family: "Courier New", Courier; }
p { text-indent: 0px; }
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-style: normal; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa; }div.highlight {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    line-height: 1.21429em;
}
div.cell {
    width: 100%;
    padding: 5px 5px 5px 0;
    margin: 0;
    outline: none;
}
div.input {
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.inner_cell {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
    box-flex: 1;
    flex: 1;
}
div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 4px;
    background: #f7f7f7;
    line-height: 1.21429em;
}
div.input_area > div.highlight {
    margin: .4em;
    border: none;
    padding: 0;
    background-color: transparent;
}
div.output_wrapper {
    position: relative;
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
.output {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
div.output_area {
    padding: 0;
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.output_subarea {
    padding: .4em .4em 0 .4em;
    box-flex: 1;
    flex: 1;
}
div.output_text {
    text-align: left;
    color: #000;
    line-height: 1.21429em;
}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #bababa;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #f8f8f8;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_question.png); }
div { text-align: justify; text-justify: inter-word; }
.tab {
  padding-left: 1.5em;
}
div.toc p,a {
  line-height: 1.3;
  margin-top: 1.1;
  margin-bottom: 1.1;
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plans for the week April 22-26, 2024',
               2,
               None,
               'plans-for-the-week-april-22-26-2024'),
              ('Mathematics of  VAEs', 2, None, 'mathematics-of-vaes'),
              ('Using the KL divergence', 2, None, 'using-the-kl-divergence'),
              ('Kullback-Leibler again', 2, None, 'kullback-leibler-again'),
              ('And applying Bayes rule', 2, None, 'and-applying-bayes-rule'),
              ('Rearranging', 2, None, 'rearranging'),
              ('Inferring the probability',
               2,
               None,
               'inferring-the-probability'),
              ('Central equation of VAEs', 2, None, 'central-equation-of-vaes'),
              ('Setting up SGD', 2, None, 'setting-up-sgd'),
              ('More on the SGD', 2, None, 'more-on-the-sgd'),
              ('Simplification', 2, None, 'simplification'),
              ('Terms to compute', 2, None, 'terms-to-compute'),
              ('Computing the gradients', 2, None, 'computing-the-gradients'),
              ('Code examples using Keras',
               2,
               None,
               'code-examples-using-keras'),
              ('Code in PyTorch for VAEs', 2, None, 'code-in-pytorch-for-vaes'),
              ('More notes to be added on VAEs',
               2,
               None,
               'more-notes-to-be-added-on-vaes')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- ------------------- main content ---------------------- -->
<center>
<h1>Advanced machine learning and data analysis for the physical sciences</h1>
</center>  <!-- document title -->

<!-- author(s): Morten Hjorth-Jensen -->
<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>
<!-- institution(s) -->
<center>
[1] <b>Department of Physics and Center for Computing in Science Education, University of Oslo, Norway</b>
</center>
<center>
[2] <b>Department of Physics and Astronomy and Facility for Rare Isotope Beams, Michigan State University, East Lansing, Michigan, USA</b>
</center>
<br>
<center>
<h4>April 23, 2024</h4>
</center> <!-- date -->
<br>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="plans-for-the-week-april-22-26-2024">Plans for the week April 22-26, 2024  </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Deep generative models</b>
<p>
<ol>
<li> Variational Autoencoders (VAE), Mathematics and codes, continuation from last week</li>
<li> Diffusion models</li>
<li> Reading recommendation:
<ol type="a"></li>
 <li> Goodfellow et al chapter 20.10-20-14</li>
 <li> Calvin Luo <a href="https://calvinyluo.com/2022/08/26/diffusion-tutorial.html" target="_blank"><tt>https://calvinyluo.com/2022/08/26/diffusion-tutorial.html</tt></a></li>
 <li> An Introduction to Variational Autoencoders, by Kingma and Welling, see <a href="https://arxiv.org/abs/1906.02691" target="_blank"><tt>https://arxiv.org/abs/1906.02691</tt></a>
<!-- o "Video of lecture":"" -->
<!-- o <a href="https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/HandwrittenNotes/2024/NotesApril23.pdf" target="_blank">Whiteboard notes</a> --></li>
</ol>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="mathematics-of-vaes">Mathematics of  VAEs </h2>

<p>We want to train the marginal probability with some latent varrables \( \boldsymbol{h} \)</p>
$$
p(\boldsymbol{x};\boldsymbol{\Theta}) = \int d\boldsymbol{h}p(\boldsymbol{x},\boldsymbol{h};\boldsymbol{\Theta}),
$$

<p>for the continuous version (see previous slides for the discrete variant).</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="using-the-kl-divergence">Using the KL divergence </h2>

<p>In practice, for most \( \boldsymbol{h} \), \( p(\boldsymbol{x}\vert \boldsymbol{h}; \boldsymbol{\Theta}) \)
will be nearly zero, and hence contributes almost nothing to our
estimate of \( p(\boldsymbol{x}) \).
</p>

<p>The key idea behind the variational autoencoder is to attempt to
sample values of \( \boldsymbol{h} \) that are likely to have produced \( \boldsymbol{x} \),
and compute \( p(\boldsymbol{x}) \) just from those.
</p>

<p>This means that we need a new function \( Q(\boldsymbol{h}|\boldsymbol{x}) \) which can
take a value of \( \boldsymbol{x} \) and give us a distribution over \( \boldsymbol{h} \)
values that are likely to produce \( \boldsymbol{x} \).  Hopefully the space of
\( \boldsymbol{h} \) values that are likely under \( Q \) will be much smaller than
the space of all \( \boldsymbol{h} \)'s that are likely under the prior
\( p(\boldsymbol{h}) \).  This lets us, for example, compute \( E_{\boldsymbol{h}\sim
Q}p(\boldsymbol{x}\vert \boldsymbol{h}) \) relatively easily. Note that we drop
\( \boldsymbol{\Theta} \) from here and for notational simplicity.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="kullback-leibler-again">Kullback-Leibler again </h2>

<p>However, if \( \boldsymbol{h} \) is sampled from an arbitrary distribution with
PDF \( Q(\boldsymbol{h}) \), which is not \( \mathcal{N}(0,I) \), then how does that
help us optimize \( p(\boldsymbol{x}) \)?
</p>

<p>The first thing we need to do is relate
\( E_{\boldsymbol{h}\sim Q}P(\boldsymbol{x}\vert \boldsymbol{h}) \) and \( p(\boldsymbol{x}) \).  We will see where \( Q \) comes from later.
</p>

<p>The relationship between \( E_{\boldsymbol{h}\sim Q}p(\boldsymbol{x}\vert \boldsymbol{h}) \) and \( p(\boldsymbol{x}) \) is one of the cornerstones of variational Bayesian methods.
We begin with the definition of Kullback-Leibler divergence (KL divergence or \( \mathcal{D} \)) between \( p(\boldsymbol{h}\vert \boldsymbol{x}) \) and \( Q(\boldsymbol{h}) \), for some arbitrary \( Q \) (which may or may not depend on \( \boldsymbol{x} \)):
</p>
$$
    \mathcal{D}\left[Q(\boldsymbol{h})\|p(\boldsymbol{h}|\boldsymbol{x})\right]=E_{\boldsymbol{h}\sim Q}\left[\log Q(\boldsymbol{h}) - \log p(\boldsymbol{h}|\boldsymbol{x}) \right].
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="and-applying-bayes-rule">And applying Bayes rule </h2>

<p>We can get both \( p(\boldsymbol{x}) \) and \( p(\boldsymbol{x}\vert \boldsymbol{h}) \) into this equation by applying Bayes rule to \( p(\boldsymbol{h}|\boldsymbol{x}) \)</p>
$$
    \mathcal{D}\left[Q(\boldsymbol{h})\|p(\boldsymbol{h}\vert \boldsymbol{x})\right]=E_{\boldsymbol{h}\sim Q}\left[\log Q(\boldsymbol{h}) - \log p(\boldsymbol{x}|\boldsymbol{h}) - \log p(\boldsymbol{h}) \right] + \log p(\boldsymbol{x}).
$$

<p>Here, \( \log p(\boldsymbol{x}) \) comes out of the expectation because it does not depend on \( \boldsymbol{h} \).
Negating both sides, rearranging, and contracting part of \( E_{\boldsymbol{h}\sim Q} \) into a KL-divergence terms yields:
</p>
$$
\log p(\boldsymbol{x}) - \mathcal{D}\left[Q(\boldsymbol{h})\|p(\boldsymbol{h}\vert \boldsymbol{x})\right]=E_{\boldsymbol{h}\sim Q}\left[\log p(\boldsymbol{x}\vert\boldsymbol{h})  \right] - \mathcal{D}\left[Q(\boldsymbol{h})\|P(\boldsymbol{h})\right].
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rearranging">Rearranging </h2>

<p>Using Bayes rule we obtain</p>
$$
E_{\boldsymbol{h}\sim Q}\left[\log p(y_i|\boldsymbol{h},x_i)\right]=E_{\boldsymbol{h}\sim Q}\left[\log p(\boldsymbol{h}|y_i,x_i) - \log p(\boldsymbol{h}|x_i) + \log p(y_i|x_i) \right]
$$

<p>Rearranging the terms and subtracting \( E_{\boldsymbol{h}\sim Q}\log Q(\boldsymbol{h}) \) from both sides gives</p>
$$
\begin{array}{c}
\log P(y_i|x_i) - E_{\boldsymbol{h}\sim Q}\left[\log Q(\boldsymbol{h})-\log p(\boldsymbol{h}|x_i,y_i)\right]=\hspace{10em}\\
\hspace{10em}E_{\boldsymbol{h}\sim Q}\left[\log p(y_i|\boldsymbol{h},x_i)+\log p(\boldsymbol{h}|x_i)-\log Q(\boldsymbol{h})\right]
\end{array}
$$

<p>Note that \( \boldsymbol{x} \) is fixed, and \( Q \) can be \textit{any} distribution, not
just a distribution which does a good job mapping \( \boldsymbol{x} \) to the \( \boldsymbol{h} \)'s
that can produce \( X \).
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="inferring-the-probability">Inferring the probability </h2>

<p>Since we are interested in inferring \( p(\boldsymbol{x}) \), it makes sense to
construct a \( Q \) which \textit{does} depend on \( \boldsymbol{x} \), and in particular,
one which makes \( \mathcal{D}\left[Q(\boldsymbol{h})\|p(\boldsymbol{h}|\boldsymbol{x})\right] \) small
</p>
$$
\log p(\boldsymbol{x}) - \mathcal{D}\left[Q(\boldsymbol{h}|\boldsymbol{x})\|p(\boldsymbol{h}|\boldsymbol{x})\right]=E_{\boldsymbol{h}\sim Q}\left[\log p(\boldsymbol{x}|\boldsymbol{h})  \right] - \mathcal{D}\left[Q(\boldsymbol{h}|\boldsymbol{x})\|p(\boldsymbol{h})\right].
$$

<p>Hence, during training, it makes sense to choose a \( Q \) which will make
\( E_{\boldsymbol{h}\sim Q}[\log Q(\boldsymbol{h})- \) $\log p(\boldsymbol{h}|x_i,y_i)]$ (a
\( \mathcal{D} \)-divergence) small, such that the right hand side is a
close approximation to \( \log p(y_i|y_i) \).
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="central-equation-of-vaes">Central equation of VAEs </h2>

<p>This equation serves as the core of the variational autoencoder, and
it is worth spending some time thinking about what it means.
</p>

<ol>
<li> The left hand side has the quantity we want to maximize, namely \( \log p(\boldsymbol{x}) \) plus an error term.</li>
<li> The right hand side is something we can optimize via stochastic gradient descent given the right choice of \( Q \).</li>
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="setting-up-sgd">Setting up SGD </h2>
<p>So how can we perform stochastic gradient descent?</p>

<p>First we need to be a bit more specific about the form that \( Q(\boldsymbol{h}|\boldsymbol{x}) \)
will take.  The usual choice is to say that
\( Q(\boldsymbol{h}|\boldsymbol{x})=\mathcal{N}(\boldsymbol{h}|\mu(\boldsymbol{x};\vartheta),\Sigma(;\vartheta)) \), where
\( \mu \) and \( \Sigma \) are arbitrary deterministic functions with
parameters \( \vartheta \) that can be learned from data (we will omit
\( \vartheta \) in later equations).  In practice, \( \mu \) and \( \Sigma \) are
again implemented via neural networks, and \( \Sigma \) is constrained to
be a diagonal matrix.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="more-on-the-sgd">More on the SGD </h2>

<p>The name variational &quot;autoencoder&quot; comes from
the fact that \( \mu \) and \( \Sigma \) are &quot;encoding&quot; \( \boldsymbol{x} \) into the latent
space \( \boldsymbol{h} \).  The advantages of this choice are computational, as they
make it clear how to compute the right hand side.  The last
term---\( \mathcal{D}\left[Q(\boldsymbol{h}|\boldsymbol{x})\|p(\boldsymbol{h})\right] \)---is now a KL-divergence
between two multivariate Gaussian distributions, which can be computed
in closed form as:
</p>
$$
\begin{array}{c}
 \mathcal{D}[\mathcal{N}(\mu_0,\Sigma_0) \| \mathcal{N}(\mu_1,\Sigma_1)] = \hspace{20em}\\
  \hspace{5em}\frac{ 1 }{ 2 } \left( \mathrm{tr} \left( \Sigma_1^{-1} \Sigma_0 \right) + \left( \mu_1 - \mu_0\right)^\top \Sigma_1^{-1} ( \mu_1 - \mu_0 ) - k + \log \left( \frac{ \det \Sigma_1 }{ \det \Sigma_0  } \right)  \right)
\end{array}
$$

<p>where \( k \) is the dimensionality of the distribution.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="simplification">Simplification </h2>
<p>In our case, this simplifies to:</p>
$$
\begin{array}{c}
 \mathcal{D}[\mathcal{N}(\mu(X),\Sigma(X)) \| \mathcal{N}(0,I)] = \hspace{20em}\\
\hspace{6em}\frac{ 1 }{ 2 } \left( \mathrm{tr} \left( \Sigma(X) \right) + \left( \mu(X)\right)^\top ( \mu(X) ) - k - \log\det\left(  \Sigma(X)  \right)  \right).
\end{array}
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="terms-to-compute">Terms to compute </h2>

<p>The first term on the right hand side is a bit more tricky.
We could use sampling to estimate \( E_{z\sim Q}\left[\log P(X|z)  \right] \), but getting a good estimate would require passing many samples of \( z \) through \( f \), which would be expensive.
Hence, as is standard in stochastic gradient descent, we take one sample of \( z \) and treat \( \log P(X|z) \) for that \( z \) as an approximation of \( E_{z\sim Q}\left[\log P(X|z)  \right] \).
After all, we are already doing stochastic gradient descent over different values of \( X \) sampled from a dataset \( D \).
The full equation we want to optimize is:
</p>

$$
\begin{array}{c}
    E_{X\sim D}\left[\log P(X) - \mathcal{D}\left[Q(z|X)\|P(z|X)\right]\right]=\hspace{16em}\\
\hspace{10em}E_{X\sim D}\left[E_{z\sim Q}\left[\log P(X|z)  \right] - \mathcal{D}\left[Q(z|X)\|P(z)\right]\right].
\end{array}
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="computing-the-gradients">Computing the gradients </h2>

<p>If we take the gradient of this equation, the gradient symbol can be moved into the expectations.
Therefore, we can sample a single value of \( X \) and a single value of \( z \) from the distribution \( Q(z|X) \), and compute the gradient of:
</p>
$$
\begin{equation}
 \log P(X|z)-\mathcal{D}\left[Q(z|X)\|P(z)\right].
\label{_auto1}
\end{equation}
$$

<p>We can then average the gradient of this function over arbitrarily many samples of \( X \) and \( z \), and the result converges to the gradient.</p>

<p>There is, however, a significant problem
\( E_{z\sim Q}\left[\log P(X|z)  \right] \) depends not just on the parameters of \( P \), but also on the parameters of \( Q \).
</p>

<p>In order to make VAEs work, it is essential to drive \( Q \) to produce codes for \( X \) that \( P \) can reliably decode.  </p>
$$
 E_{X\sim D}\left[E_{\epsilon\sim\mathcal{N}(0,I)}[\log P(X|z=\mu(X)+\Sigma^{1/2}(X)*\epsilon)]-\mathcal{D}\left[Q(z|X)\|P(z)\right]\right].
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="code-examples-using-keras">Code examples using Keras </h2>

<p>Code taken from  <a href="https://keras.io/examples/generative/vae/" target="_blank"><tt>https://keras.io/examples/generative/vae/</tt></a></p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">Title: Variational AutoEncoder</span>
<span style="color: #BA2121; font-style: italic">Author: [fchollet](https://twitter.com/fchollet)</span>
<span style="color: #BA2121; font-style: italic">Date created: 2020/05/03</span>
<span style="color: #BA2121; font-style: italic">Last modified: 2023/11/22</span>
<span style="color: #BA2121; font-style: italic">Description: Convolutional Variational AutoEncoder (VAE) trained on MNIST digits.</span>
<span style="color: #BA2121; font-style: italic">Accelerator: GPU</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Setup</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>

os<span style="color: #666666">.</span>environ[<span style="color: #BA2121">&quot;KERAS_BACKEND&quot;</span>] <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;tensorflow&quot;</span>

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">tf</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">keras</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">keras</span> <span style="color: #008000; font-weight: bold">import</span> layers

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Create a sampling layer</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>


<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">Sampling</span>(layers<span style="color: #666666">.</span>Layer):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.&quot;&quot;&quot;</span>

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">call</span>(<span style="color: #008000">self</span>, inputs):
        z_mean, z_log_var <span style="color: #666666">=</span> inputs
        batch <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>shape(z_mean)[<span style="color: #666666">0</span>]
        dim <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>shape(z_mean)[<span style="color: #666666">1</span>]
        epsilon <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(shape<span style="color: #666666">=</span>(batch, dim))
        <span style="color: #008000; font-weight: bold">return</span> z_mean <span style="color: #666666">+</span> tf<span style="color: #666666">.</span>exp(<span style="color: #666666">0.5</span> <span style="color: #666666">*</span> z_log_var) <span style="color: #666666">*</span> epsilon


<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Build the encoder</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

latent_dim <span style="color: #666666">=</span> <span style="color: #666666">2</span>

encoder_inputs <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>Input(shape<span style="color: #666666">=</span>(<span style="color: #666666">28</span>, <span style="color: #666666">28</span>, <span style="color: #666666">1</span>))
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Conv2D(<span style="color: #666666">32</span>, <span style="color: #666666">3</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>, strides<span style="color: #666666">=2</span>, padding<span style="color: #666666">=</span><span style="color: #BA2121">&quot;same&quot;</span>)(encoder_inputs)
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Conv2D(<span style="color: #666666">64</span>, <span style="color: #666666">3</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>, strides<span style="color: #666666">=2</span>, padding<span style="color: #666666">=</span><span style="color: #BA2121">&quot;same&quot;</span>)(x)
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Flatten()(x)
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Dense(<span style="color: #666666">16</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>)(x)
z_mean <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Dense(latent_dim, name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;z_mean&quot;</span>)(x)
z_log_var <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Dense(latent_dim, name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;z_log_var&quot;</span>)(x)
z <span style="color: #666666">=</span> Sampling()([z_mean, z_log_var])
encoder <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>Model(encoder_inputs, [z_mean, z_log_var, z], name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;encoder&quot;</span>)
encoder<span style="color: #666666">.</span>summary()

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Build the decoder</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

latent_inputs <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>Input(shape<span style="color: #666666">=</span>(latent_dim,))
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Dense(<span style="color: #666666">7</span> <span style="color: #666666">*</span> <span style="color: #666666">7</span> <span style="color: #666666">*</span> <span style="color: #666666">64</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>)(latent_inputs)
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Reshape((<span style="color: #666666">7</span>, <span style="color: #666666">7</span>, <span style="color: #666666">64</span>))(x)
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Conv2DTranspose(<span style="color: #666666">64</span>, <span style="color: #666666">3</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>, strides<span style="color: #666666">=2</span>, padding<span style="color: #666666">=</span><span style="color: #BA2121">&quot;same&quot;</span>)(x)
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Conv2DTranspose(<span style="color: #666666">32</span>, <span style="color: #666666">3</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>, strides<span style="color: #666666">=2</span>, padding<span style="color: #666666">=</span><span style="color: #BA2121">&quot;same&quot;</span>)(x)
decoder_outputs <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Conv2DTranspose(<span style="color: #666666">1</span>, <span style="color: #666666">3</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;sigmoid&quot;</span>, padding<span style="color: #666666">=</span><span style="color: #BA2121">&quot;same&quot;</span>)(x)
decoder <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>Model(latent_inputs, decoder_outputs, name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;decoder&quot;</span>)
decoder<span style="color: #666666">.</span>summary()

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Define the VAE as a `Model` with a custom `train_step`</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>


<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">VAE</span>(keras<span style="color: #666666">.</span>Model):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, encoder, decoder, <span style="color: #666666">**</span>kwargs):
        <span style="color: #008000">super</span>()<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>(<span style="color: #666666">**</span>kwargs)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>encoder <span style="color: #666666">=</span> encoder
        <span style="color: #008000">self</span><span style="color: #666666">.</span>decoder <span style="color: #666666">=</span> decoder
        <span style="color: #008000">self</span><span style="color: #666666">.</span>total_loss_tracker <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>Mean(name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;total_loss&quot;</span>)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>reconstruction_loss_tracker <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>Mean(
            name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;reconstruction_loss&quot;</span>
        )
        <span style="color: #008000">self</span><span style="color: #666666">.</span>kl_loss_tracker <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>Mean(name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;kl_loss&quot;</span>)

    <span style="color: #AA22FF">@property</span>
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">metrics</span>(<span style="color: #008000">self</span>):
        <span style="color: #008000; font-weight: bold">return</span> [
            <span style="color: #008000">self</span><span style="color: #666666">.</span>total_loss_tracker,
            <span style="color: #008000">self</span><span style="color: #666666">.</span>reconstruction_loss_tracker,
            <span style="color: #008000">self</span><span style="color: #666666">.</span>kl_loss_tracker,
        ]

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">train_step</span>(<span style="color: #008000">self</span>, data):
        <span style="color: #008000; font-weight: bold">with</span> tf<span style="color: #666666">.</span>GradientTape() <span style="color: #008000; font-weight: bold">as</span> tape:
            z_mean, z_log_var, z <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>encoder(data)
            reconstruction <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>decoder(z)
            reconstruction_loss <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>reduce_mean(
                tf<span style="color: #666666">.</span>reduce_sum(
                    keras<span style="color: #666666">.</span>losses<span style="color: #666666">.</span>binary_crossentropy(data, reconstruction),
                    axis<span style="color: #666666">=</span>(<span style="color: #666666">1</span>, <span style="color: #666666">2</span>),
                )
            )
            kl_loss <span style="color: #666666">=</span> <span style="color: #666666">-0.5</span> <span style="color: #666666">*</span> (<span style="color: #666666">1</span> <span style="color: #666666">+</span> z_log_var <span style="color: #666666">-</span> tf<span style="color: #666666">.</span>square(z_mean) <span style="color: #666666">-</span> tf<span style="color: #666666">.</span>exp(z_log_var))
            kl_loss <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>reduce_mean(tf<span style="color: #666666">.</span>reduce_sum(kl_loss, axis<span style="color: #666666">=1</span>))
            total_loss <span style="color: #666666">=</span> reconstruction_loss <span style="color: #666666">+</span> kl_loss
        grads <span style="color: #666666">=</span> tape<span style="color: #666666">.</span>gradient(total_loss, <span style="color: #008000">self</span><span style="color: #666666">.</span>trainable_weights)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>optimizer<span style="color: #666666">.</span>apply_gradients(<span style="color: #008000">zip</span>(grads, <span style="color: #008000">self</span><span style="color: #666666">.</span>trainable_weights))
        <span style="color: #008000">self</span><span style="color: #666666">.</span>total_loss_tracker<span style="color: #666666">.</span>update_state(total_loss)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>reconstruction_loss_tracker<span style="color: #666666">.</span>update_state(reconstruction_loss)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>kl_loss_tracker<span style="color: #666666">.</span>update_state(kl_loss)
        <span style="color: #008000; font-weight: bold">return</span> {
            <span style="color: #BA2121">&quot;loss&quot;</span>: <span style="color: #008000">self</span><span style="color: #666666">.</span>total_loss_tracker<span style="color: #666666">.</span>result(),
            <span style="color: #BA2121">&quot;reconstruction_loss&quot;</span>: <span style="color: #008000">self</span><span style="color: #666666">.</span>reconstruction_loss_tracker<span style="color: #666666">.</span>result(),
            <span style="color: #BA2121">&quot;kl_loss&quot;</span>: <span style="color: #008000">self</span><span style="color: #666666">.</span>kl_loss_tracker<span style="color: #666666">.</span>result(),
        }


<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Train the VAE</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

(x_train, _), (x_test, _) <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>datasets<span style="color: #666666">.</span>mnist<span style="color: #666666">.</span>load_data()
mnist_digits <span style="color: #666666">=</span> np<span style="color: #666666">.</span>concatenate([x_train, x_test], axis<span style="color: #666666">=0</span>)
mnist_digits <span style="color: #666666">=</span> np<span style="color: #666666">.</span>expand_dims(mnist_digits, <span style="color: #666666">-1</span>)<span style="color: #666666">.</span>astype(<span style="color: #BA2121">&quot;float32&quot;</span>) <span style="color: #666666">/</span> <span style="color: #666666">255</span>

vae <span style="color: #666666">=</span> VAE(encoder, decoder)
vae<span style="color: #666666">.</span>compile(optimizer<span style="color: #666666">=</span>keras<span style="color: #666666">.</span>optimizers<span style="color: #666666">.</span>Adam())
vae<span style="color: #666666">.</span>fit(mnist_digits, epochs<span style="color: #666666">=30</span>, batch_size<span style="color: #666666">=128</span>)

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Display a grid of sampled digits</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">plot_latent_space</span>(vae, n<span style="color: #666666">=30</span>, figsize<span style="color: #666666">=15</span>):
    <span style="color: #408080; font-style: italic"># display a n*n 2D manifold of digits</span>
    digit_size <span style="color: #666666">=</span> <span style="color: #666666">28</span>
    scale <span style="color: #666666">=</span> <span style="color: #666666">1.0</span>
    figure <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((digit_size <span style="color: #666666">*</span> n, digit_size <span style="color: #666666">*</span> n))
    <span style="color: #408080; font-style: italic"># linearly spaced coordinates corresponding to the 2D plot</span>
    <span style="color: #408080; font-style: italic"># of digit classes in the latent space</span>
    grid_x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-</span>scale, scale, n)
    grid_y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-</span>scale, scale, n)[::<span style="color: #666666">-1</span>]

    <span style="color: #008000; font-weight: bold">for</span> i, yi <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(grid_y):
        <span style="color: #008000; font-weight: bold">for</span> j, xi <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(grid_x):
            z_sample <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([[xi, yi]])
            x_decoded <span style="color: #666666">=</span> vae<span style="color: #666666">.</span>decoder<span style="color: #666666">.</span>predict(z_sample, verbose<span style="color: #666666">=0</span>)
            digit <span style="color: #666666">=</span> x_decoded[<span style="color: #666666">0</span>]<span style="color: #666666">.</span>reshape(digit_size, digit_size)
            figure[
                i <span style="color: #666666">*</span> digit_size : (i <span style="color: #666666">+</span> <span style="color: #666666">1</span>) <span style="color: #666666">*</span> digit_size,
                j <span style="color: #666666">*</span> digit_size : (j <span style="color: #666666">+</span> <span style="color: #666666">1</span>) <span style="color: #666666">*</span> digit_size,
            ] <span style="color: #666666">=</span> digit

    plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(figsize, figsize))
    start_range <span style="color: #666666">=</span> digit_size <span style="color: #666666">//</span> <span style="color: #666666">2</span>
    end_range <span style="color: #666666">=</span> n <span style="color: #666666">*</span> digit_size <span style="color: #666666">+</span> start_range
    pixel_range <span style="color: #666666">=</span> np<span style="color: #666666">.</span>arange(start_range, end_range, digit_size)
    sample_range_x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>round(grid_x, <span style="color: #666666">1</span>)
    sample_range_y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>round(grid_y, <span style="color: #666666">1</span>)
    plt<span style="color: #666666">.</span>xticks(pixel_range, sample_range_x)
    plt<span style="color: #666666">.</span>yticks(pixel_range, sample_range_y)
    plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;z[0]&quot;</span>)
    plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;z[1]&quot;</span>)
    plt<span style="color: #666666">.</span>imshow(figure, cmap<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Greys_r&quot;</span>)
    plt<span style="color: #666666">.</span>show()


plot_latent_space(vae)

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Display how the latent space clusters different digit classes</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">plot_label_clusters</span>(vae, data, labels):
    <span style="color: #408080; font-style: italic"># display a 2D plot of the digit classes in the latent space</span>
    z_mean, _, _ <span style="color: #666666">=</span> vae<span style="color: #666666">.</span>encoder<span style="color: #666666">.</span>predict(data, verbose<span style="color: #666666">=0</span>)
    plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">12</span>, <span style="color: #666666">10</span>))
    plt<span style="color: #666666">.</span>scatter(z_mean[:, <span style="color: #666666">0</span>], z_mean[:, <span style="color: #666666">1</span>], c<span style="color: #666666">=</span>labels)
    plt<span style="color: #666666">.</span>colorbar()
    plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;z[0]&quot;</span>)
    plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;z[1]&quot;</span>)
    plt<span style="color: #666666">.</span>show()


(x_train, y_train), _ <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>datasets<span style="color: #666666">.</span>mnist<span style="color: #666666">.</span>load_data()
x_train <span style="color: #666666">=</span> np<span style="color: #666666">.</span>expand_dims(x_train, <span style="color: #666666">-1</span>)<span style="color: #666666">.</span>astype(<span style="color: #BA2121">&quot;float32&quot;</span>) <span style="color: #666666">/</span> <span style="color: #666666">255</span>

plot_label_clusters(vae, x_train, y_train)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="code-in-pytorch-for-vaes">Code in PyTorch for VAEs </h2>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torch.autograd</span> <span style="color: #008000; font-weight: bold">import</span> Variable
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.nn.functional</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">F</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torchvision</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torchvision</span> <span style="color: #008000; font-weight: bold">import</span> transforms
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.optim</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">optim</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torch</span> <span style="color: #008000; font-weight: bold">import</span> nn
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torch</span> <span style="color: #008000; font-weight: bold">import</span> distributions

<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">Encoder</span>(torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Module):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, D_in, H, latent_size):
        <span style="color: #008000">super</span>(Encoder, <span style="color: #008000">self</span>)<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
        <span style="color: #008000">self</span><span style="color: #666666">.</span>linear1 <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Linear(D_in, H)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>linear2 <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Linear(H, H)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>enc_mu <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Linear(H, latent_size)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>enc_log_sigma <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Linear(H, latent_size)

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, x):
        x <span style="color: #666666">=</span> F<span style="color: #666666">.</span>relu(<span style="color: #008000">self</span><span style="color: #666666">.</span>linear1(x))
        x <span style="color: #666666">=</span> F<span style="color: #666666">.</span>relu(<span style="color: #008000">self</span><span style="color: #666666">.</span>linear2(x))
        mu <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>enc_mu(x)
        log_sigma <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>enc_log_sigma(x)
        sigma <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>exp(log_sigma)
        <span style="color: #008000; font-weight: bold">return</span> torch<span style="color: #666666">.</span>distributions<span style="color: #666666">.</span>Normal(loc<span style="color: #666666">=</span>mu, scale<span style="color: #666666">=</span>sigma)


<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">Decoder</span>(torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Module):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, D_in, H, D_out):
        <span style="color: #008000">super</span>(Decoder, <span style="color: #008000">self</span>)<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
        <span style="color: #008000">self</span><span style="color: #666666">.</span>linear1 <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Linear(D_in, H)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>linear2 <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Linear(H, D_out)
        

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, x):
        x <span style="color: #666666">=</span> F<span style="color: #666666">.</span>relu(<span style="color: #008000">self</span><span style="color: #666666">.</span>linear1(x))
        mu <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>tanh(<span style="color: #008000">self</span><span style="color: #666666">.</span>linear2(x))
        <span style="color: #008000; font-weight: bold">return</span> torch<span style="color: #666666">.</span>distributions<span style="color: #666666">.</span>Normal(mu, torch<span style="color: #666666">.</span>ones_like(mu))

<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">VAE</span>(torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Module):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, encoder, decoder):
        <span style="color: #008000">super</span>(VAE, <span style="color: #008000">self</span>)<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
        <span style="color: #008000">self</span><span style="color: #666666">.</span>encoder <span style="color: #666666">=</span> encoder
        <span style="color: #008000">self</span><span style="color: #666666">.</span>decoder <span style="color: #666666">=</span> decoder

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, state):
        q_z <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>encoder(state)
        z <span style="color: #666666">=</span> q_z<span style="color: #666666">.</span>rsample()
        <span style="color: #008000; font-weight: bold">return</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>decoder(z), q_z


transform <span style="color: #666666">=</span> transforms<span style="color: #666666">.</span>Compose(
    [transforms<span style="color: #666666">.</span>ToTensor(),
     <span style="color: #408080; font-style: italic"># Normalize the images to be -0.5, 0.5</span>
     transforms<span style="color: #666666">.</span>Normalize(<span style="color: #666666">0.5</span>, <span style="color: #666666">1</span>)]
    )
mnist <span style="color: #666666">=</span> torchvision<span style="color: #666666">.</span>datasets<span style="color: #666666">.</span>MNIST(<span style="color: #BA2121">&#39;./&#39;</span>, download<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, transform<span style="color: #666666">=</span>transform)

input_dim <span style="color: #666666">=</span> <span style="color: #666666">28</span> <span style="color: #666666">*</span> <span style="color: #666666">28</span>
batch_size <span style="color: #666666">=</span> <span style="color: #666666">128</span>
num_epochs <span style="color: #666666">=</span> <span style="color: #666666">100</span>
learning_rate <span style="color: #666666">=</span> <span style="color: #666666">0.001</span>
hidden_size <span style="color: #666666">=</span> <span style="color: #666666">512</span>
latent_size <span style="color: #666666">=</span> <span style="color: #666666">8</span>

<span style="color: #008000; font-weight: bold">if</span> torch<span style="color: #666666">.</span>cuda<span style="color: #666666">.</span>is_available():
    device <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>device(<span style="color: #BA2121">&#39;cuda&#39;</span>)
<span style="color: #008000; font-weight: bold">else</span>:
    device <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>device(<span style="color: #BA2121">&#39;cpu&#39;</span>)

dataloader <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>utils<span style="color: #666666">.</span>data<span style="color: #666666">.</span>DataLoader(
    mnist, batch_size<span style="color: #666666">=</span>batch_size,
    shuffle<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, 
    pin_memory<span style="color: #666666">=</span>torch<span style="color: #666666">.</span>cuda<span style="color: #666666">.</span>is_available())

<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Number of samples: &#39;</span>, <span style="color: #008000">len</span>(mnist))

encoder <span style="color: #666666">=</span> Encoder(input_dim, hidden_size, latent_size)
decoder <span style="color: #666666">=</span> Decoder(latent_size, hidden_size, input_dim)

vae <span style="color: #666666">=</span> VAE(encoder, decoder)<span style="color: #666666">.</span>to(device)

optimizer <span style="color: #666666">=</span> optim<span style="color: #666666">.</span>Adam(vae<span style="color: #666666">.</span>parameters(), lr<span style="color: #666666">=</span>learning_rate)
<span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(num_epochs):
    <span style="color: #008000; font-weight: bold">for</span> data <span style="color: #AA22FF; font-weight: bold">in</span> dataloader:
        inputs, _ <span style="color: #666666">=</span> data
        inputs <span style="color: #666666">=</span> inputs<span style="color: #666666">.</span>view(<span style="color: #666666">-1</span>, input_dim)<span style="color: #666666">.</span>to(device)
        optimizer<span style="color: #666666">.</span>zero_grad()
        p_x, q_z <span style="color: #666666">=</span> vae(inputs)
        log_likelihood <span style="color: #666666">=</span> p_x<span style="color: #666666">.</span>log_prob(inputs)<span style="color: #666666">.</span>sum(<span style="color: #666666">-1</span>)<span style="color: #666666">.</span>mean()
        kl <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>distributions<span style="color: #666666">.</span>kl_divergence(
            q_z, 
            torch<span style="color: #666666">.</span>distributions<span style="color: #666666">.</span>Normal(<span style="color: #666666">0</span>, <span style="color: #666666">1.</span>)
        )<span style="color: #666666">.</span>sum(<span style="color: #666666">-1</span>)<span style="color: #666666">.</span>mean()
        loss <span style="color: #666666">=</span> <span style="color: #666666">-</span>(log_likelihood <span style="color: #666666">-</span> kl)
        loss<span style="color: #666666">.</span>backward()
        optimizer<span style="color: #666666">.</span>step()
        l <span style="color: #666666">=</span> loss<span style="color: #666666">.</span>item()
    <span style="color: #008000">print</span>(epoch, l, log_likelihood<span style="color: #666666">.</span>item(), kl<span style="color: #666666">.</span>item())
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="more-notes-to-be-added-on-vaes">More notes to be added on VAEs </h2>

<p>Variational diffusion models to be discussed on whiteboard</p>
<!-- ------------------- end of main content --------------- -->
<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2024, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</body>
</html>

