<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week14.do.txt --pygments_html_style=default --html_style=bloodish --html_links_in_new_window --html_output=week14 --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Advanced machine learning and data analysis for the physical sciences">
<title>Advanced machine learning and data analysis for the physical sciences</title>
<style type="text/css">
/* bloodish style */
body {
  font-family: Helvetica, Verdana, Arial, Sans-serif;
  color: #404040;
  background: #ffffff;
}
h1 { font-size: 1.8em; color: #8A0808; }
h2 { font-size: 1.6em; color: #8A0808; }
h3 { font-size: 1.4em; color: #8A0808; }
h4 { font-size: 1.2em; color: #8A0808; }
a { color: #8A0808; text-decoration:none; }
tt { font-family: "Courier New", Courier; }
p { text-indent: 0px; }
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-style: normal; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa; }div.highlight {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    line-height: 1.21429em;
}
div.cell {
    width: 100%;
    padding: 5px 5px 5px 0;
    margin: 0;
    outline: none;
}
div.input {
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.inner_cell {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
    box-flex: 1;
    flex: 1;
}
div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 4px;
    background: #f7f7f7;
    line-height: 1.21429em;
}
div.input_area > div.highlight {
    margin: .4em;
    border: none;
    padding: 0;
    background-color: transparent;
}
div.output_wrapper {
    position: relative;
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
.output {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
div.output_area {
    padding: 0;
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.output_subarea {
    padding: .4em .4em 0 .4em;
    box-flex: 1;
    flex: 1;
}
div.output_text {
    text-align: left;
    color: #000;
    line-height: 1.21429em;
}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #bababa;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #f8f8f8;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_question.png); }
div { text-align: justify; text-justify: inter-word; }
.tab {
  padding-left: 1.5em;
}
div.toc p,a {
  line-height: 1.3;
  margin-top: 1.1;
  margin-bottom: 1.1;
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plans for the week April 22-26, 2024',
               2,
               None,
               'plans-for-the-week-april-22-26-2024'),
              ('Motivation from Kingma and Welling, An Introduction to '
               'Variational Autoencoders, '
               'URL:"https://arxiv.org/abs/1906.02691"',
               2,
               None,
               'motivation-from-kingma-and-welling-an-introduction-to-variational-autoencoders-url-https-arxiv-org-abs-1906-02691'),
              ('Mathematics of  VAEs', 2, None, 'mathematics-of-vaes'),
              ('Using the KL divergence', 2, None, 'using-the-kl-divergence'),
              ('Kullback-Leibler again', 2, None, 'kullback-leibler-again'),
              ('And applying Bayes rule', 2, None, 'and-applying-bayes-rule'),
              ('Rearranging', 2, None, 'rearranging'),
              ('Inferring the probability',
               2,
               None,
               'inferring-the-probability'),
              ('Central equation of VAEs', 2, None, 'central-equation-of-vaes'),
              ('Setting up SGD', 2, None, 'setting-up-sgd'),
              ('More on the SGD', 2, None, 'more-on-the-sgd'),
              ('Simplification', 2, None, 'simplification'),
              ('Terms to compute', 2, None, 'terms-to-compute'),
              ('Computing the gradients', 2, None, 'computing-the-gradients'),
              ('Code examples using Keras',
               2,
               None,
               'code-examples-using-keras'),
              ('Code in PyTorch for VAEs', 2, None, 'code-in-pytorch-for-vaes'),
              ('What is a GAN?', 2, None, 'what-is-a-gan'),
              ('What is a generator network?',
               2,
               None,
               'what-is-a-generator-network'),
              ('And what is a discriminator network?',
               2,
               None,
               'and-what-is-a-discriminator-network'),
              ('Appplications of GANs', 2, None, 'appplications-of-gans'),
              ('Generative Adversarial Networks',
               2,
               None,
               'generative-adversarial-networks'),
              ('Discriminator', 2, None, 'discriminator'),
              ('Zero-sum game', 2, None, 'zero-sum-game'),
              ('Maximizing reward', 2, None, 'maximizing-reward'),
              ('Progression in training', 2, None, 'progression-in-training'),
              ('Deafault choice', 2, None, 'deafault-choice'),
              ('Design of GANs', 2, None, 'design-of-gans'),
              ('More references', 2, None, 'more-references'),
              ('Writing Our First Generative Adversarial Network',
               2,
               None,
               'writing-our-first-generative-adversarial-network'),
              ('Exploring the Latent Space',
               2,
               None,
               'exploring-the-latent-space')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- ------------------- main content ---------------------- -->
<center>
<h1>Advanced machine learning and data analysis for the physical sciences</h1>
</center>  <!-- document title -->

<!-- author(s): Morten Hjorth-Jensen -->
<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>
<!-- institution(s) -->
<center>
[1] <b>Department of Physics and Center for Computing in Science Education, University of Oslo, Norway</b>
</center>
<center>
[2] <b>Department of Physics and Astronomy and Facility for Rare Isotope Beams, Michigan State University, East Lansing, Michigan, USA</b>
</center>
<br>
<center>
<h4>April 23, 2024</h4>
</center> <!-- date -->
<br>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="plans-for-the-week-april-22-26-2024">Plans for the week April 22-26, 2024  </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Deep generative models</b>
<p>
<ol>
<li> Variational Autoencoders (VAE), Mathematics and codes, continuation from last week</li>
<li> Generative Adversarial Networks (GANs)</li>
<li> Reading recommendation:
<ol type="a"></li>
 <li> Goodfellow et al chapter 20.10-20-14</li>
 <li> Calvin Luo <a href="https://calvinyluo.com/2022/08/26/diffusion-tutorial.html" target="_blank"><tt>https://calvinyluo.com/2022/08/26/diffusion-tutorial.html</tt></a></li>
 <li> An Introduction to Variational Autoencoders, by Kingma and Welling, see <a href="https://arxiv.org/abs/1906.02691" target="_blank"><tt>https://arxiv.org/abs/1906.02691</tt></a>
<!-- o "Video of lecture":"" -->
<!-- o <a href="https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/HandwrittenNotes/2024/NotesApril23.pdf" target="_blank">Whiteboard notes</a> --></li>
</ol>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="motivation-from-kingma-and-welling-an-introduction-to-variational-autoencoders-url-https-arxiv-org-abs-1906-02691">Motivation from Kingma and Welling, An Introduction to Variational Autoencoders, <a href="https://arxiv.org/abs/1906.02691" target="_blank"><tt>https://arxiv.org/abs/1906.02691</tt></a> </h2>

<p><em>There are many reasons why generative modeling is attractive. First,
we can express physical laws and constraints into the generative
process while details that we don&#8217;t know or care about, i.e. nuisance
variables, are treated as noise. The resulting models are usually
highly intuitive and interpretable and by testing them against
observations we can confirm or reject our theories about how the world
works.  Another reason for trying to understand the generative process
of data is that it naturally expresses causal relations of the
world. Causal relations have the great advantage that they generalize
much better to new situations than mere correlations. For instance,
once we understand the generative process of an earthquake, we can use
that knowledge both in California and in Chile.</em>
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="mathematics-of-vaes">Mathematics of  VAEs </h2>

<p>We want to train the marginal probability with some latent varrables \( \boldsymbol{h} \)</p>
$$
p(\boldsymbol{x};\boldsymbol{\Theta}) = \int d\boldsymbol{h}p(\boldsymbol{x},\boldsymbol{h};\boldsymbol{\Theta}),
$$

<p>for the continuous version (see previous slides for the discrete variant).</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="using-the-kl-divergence">Using the KL divergence </h2>

<p>In practice, for most \( \boldsymbol{h} \), \( p(\boldsymbol{x}\vert \boldsymbol{h}; \boldsymbol{\Theta}) \)
will be nearly zero, and hence contributes almost nothing to our
estimate of \( p(\boldsymbol{x}) \).
</p>

<p>The key idea behind the variational autoencoder is to attempt to
sample values of \( \boldsymbol{h} \) that are likely to have produced \( \boldsymbol{x} \),
and compute \( p(\boldsymbol{x}) \) just from those.
</p>

<p>This means that we need a new function \( Q(\boldsymbol{h}|\boldsymbol{x}) \) which can
take a value of \( \boldsymbol{x} \) and give us a distribution over \( \boldsymbol{h} \)
values that are likely to produce \( \boldsymbol{x} \).  Hopefully the space of
\( \boldsymbol{h} \) values that are likely under \( Q \) will be much smaller than
the space of all \( \boldsymbol{h} \)'s that are likely under the prior
\( p(\boldsymbol{h}) \).  This lets us, for example, compute \( E_{\boldsymbol{h}\sim
Q}p(\boldsymbol{x}\vert \boldsymbol{h}) \) relatively easily. Note that we drop
\( \boldsymbol{\Theta} \) from here and for notational simplicity.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="kullback-leibler-again">Kullback-Leibler again </h2>

<p>However, if \( \boldsymbol{h} \) is sampled from an arbitrary distribution with
PDF \( Q(\boldsymbol{h}) \), which is not \( \mathcal{N}(0,I) \), then how does that
help us optimize \( p(\boldsymbol{x}) \)?
</p>

<p>The first thing we need to do is relate
\( E_{\boldsymbol{h}\sim Q}P(\boldsymbol{x}\vert \boldsymbol{h}) \) and \( p(\boldsymbol{x}) \).  We will see where \( Q \) comes from later.
</p>

<p>The relationship between \( E_{\boldsymbol{h}\sim Q}p(\boldsymbol{x}\vert \boldsymbol{h}) \) and \( p(\boldsymbol{x}) \) is one of the cornerstones of variational Bayesian methods.
We begin with the definition of Kullback-Leibler divergence (KL divergence or \( \mathcal{D} \)) between \( p(\boldsymbol{h}\vert \boldsymbol{x}) \) and \( Q(\boldsymbol{h}) \), for some arbitrary \( Q \) (which may or may not depend on \( \boldsymbol{x} \)):
</p>
$$
    \mathcal{D}\left[Q(\boldsymbol{h})\|p(\boldsymbol{h}|\boldsymbol{x})\right]=E_{\boldsymbol{h}\sim Q}\left[\log Q(\boldsymbol{h}) - \log p(\boldsymbol{h}|\boldsymbol{x}) \right].
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="and-applying-bayes-rule">And applying Bayes rule </h2>

<p>We can get both \( p(\boldsymbol{x}) \) and \( p(\boldsymbol{x}\vert \boldsymbol{h}) \) into this equation by applying Bayes rule to \( p(\boldsymbol{h}|\boldsymbol{x}) \)</p>
$$
    \mathcal{D}\left[Q(\boldsymbol{h})\|p(\boldsymbol{h}\vert \boldsymbol{x})\right]=E_{\boldsymbol{h}\sim Q}\left[\log Q(\boldsymbol{h}) - \log p(\boldsymbol{x}|\boldsymbol{h}) - \log p(\boldsymbol{h}) \right] + \log p(\boldsymbol{x}).
$$

<p>Here, \( \log p(\boldsymbol{x}) \) comes out of the expectation because it does not depend on \( \boldsymbol{h} \).
Negating both sides, rearranging, and contracting part of \( E_{\boldsymbol{h}\sim Q} \) into a KL-divergence terms yields:
</p>
$$
\log p(\boldsymbol{x}) - \mathcal{D}\left[Q(\boldsymbol{h})\|p(\boldsymbol{h}\vert \boldsymbol{x})\right]=E_{\boldsymbol{h}\sim Q}\left[\log p(\boldsymbol{x}\vert\boldsymbol{h})  \right] - \mathcal{D}\left[Q(\boldsymbol{h})\|P(\boldsymbol{h})\right].
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rearranging">Rearranging </h2>

<p>Using Bayes rule we obtain</p>
$$
E_{\boldsymbol{h}\sim Q}\left[\log p(y_i|\boldsymbol{h},x_i)\right]=E_{\boldsymbol{h}\sim Q}\left[\log p(\boldsymbol{h}|y_i,x_i) - \log p(\boldsymbol{h}|x_i) + \log p(y_i|x_i) \right]
$$

<p>Rearranging the terms and subtracting \( E_{\boldsymbol{h}\sim Q}\log Q(\boldsymbol{h}) \) from both sides gives</p>
$$
\begin{array}{c}
\log P(y_i|x_i) - E_{\boldsymbol{h}\sim Q}\left[\log Q(\boldsymbol{h})-\log p(\boldsymbol{h}|x_i,y_i)\right]=\hspace{10em}\\
\hspace{10em}E_{\boldsymbol{h}\sim Q}\left[\log p(y_i|\boldsymbol{h},x_i)+\log p(\boldsymbol{h}|x_i)-\log Q(\boldsymbol{h})\right]
\end{array}
$$

<p>Note that \( \boldsymbol{x} \) is fixed, and \( Q \) can be \textit{any} distribution, not
just a distribution which does a good job mapping \( \boldsymbol{x} \) to the \( \boldsymbol{h} \)'s
that can produce \( X \).
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="inferring-the-probability">Inferring the probability </h2>

<p>Since we are interested in inferring \( p(\boldsymbol{x}) \), it makes sense to
construct a \( Q \) which \textit{does} depend on \( \boldsymbol{x} \), and in particular,
one which makes \( \mathcal{D}\left[Q(\boldsymbol{h})\|p(\boldsymbol{h}|\boldsymbol{x})\right] \) small
</p>
$$
\log p(\boldsymbol{x}) - \mathcal{D}\left[Q(\boldsymbol{h}|\boldsymbol{x})\|p(\boldsymbol{h}|\boldsymbol{x})\right]=E_{\boldsymbol{h}\sim Q}\left[\log p(\boldsymbol{x}|\boldsymbol{h})  \right] - \mathcal{D}\left[Q(\boldsymbol{h}|\boldsymbol{x})\|p(\boldsymbol{h})\right].
$$

<p>Hence, during training, it makes sense to choose a \( Q \) which will make
\( E_{\boldsymbol{h}\sim Q}[\log Q(\boldsymbol{h})- \) $\log p(\boldsymbol{h}|x_i,y_i)]$ (a
\( \mathcal{D} \)-divergence) small, such that the right hand side is a
close approximation to \( \log p(y_i|y_i) \).
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="central-equation-of-vaes">Central equation of VAEs </h2>

<p>This equation serves as the core of the variational autoencoder, and
it is worth spending some time thinking about what it means.
</p>

<ol>
<li> The left hand side has the quantity we want to maximize, namely \( \log p(\boldsymbol{x}) \) plus an error term.</li>
<li> The right hand side is something we can optimize via stochastic gradient descent given the right choice of \( Q \).</li>
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="setting-up-sgd">Setting up SGD </h2>
<p>So how can we perform stochastic gradient descent?</p>

<p>First we need to be a bit more specific about the form that \( Q(\boldsymbol{h}|\boldsymbol{x}) \)
will take.  The usual choice is to say that
\( Q(\boldsymbol{h}|\boldsymbol{x})=\mathcal{N}(\boldsymbol{h}|\mu(\boldsymbol{x};\vartheta),\Sigma(;\vartheta)) \), where
\( \mu \) and \( \Sigma \) are arbitrary deterministic functions with
parameters \( \vartheta \) that can be learned from data (we will omit
\( \vartheta \) in later equations).  In practice, \( \mu \) and \( \Sigma \) are
again implemented via neural networks, and \( \Sigma \) is constrained to
be a diagonal matrix.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="more-on-the-sgd">More on the SGD </h2>

<p>The name variational &quot;autoencoder&quot; comes from
the fact that \( \mu \) and \( \Sigma \) are &quot;encoding&quot; \( \boldsymbol{x} \) into the latent
space \( \boldsymbol{h} \).  The advantages of this choice are computational, as they
make it clear how to compute the right hand side.  The last
term---\( \mathcal{D}\left[Q(\boldsymbol{h}|\boldsymbol{x})\|p(\boldsymbol{h})\right] \)---is now a KL-divergence
between two multivariate Gaussian distributions, which can be computed
in closed form as:
</p>
$$
\begin{array}{c}
 \mathcal{D}[\mathcal{N}(\mu_0,\Sigma_0) \| \mathcal{N}(\mu_1,\Sigma_1)] = \hspace{20em}\\
  \hspace{5em}\frac{ 1 }{ 2 } \left( \mathrm{tr} \left( \Sigma_1^{-1} \Sigma_0 \right) + \left( \mu_1 - \mu_0\right)^\top \Sigma_1^{-1} ( \mu_1 - \mu_0 ) - k + \log \left( \frac{ \det \Sigma_1 }{ \det \Sigma_0  } \right)  \right)
\end{array}
$$

<p>where \( k \) is the dimensionality of the distribution.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="simplification">Simplification </h2>
<p>In our case, this simplifies to:</p>
$$
\begin{array}{c}
 \mathcal{D}[\mathcal{N}(\mu(X),\Sigma(X)) \| \mathcal{N}(0,I)] = \hspace{20em}\\
\hspace{6em}\frac{ 1 }{ 2 } \left( \mathrm{tr} \left( \Sigma(X) \right) + \left( \mu(X)\right)^\top ( \mu(X) ) - k - \log\det\left(  \Sigma(X)  \right)  \right).
\end{array}
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="terms-to-compute">Terms to compute </h2>

<p>The first term on the right hand side is a bit more tricky.
We could use sampling to estimate \( E_{z\sim Q}\left[\log P(X|z)  \right] \), but getting a good estimate would require passing many samples of \( z \) through \( f \), which would be expensive.
Hence, as is standard in stochastic gradient descent, we take one sample of \( z \) and treat \( \log P(X|z) \) for that \( z \) as an approximation of \( E_{z\sim Q}\left[\log P(X|z)  \right] \).
After all, we are already doing stochastic gradient descent over different values of \( X \) sampled from a dataset \( D \).
The full equation we want to optimize is:
</p>

$$
\begin{array}{c}
    E_{X\sim D}\left[\log P(X) - \mathcal{D}\left[Q(z|X)\|P(z|X)\right]\right]=\hspace{16em}\\
\hspace{10em}E_{X\sim D}\left[E_{z\sim Q}\left[\log P(X|z)  \right] - \mathcal{D}\left[Q(z|X)\|P(z)\right]\right].
\end{array}
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="computing-the-gradients">Computing the gradients </h2>

<p>If we take the gradient of this equation, the gradient symbol can be moved into the expectations.
Therefore, we can sample a single value of \( X \) and a single value of \( z \) from the distribution \( Q(z|X) \), and compute the gradient of:
</p>
$$
\begin{equation}
 \log P(X|z)-\mathcal{D}\left[Q(z|X)\|P(z)\right].
\label{_auto1}
\end{equation}
$$

<p>We can then average the gradient of this function over arbitrarily many samples of \( X \) and \( z \), and the result converges to the gradient.</p>

<p>There is, however, a significant problem
\( E_{z\sim Q}\left[\log P(X|z)  \right] \) depends not just on the parameters of \( P \), but also on the parameters of \( Q \).
</p>

<p>In order to make VAEs work, it is essential to drive \( Q \) to produce codes for \( X \) that \( P \) can reliably decode.  </p>
$$
 E_{X\sim D}\left[E_{\epsilon\sim\mathcal{N}(0,I)}[\log P(X|z=\mu(X)+\Sigma^{1/2}(X)*\epsilon)]-\mathcal{D}\left[Q(z|X)\|P(z)\right]\right].
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="code-examples-using-keras">Code examples using Keras </h2>

<p>Code taken from  <a href="https://keras.io/examples/generative/vae/" target="_blank"><tt>https://keras.io/examples/generative/vae/</tt></a></p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">Title: Variational AutoEncoder</span>
<span style="color: #BA2121; font-style: italic">Author: [fchollet](https://twitter.com/fchollet)</span>
<span style="color: #BA2121; font-style: italic">Date created: 2020/05/03</span>
<span style="color: #BA2121; font-style: italic">Last modified: 2023/11/22</span>
<span style="color: #BA2121; font-style: italic">Description: Convolutional Variational AutoEncoder (VAE) trained on MNIST digits.</span>
<span style="color: #BA2121; font-style: italic">Accelerator: GPU</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Setup</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>

os<span style="color: #666666">.</span>environ[<span style="color: #BA2121">&quot;KERAS_BACKEND&quot;</span>] <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;tensorflow&quot;</span>

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">tf</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">keras</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">keras</span> <span style="color: #008000; font-weight: bold">import</span> layers

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Create a sampling layer</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>


<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">Sampling</span>(layers<span style="color: #666666">.</span>Layer):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.&quot;&quot;&quot;</span>

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">call</span>(<span style="color: #008000">self</span>, inputs):
        z_mean, z_log_var <span style="color: #666666">=</span> inputs
        batch <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>shape(z_mean)[<span style="color: #666666">0</span>]
        dim <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>shape(z_mean)[<span style="color: #666666">1</span>]
        epsilon <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(shape<span style="color: #666666">=</span>(batch, dim))
        <span style="color: #008000; font-weight: bold">return</span> z_mean <span style="color: #666666">+</span> tf<span style="color: #666666">.</span>exp(<span style="color: #666666">0.5</span> <span style="color: #666666">*</span> z_log_var) <span style="color: #666666">*</span> epsilon


<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Build the encoder</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

latent_dim <span style="color: #666666">=</span> <span style="color: #666666">2</span>

encoder_inputs <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>Input(shape<span style="color: #666666">=</span>(<span style="color: #666666">28</span>, <span style="color: #666666">28</span>, <span style="color: #666666">1</span>))
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Conv2D(<span style="color: #666666">32</span>, <span style="color: #666666">3</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>, strides<span style="color: #666666">=2</span>, padding<span style="color: #666666">=</span><span style="color: #BA2121">&quot;same&quot;</span>)(encoder_inputs)
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Conv2D(<span style="color: #666666">64</span>, <span style="color: #666666">3</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>, strides<span style="color: #666666">=2</span>, padding<span style="color: #666666">=</span><span style="color: #BA2121">&quot;same&quot;</span>)(x)
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Flatten()(x)
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Dense(<span style="color: #666666">16</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>)(x)
z_mean <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Dense(latent_dim, name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;z_mean&quot;</span>)(x)
z_log_var <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Dense(latent_dim, name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;z_log_var&quot;</span>)(x)
z <span style="color: #666666">=</span> Sampling()([z_mean, z_log_var])
encoder <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>Model(encoder_inputs, [z_mean, z_log_var, z], name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;encoder&quot;</span>)
encoder<span style="color: #666666">.</span>summary()

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Build the decoder</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

latent_inputs <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>Input(shape<span style="color: #666666">=</span>(latent_dim,))
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Dense(<span style="color: #666666">7</span> <span style="color: #666666">*</span> <span style="color: #666666">7</span> <span style="color: #666666">*</span> <span style="color: #666666">64</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>)(latent_inputs)
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Reshape((<span style="color: #666666">7</span>, <span style="color: #666666">7</span>, <span style="color: #666666">64</span>))(x)
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Conv2DTranspose(<span style="color: #666666">64</span>, <span style="color: #666666">3</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>, strides<span style="color: #666666">=2</span>, padding<span style="color: #666666">=</span><span style="color: #BA2121">&quot;same&quot;</span>)(x)
x <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Conv2DTranspose(<span style="color: #666666">32</span>, <span style="color: #666666">3</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>, strides<span style="color: #666666">=2</span>, padding<span style="color: #666666">=</span><span style="color: #BA2121">&quot;same&quot;</span>)(x)
decoder_outputs <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Conv2DTranspose(<span style="color: #666666">1</span>, <span style="color: #666666">3</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;sigmoid&quot;</span>, padding<span style="color: #666666">=</span><span style="color: #BA2121">&quot;same&quot;</span>)(x)
decoder <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>Model(latent_inputs, decoder_outputs, name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;decoder&quot;</span>)
decoder<span style="color: #666666">.</span>summary()

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Define the VAE as a `Model` with a custom `train_step`</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>


<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">VAE</span>(keras<span style="color: #666666">.</span>Model):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, encoder, decoder, <span style="color: #666666">**</span>kwargs):
        <span style="color: #008000">super</span>()<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>(<span style="color: #666666">**</span>kwargs)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>encoder <span style="color: #666666">=</span> encoder
        <span style="color: #008000">self</span><span style="color: #666666">.</span>decoder <span style="color: #666666">=</span> decoder
        <span style="color: #008000">self</span><span style="color: #666666">.</span>total_loss_tracker <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>Mean(name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;total_loss&quot;</span>)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>reconstruction_loss_tracker <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>Mean(
            name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;reconstruction_loss&quot;</span>
        )
        <span style="color: #008000">self</span><span style="color: #666666">.</span>kl_loss_tracker <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>Mean(name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;kl_loss&quot;</span>)

    <span style="color: #AA22FF">@property</span>
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">metrics</span>(<span style="color: #008000">self</span>):
        <span style="color: #008000; font-weight: bold">return</span> [
            <span style="color: #008000">self</span><span style="color: #666666">.</span>total_loss_tracker,
            <span style="color: #008000">self</span><span style="color: #666666">.</span>reconstruction_loss_tracker,
            <span style="color: #008000">self</span><span style="color: #666666">.</span>kl_loss_tracker,
        ]

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">train_step</span>(<span style="color: #008000">self</span>, data):
        <span style="color: #008000; font-weight: bold">with</span> tf<span style="color: #666666">.</span>GradientTape() <span style="color: #008000; font-weight: bold">as</span> tape:
            z_mean, z_log_var, z <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>encoder(data)
            reconstruction <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>decoder(z)
            reconstruction_loss <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>reduce_mean(
                tf<span style="color: #666666">.</span>reduce_sum(
                    keras<span style="color: #666666">.</span>losses<span style="color: #666666">.</span>binary_crossentropy(data, reconstruction),
                    axis<span style="color: #666666">=</span>(<span style="color: #666666">1</span>, <span style="color: #666666">2</span>),
                )
            )
            kl_loss <span style="color: #666666">=</span> <span style="color: #666666">-0.5</span> <span style="color: #666666">*</span> (<span style="color: #666666">1</span> <span style="color: #666666">+</span> z_log_var <span style="color: #666666">-</span> tf<span style="color: #666666">.</span>square(z_mean) <span style="color: #666666">-</span> tf<span style="color: #666666">.</span>exp(z_log_var))
            kl_loss <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>reduce_mean(tf<span style="color: #666666">.</span>reduce_sum(kl_loss, axis<span style="color: #666666">=1</span>))
            total_loss <span style="color: #666666">=</span> reconstruction_loss <span style="color: #666666">+</span> kl_loss
        grads <span style="color: #666666">=</span> tape<span style="color: #666666">.</span>gradient(total_loss, <span style="color: #008000">self</span><span style="color: #666666">.</span>trainable_weights)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>optimizer<span style="color: #666666">.</span>apply_gradients(<span style="color: #008000">zip</span>(grads, <span style="color: #008000">self</span><span style="color: #666666">.</span>trainable_weights))
        <span style="color: #008000">self</span><span style="color: #666666">.</span>total_loss_tracker<span style="color: #666666">.</span>update_state(total_loss)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>reconstruction_loss_tracker<span style="color: #666666">.</span>update_state(reconstruction_loss)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>kl_loss_tracker<span style="color: #666666">.</span>update_state(kl_loss)
        <span style="color: #008000; font-weight: bold">return</span> {
            <span style="color: #BA2121">&quot;loss&quot;</span>: <span style="color: #008000">self</span><span style="color: #666666">.</span>total_loss_tracker<span style="color: #666666">.</span>result(),
            <span style="color: #BA2121">&quot;reconstruction_loss&quot;</span>: <span style="color: #008000">self</span><span style="color: #666666">.</span>reconstruction_loss_tracker<span style="color: #666666">.</span>result(),
            <span style="color: #BA2121">&quot;kl_loss&quot;</span>: <span style="color: #008000">self</span><span style="color: #666666">.</span>kl_loss_tracker<span style="color: #666666">.</span>result(),
        }


<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Train the VAE</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

(x_train, _), (x_test, _) <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>datasets<span style="color: #666666">.</span>mnist<span style="color: #666666">.</span>load_data()
mnist_digits <span style="color: #666666">=</span> np<span style="color: #666666">.</span>concatenate([x_train, x_test], axis<span style="color: #666666">=0</span>)
mnist_digits <span style="color: #666666">=</span> np<span style="color: #666666">.</span>expand_dims(mnist_digits, <span style="color: #666666">-1</span>)<span style="color: #666666">.</span>astype(<span style="color: #BA2121">&quot;float32&quot;</span>) <span style="color: #666666">/</span> <span style="color: #666666">255</span>

vae <span style="color: #666666">=</span> VAE(encoder, decoder)
vae<span style="color: #666666">.</span>compile(optimizer<span style="color: #666666">=</span>keras<span style="color: #666666">.</span>optimizers<span style="color: #666666">.</span>Adam())
vae<span style="color: #666666">.</span>fit(mnist_digits, epochs<span style="color: #666666">=30</span>, batch_size<span style="color: #666666">=128</span>)

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Display a grid of sampled digits</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">plot_latent_space</span>(vae, n<span style="color: #666666">=30</span>, figsize<span style="color: #666666">=15</span>):
    <span style="color: #408080; font-style: italic"># display a n*n 2D manifold of digits</span>
    digit_size <span style="color: #666666">=</span> <span style="color: #666666">28</span>
    scale <span style="color: #666666">=</span> <span style="color: #666666">1.0</span>
    figure <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((digit_size <span style="color: #666666">*</span> n, digit_size <span style="color: #666666">*</span> n))
    <span style="color: #408080; font-style: italic"># linearly spaced coordinates corresponding to the 2D plot</span>
    <span style="color: #408080; font-style: italic"># of digit classes in the latent space</span>
    grid_x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-</span>scale, scale, n)
    grid_y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-</span>scale, scale, n)[::<span style="color: #666666">-1</span>]

    <span style="color: #008000; font-weight: bold">for</span> i, yi <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(grid_y):
        <span style="color: #008000; font-weight: bold">for</span> j, xi <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(grid_x):
            z_sample <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([[xi, yi]])
            x_decoded <span style="color: #666666">=</span> vae<span style="color: #666666">.</span>decoder<span style="color: #666666">.</span>predict(z_sample, verbose<span style="color: #666666">=0</span>)
            digit <span style="color: #666666">=</span> x_decoded[<span style="color: #666666">0</span>]<span style="color: #666666">.</span>reshape(digit_size, digit_size)
            figure[
                i <span style="color: #666666">*</span> digit_size : (i <span style="color: #666666">+</span> <span style="color: #666666">1</span>) <span style="color: #666666">*</span> digit_size,
                j <span style="color: #666666">*</span> digit_size : (j <span style="color: #666666">+</span> <span style="color: #666666">1</span>) <span style="color: #666666">*</span> digit_size,
            ] <span style="color: #666666">=</span> digit

    plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(figsize, figsize))
    start_range <span style="color: #666666">=</span> digit_size <span style="color: #666666">//</span> <span style="color: #666666">2</span>
    end_range <span style="color: #666666">=</span> n <span style="color: #666666">*</span> digit_size <span style="color: #666666">+</span> start_range
    pixel_range <span style="color: #666666">=</span> np<span style="color: #666666">.</span>arange(start_range, end_range, digit_size)
    sample_range_x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>round(grid_x, <span style="color: #666666">1</span>)
    sample_range_y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>round(grid_y, <span style="color: #666666">1</span>)
    plt<span style="color: #666666">.</span>xticks(pixel_range, sample_range_x)
    plt<span style="color: #666666">.</span>yticks(pixel_range, sample_range_y)
    plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;z[0]&quot;</span>)
    plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;z[1]&quot;</span>)
    plt<span style="color: #666666">.</span>imshow(figure, cmap<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Greys_r&quot;</span>)
    plt<span style="color: #666666">.</span>show()


plot_latent_space(vae)

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">## Display how the latent space clusters different digit classes</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">plot_label_clusters</span>(vae, data, labels):
    <span style="color: #408080; font-style: italic"># display a 2D plot of the digit classes in the latent space</span>
    z_mean, _, _ <span style="color: #666666">=</span> vae<span style="color: #666666">.</span>encoder<span style="color: #666666">.</span>predict(data, verbose<span style="color: #666666">=0</span>)
    plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">12</span>, <span style="color: #666666">10</span>))
    plt<span style="color: #666666">.</span>scatter(z_mean[:, <span style="color: #666666">0</span>], z_mean[:, <span style="color: #666666">1</span>], c<span style="color: #666666">=</span>labels)
    plt<span style="color: #666666">.</span>colorbar()
    plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;z[0]&quot;</span>)
    plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;z[1]&quot;</span>)
    plt<span style="color: #666666">.</span>show()


(x_train, y_train), _ <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>datasets<span style="color: #666666">.</span>mnist<span style="color: #666666">.</span>load_data()
x_train <span style="color: #666666">=</span> np<span style="color: #666666">.</span>expand_dims(x_train, <span style="color: #666666">-1</span>)<span style="color: #666666">.</span>astype(<span style="color: #BA2121">&quot;float32&quot;</span>) <span style="color: #666666">/</span> <span style="color: #666666">255</span>

plot_label_clusters(vae, x_train, y_train)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="code-in-pytorch-for-vaes">Code in PyTorch for VAEs </h2>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torch.autograd</span> <span style="color: #008000; font-weight: bold">import</span> Variable
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.nn.functional</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">F</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torchvision</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torchvision</span> <span style="color: #008000; font-weight: bold">import</span> transforms
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.optim</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">optim</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torch</span> <span style="color: #008000; font-weight: bold">import</span> nn
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torch</span> <span style="color: #008000; font-weight: bold">import</span> distributions

<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">Encoder</span>(torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Module):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, D_in, H, latent_size):
        <span style="color: #008000">super</span>(Encoder, <span style="color: #008000">self</span>)<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
        <span style="color: #008000">self</span><span style="color: #666666">.</span>linear1 <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Linear(D_in, H)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>linear2 <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Linear(H, H)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>enc_mu <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Linear(H, latent_size)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>enc_log_sigma <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Linear(H, latent_size)

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, x):
        x <span style="color: #666666">=</span> F<span style="color: #666666">.</span>relu(<span style="color: #008000">self</span><span style="color: #666666">.</span>linear1(x))
        x <span style="color: #666666">=</span> F<span style="color: #666666">.</span>relu(<span style="color: #008000">self</span><span style="color: #666666">.</span>linear2(x))
        mu <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>enc_mu(x)
        log_sigma <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>enc_log_sigma(x)
        sigma <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>exp(log_sigma)
        <span style="color: #008000; font-weight: bold">return</span> torch<span style="color: #666666">.</span>distributions<span style="color: #666666">.</span>Normal(loc<span style="color: #666666">=</span>mu, scale<span style="color: #666666">=</span>sigma)


<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">Decoder</span>(torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Module):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, D_in, H, D_out):
        <span style="color: #008000">super</span>(Decoder, <span style="color: #008000">self</span>)<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
        <span style="color: #008000">self</span><span style="color: #666666">.</span>linear1 <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Linear(D_in, H)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>linear2 <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Linear(H, D_out)
        

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, x):
        x <span style="color: #666666">=</span> F<span style="color: #666666">.</span>relu(<span style="color: #008000">self</span><span style="color: #666666">.</span>linear1(x))
        mu <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>tanh(<span style="color: #008000">self</span><span style="color: #666666">.</span>linear2(x))
        <span style="color: #008000; font-weight: bold">return</span> torch<span style="color: #666666">.</span>distributions<span style="color: #666666">.</span>Normal(mu, torch<span style="color: #666666">.</span>ones_like(mu))

<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">VAE</span>(torch<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>Module):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, encoder, decoder):
        <span style="color: #008000">super</span>(VAE, <span style="color: #008000">self</span>)<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
        <span style="color: #008000">self</span><span style="color: #666666">.</span>encoder <span style="color: #666666">=</span> encoder
        <span style="color: #008000">self</span><span style="color: #666666">.</span>decoder <span style="color: #666666">=</span> decoder

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, state):
        q_z <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>encoder(state)
        z <span style="color: #666666">=</span> q_z<span style="color: #666666">.</span>rsample()
        <span style="color: #008000; font-weight: bold">return</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>decoder(z), q_z


transform <span style="color: #666666">=</span> transforms<span style="color: #666666">.</span>Compose(
    [transforms<span style="color: #666666">.</span>ToTensor(),
     <span style="color: #408080; font-style: italic"># Normalize the images to be -0.5, 0.5</span>
     transforms<span style="color: #666666">.</span>Normalize(<span style="color: #666666">0.5</span>, <span style="color: #666666">1</span>)]
    )
mnist <span style="color: #666666">=</span> torchvision<span style="color: #666666">.</span>datasets<span style="color: #666666">.</span>MNIST(<span style="color: #BA2121">&#39;./&#39;</span>, download<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, transform<span style="color: #666666">=</span>transform)

input_dim <span style="color: #666666">=</span> <span style="color: #666666">28</span> <span style="color: #666666">*</span> <span style="color: #666666">28</span>
batch_size <span style="color: #666666">=</span> <span style="color: #666666">128</span>
num_epochs <span style="color: #666666">=</span> <span style="color: #666666">100</span>
learning_rate <span style="color: #666666">=</span> <span style="color: #666666">0.001</span>
hidden_size <span style="color: #666666">=</span> <span style="color: #666666">512</span>
latent_size <span style="color: #666666">=</span> <span style="color: #666666">8</span>

<span style="color: #008000; font-weight: bold">if</span> torch<span style="color: #666666">.</span>cuda<span style="color: #666666">.</span>is_available():
    device <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>device(<span style="color: #BA2121">&#39;cuda&#39;</span>)
<span style="color: #008000; font-weight: bold">else</span>:
    device <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>device(<span style="color: #BA2121">&#39;cpu&#39;</span>)

dataloader <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>utils<span style="color: #666666">.</span>data<span style="color: #666666">.</span>DataLoader(
    mnist, batch_size<span style="color: #666666">=</span>batch_size,
    shuffle<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, 
    pin_memory<span style="color: #666666">=</span>torch<span style="color: #666666">.</span>cuda<span style="color: #666666">.</span>is_available())

<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Number of samples: &#39;</span>, <span style="color: #008000">len</span>(mnist))

encoder <span style="color: #666666">=</span> Encoder(input_dim, hidden_size, latent_size)
decoder <span style="color: #666666">=</span> Decoder(latent_size, hidden_size, input_dim)

vae <span style="color: #666666">=</span> VAE(encoder, decoder)<span style="color: #666666">.</span>to(device)

optimizer <span style="color: #666666">=</span> optim<span style="color: #666666">.</span>Adam(vae<span style="color: #666666">.</span>parameters(), lr<span style="color: #666666">=</span>learning_rate)
<span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(num_epochs):
    <span style="color: #008000; font-weight: bold">for</span> data <span style="color: #AA22FF; font-weight: bold">in</span> dataloader:
        inputs, _ <span style="color: #666666">=</span> data
        inputs <span style="color: #666666">=</span> inputs<span style="color: #666666">.</span>view(<span style="color: #666666">-1</span>, input_dim)<span style="color: #666666">.</span>to(device)
        optimizer<span style="color: #666666">.</span>zero_grad()
        p_x, q_z <span style="color: #666666">=</span> vae(inputs)
        log_likelihood <span style="color: #666666">=</span> p_x<span style="color: #666666">.</span>log_prob(inputs)<span style="color: #666666">.</span>sum(<span style="color: #666666">-1</span>)<span style="color: #666666">.</span>mean()
        kl <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>distributions<span style="color: #666666">.</span>kl_divergence(
            q_z, 
            torch<span style="color: #666666">.</span>distributions<span style="color: #666666">.</span>Normal(<span style="color: #666666">0</span>, <span style="color: #666666">1.</span>)
        )<span style="color: #666666">.</span>sum(<span style="color: #666666">-1</span>)<span style="color: #666666">.</span>mean()
        loss <span style="color: #666666">=</span> <span style="color: #666666">-</span>(log_likelihood <span style="color: #666666">-</span> kl)
        loss<span style="color: #666666">.</span>backward()
        optimizer<span style="color: #666666">.</span>step()
        l <span style="color: #666666">=</span> loss<span style="color: #666666">.</span>item()
    <span style="color: #008000">print</span>(epoch, l, log_likelihood<span style="color: #666666">.</span>item(), kl<span style="color: #666666">.</span>item())
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="what-is-a-gan">What is a GAN? </h2>

<p>A GAN is a deep neural network which consists of two networks, a
so-called generator network and a discriminating network, or just
discriminator. Through several iterations of generation and
discrimination, the idea is that these networks will train each other,
while also trying to outsmart each other.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="what-is-a-generator-network">What is a generator network? </h2>

<p>A generator network is often a deep network which uses existing data
to generate new data (from for example simulations of physical
systems, imagesm video, audio and more) from randomly generated
inputs, the so-called latent space. Training the network allows us to
generate say new data, images etc. As an example a generator network
could for example be a Boltzmann machine as discussed earlier. This
machine is trained to produce for example a quantum mechanical
probability distribution.
</p>

<p>It can a simple neural network with an input layer and an output layer and a given number of hidden layers.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="and-what-is-a-discriminator-network">And what is a discriminator network? </h2>

<p>A discriminator tries to distinguish between real data and those generated by the abovementioned generator.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="appplications-of-gans">Appplications of GANs </h2>

<p>There are exteremely many applications of GANs</p>
<ol>
<li> Image generation</li>
<li> Text-to-image analysis</li>
<li> Face-aging</li>
<li> Image-to-image translation</li>
<li> Video synthesis</li>
<li> High-resolution image generation</li>
<li> Completing missing parts of images and much more</li>
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="generative-adversarial-networks">Generative Adversarial Networks </h2>

<p><b>Generative Adversarial Networks</b> are a type of unsupervised machine learning
algorithm proposed by Goodfellow et. al, see <a href="https://arxiv.org/pdf/1406.2661.pdf" target="_blank"><tt>https://arxiv.org/pdf/1406.2661.pdf</tt></a>
in 2014 (Read the paper first it's only 6 pages). The simplest formulation of
the model is based on a game theoretic approach, <em>zero sum game</em>, where we pit
two neural networks against one another. We define two rival networks, one
generator \( g \), and one discriminator \( d \). The generator directly produces
samples
</p>
$$
    x = g(z; \theta^{(g)}).
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="discriminator">Discriminator </h2>

<p>The discriminator attempts to distinguish between samples drawn from the
training data and samples drawn from the generator. In other words, it tries to
tell the difference between the fake data produced by \( g \) and the actual data
samples we want to do prediction on. The discriminator outputs a probability
value given by
</p>

$$
    d(x; \theta^{(d)}).
$$

<p>indicating the probability that \( x \) is a real training example rather than a
fake sample the generator has generated.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="zero-sum-game">Zero-sum game </h2>

<p>The simplest way to formulate the
learning process in a generative adversarial network is a zero-sum game, in
which a function
</p>

$$
    v(\theta^{(g)}, \theta^{(d)}),
$$

<p>determines the reward for the discriminator, while the generator gets the
conjugate reward
</p>

$$
    -v(\theta^{(g)}, \theta^{(d)})
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="maximizing-reward">Maximizing reward </h2>

<p>During learning both of the networks maximize their own reward function, so that
the generator gets better and better at tricking the discriminator, while the
discriminator gets better and better at telling the difference between the fake
and real data. The generator and discriminator alternate on which one trains at
one time (i.e. for one epoch). In other words, we keep the generator constant
and train the discriminator, then we keep the discriminator constant to train
the generator and repeat. It is this back and forth dynamic which lets GANs
tackle otherwise intractable generative problems. As the generator improves with
 training, the discriminator's performance gets worse because it cannot easily
 tell the difference between real and fake. If the generator ends up succeeding
 perfectly, the the discriminator will do no better than random guessing i.e.
 50\%.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="progression-in-training">Progression in training </h2>

<p>This progression in the training poses a problem for the convergence
 criteria for GANs. The discriminator feedback gets less meaningful over time,
 if we continue training after this point then the generator is effectively
 training on junk data which can undo the learning up to that point. Therefore,
 we stop training when the discriminator starts outputting \( 1/2 \) everywhere.
 At convergence we have
</p>

$$
    g^* = \underset{g}{\mathrm{argmin}}\hspace{2pt}
          \underset{d}{\mathrm{max}}v(\theta^{(g)}, \theta^{(d)}),
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="deafault-choice">Deafault choice </h2>
<p>The default choice for \( v \) is</p>
$$
    v(\theta^{(g)}, \theta^{(d)}) = \mathbb{E}_{x\sim p_\mathrm{data}}\log d(x)
                                  + \mathbb{E}_{x\sim p_\mathrm{model}}
                                  \log (1 - d(x)).
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="design-of-gans">Design of GANs </h2>
<p>The main motivation for the design of GANs is that the learning process requires
neither approximate inference (variational autoencoders for example) nor
approximation of a partition function. In the case where
</p>
$$
    \underset{d}{\mathrm{max}}v(\theta^{(g)}, \theta^{(d)})
$$

<p>is convex in \( \theta^{(g)} \) then the procedure is guaranteed to converge and is
asymptotically consistent
( <a href="https://arxiv.org/pdf/1804.09139.pdf" target="_blank">Seth Lloyd on QuGANs</a>  ). This is in
general not the case and it is possible to get situations where the training
process never converges because the generator and discriminator chase one
another around in the parameter space indefinitely.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="more-references">More references </h2>

<p>A much deeper discussion on
the currently open research problem of GAN convergence is available
from <a href="https://www.deeplearningbook.org/contents/generative_models.html" target="_blank"><tt>https://www.deeplearningbook.org/contents/generative_models.html</tt></a>. To
anyone interested in learning more about GANs it is a highly recommended read.
Direct quote: <b>In this best-performing formulation, the generator aims to
increase the log probability that the discriminator makes a mistake, rather than
aiming to decrease the log probability that the discriminator makes the correct
prediction.</b> Another interesting read can be found at <a href="https://arxiv.org/abs/1701.00160" target="_blank"><tt>https://arxiv.org/abs/1701.00160</tt></a>.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="writing-our-first-generative-adversarial-network">Writing Our First Generative Adversarial Network </h2>

<p>This part is best seen using the jupyter-notebook.</p>

<p>Let us implement a GAN in tensorflow. We will study
the performance of our GAN on the MNIST dataset. This code is based on and
adapted from the Google tutorial at <a href="https://www.tensorflow.org/tutorials/generative/dcgan" target="_blank"><tt>https://www.tensorflow.org/tutorials/generative/dcgan</tt></a>
</p>

<p>First we import our libraries</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">time</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">tf</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras</span> <span style="color: #008000; font-weight: bold">import</span> layers
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.utils</span> <span style="color: #008000; font-weight: bold">import</span> plot_model
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Next we define our hyperparameters and import our data the usual way</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">BUFFER_SIZE <span style="color: #666666">=</span> <span style="color: #666666">60000</span>
BATCH_SIZE <span style="color: #666666">=</span> <span style="color: #666666">256</span>
EPOCHS <span style="color: #666666">=</span> <span style="color: #666666">30</span>

data <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>datasets<span style="color: #666666">.</span>mnist<span style="color: #666666">.</span>load_data()
(train_images, train_labels), (test_images, test_labels) <span style="color: #666666">=</span> data
train_images <span style="color: #666666">=</span> np<span style="color: #666666">.</span>reshape(train_images, (train_images<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>],
                                         <span style="color: #666666">28</span>,
                                         <span style="color: #666666">28</span>,
                                         <span style="color: #666666">1</span>))<span style="color: #666666">.</span>astype(<span style="color: #BA2121">&#39;float32&#39;</span>)

<span style="color: #408080; font-style: italic"># we normalize between -1 and 1</span>
train_images <span style="color: #666666">=</span> (train_images <span style="color: #666666">-</span> <span style="color: #666666">127.5</span>) <span style="color: #666666">/</span> <span style="color: #666666">127.5</span>
training_dataset <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>data<span style="color: #666666">.</span>Dataset<span style="color: #666666">.</span>from_tensor_slices(
                      train_images)<span style="color: #666666">.</span>shuffle(BUFFER_SIZE)<span style="color: #666666">.</span>batch(BATCH_SIZE)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Let's have a quick look</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">plt<span style="color: #666666">.</span>imshow(train_images[<span style="color: #666666">0</span>], cmap<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Greys&#39;</span>)
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Now we define our two models. This is where the 'magic' happens. There are a
huge amount of possible formulations for both models. A lot of engineering and
trial and error can be done here to try to produce better performing models. For
more advanced GANs this is by far the step where you can 'make or break' a
model.
</p>

<p>We start with the generator. As stated in the introductory text the generator
\( g \) upsamples from a random sample to the shape of what we want to predict. In
our case we are trying to predict MNIST images (\( 28\times 28 \) pixels).
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">generator_model</span>():
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    The generator uses upsampling layers tf.keras.layers.Conv2DTranspose() to</span>
<span style="color: #BA2121; font-style: italic">    produce an image from a random seed. We start with a Dense layer taking this</span>
<span style="color: #BA2121; font-style: italic">    random sample as an input and subsequently upsample through multiple</span>
<span style="color: #BA2121; font-style: italic">    convolutional layers.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>

    <span style="color: #408080; font-style: italic"># we define our model</span>
    model <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>Sequential()


    <span style="color: #408080; font-style: italic"># adding our input layer. Dense means that every neuron is connected and</span>
    <span style="color: #408080; font-style: italic"># the input shape is the shape of our random noise. The units need to match</span>
    <span style="color: #408080; font-style: italic"># in some sense the upsampling strides to reach our desired output shape.</span>
    <span style="color: #408080; font-style: italic"># we are using 100 random numbers as our seed</span>
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>Dense(units<span style="color: #666666">=7*7*</span>BATCH_SIZE,
                           use_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,
                           input_shape<span style="color: #666666">=</span>(<span style="color: #666666">100</span>, )))
    <span style="color: #408080; font-style: italic"># we normalize the output form the Dense layer</span>
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>BatchNormalization())
    <span style="color: #408080; font-style: italic"># and add an activation function to our &#39;layer&#39;. LeakyReLU avoids vanishing</span>
    <span style="color: #408080; font-style: italic"># gradient problem</span>
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>LeakyReLU())
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>Reshape((<span style="color: #666666">7</span>, <span style="color: #666666">7</span>, BATCH_SIZE)))
    <span style="color: #008000; font-weight: bold">assert</span> model<span style="color: #666666">.</span>output_shape <span style="color: #666666">==</span> (<span style="color: #008000; font-weight: bold">None</span>, <span style="color: #666666">7</span>, <span style="color: #666666">7</span>, BATCH_SIZE)
    <span style="color: #408080; font-style: italic"># even though we just added four keras layers we think of everything above</span>
    <span style="color: #408080; font-style: italic"># as &#39;one&#39; layer</span>

    <span style="color: #408080; font-style: italic"># next we add our upscaling convolutional layers</span>
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>Conv2DTranspose(filters<span style="color: #666666">=128</span>,
                                     kernel_size<span style="color: #666666">=</span>(<span style="color: #666666">5</span>, <span style="color: #666666">5</span>),
                                     strides<span style="color: #666666">=</span>(<span style="color: #666666">1</span>, <span style="color: #666666">1</span>),
                                     padding<span style="color: #666666">=</span><span style="color: #BA2121">&#39;same&#39;</span>,
                                     use_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>))
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>BatchNormalization())
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>LeakyReLU())
    <span style="color: #008000; font-weight: bold">assert</span> model<span style="color: #666666">.</span>output_shape <span style="color: #666666">==</span> (<span style="color: #008000; font-weight: bold">None</span>, <span style="color: #666666">7</span>, <span style="color: #666666">7</span>, <span style="color: #666666">128</span>)

    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>Conv2DTranspose(filters<span style="color: #666666">=64</span>,
                                     kernel_size<span style="color: #666666">=</span>(<span style="color: #666666">5</span>, <span style="color: #666666">5</span>),
                                     strides<span style="color: #666666">=</span>(<span style="color: #666666">2</span>, <span style="color: #666666">2</span>),
                                     padding<span style="color: #666666">=</span><span style="color: #BA2121">&#39;same&#39;</span>,
                                     use_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>))
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>BatchNormalization())
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>LeakyReLU())
    <span style="color: #008000; font-weight: bold">assert</span> model<span style="color: #666666">.</span>output_shape <span style="color: #666666">==</span> (<span style="color: #008000; font-weight: bold">None</span>, <span style="color: #666666">14</span>, <span style="color: #666666">14</span>, <span style="color: #666666">64</span>)

    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>Conv2DTranspose(filters<span style="color: #666666">=1</span>,
                                     kernel_size<span style="color: #666666">=</span>(<span style="color: #666666">5</span>, <span style="color: #666666">5</span>),
                                     strides<span style="color: #666666">=</span>(<span style="color: #666666">2</span>, <span style="color: #666666">2</span>),
                                     padding<span style="color: #666666">=</span><span style="color: #BA2121">&#39;same&#39;</span>,
                                     use_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,
                                     activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;tanh&#39;</span>))
    <span style="color: #008000; font-weight: bold">assert</span> model<span style="color: #666666">.</span>output_shape <span style="color: #666666">==</span> (<span style="color: #008000; font-weight: bold">None</span>, <span style="color: #666666">28</span>, <span style="color: #666666">28</span>, <span style="color: #666666">1</span>)

    <span style="color: #008000; font-weight: bold">return</span> model
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>And there we have our 'simple' generator model. Now we move on to defining our
discriminator model \( d \), which is a convolutional neural network based image
classifier.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">discriminator_model</span>():
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    The discriminator is a convolutional neural network based image classifier</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>

    <span style="color: #408080; font-style: italic"># we define our model</span>
    model <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>Sequential()
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>Conv2D(filters<span style="color: #666666">=64</span>,
                            kernel_size<span style="color: #666666">=</span>(<span style="color: #666666">5</span>, <span style="color: #666666">5</span>),
                            strides<span style="color: #666666">=</span>(<span style="color: #666666">2</span>, <span style="color: #666666">2</span>),
                            padding<span style="color: #666666">=</span><span style="color: #BA2121">&#39;same&#39;</span>,
                            input_shape<span style="color: #666666">=</span>[<span style="color: #666666">28</span>, <span style="color: #666666">28</span>, <span style="color: #666666">1</span>]))
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>LeakyReLU())
    <span style="color: #408080; font-style: italic"># adding a dropout layer as you do in conv-nets</span>
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>Dropout(<span style="color: #666666">0.3</span>))


    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>Conv2D(filters<span style="color: #666666">=128</span>,
                            kernel_size<span style="color: #666666">=</span>(<span style="color: #666666">5</span>, <span style="color: #666666">5</span>),
                            strides<span style="color: #666666">=</span>(<span style="color: #666666">2</span>, <span style="color: #666666">2</span>),
                            padding<span style="color: #666666">=</span><span style="color: #BA2121">&#39;same&#39;</span>))
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>LeakyReLU())
    <span style="color: #408080; font-style: italic"># adding a dropout layer as you do in conv-nets</span>
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>Dropout(<span style="color: #666666">0.3</span>))

    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>Flatten())
    model<span style="color: #666666">.</span>add(layers<span style="color: #666666">.</span>Dense(<span style="color: #666666">1</span>))

    <span style="color: #008000; font-weight: bold">return</span> model
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Let us take a look at our models. <b>Note</b>: double click images for bigger view.</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">generator <span style="color: #666666">=</span> generator_model()
plot_model(generator, show_shapes<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, rankdir<span style="color: #666666">=</span><span style="color: #BA2121">&#39;LR&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">discriminator <span style="color: #666666">=</span> discriminator_model()
plot_model(discriminator, show_shapes<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, rankdir<span style="color: #666666">=</span><span style="color: #BA2121">&#39;LR&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Next we need a few helper objects we will use in training</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">cross_entropy <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>losses<span style="color: #666666">.</span>BinaryCrossentropy(from_logits<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
generator_optimizer <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>optimizers<span style="color: #666666">.</span>Adam(<span style="color: #666666">1e-4</span>)
discriminator_optimizer <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>optimizers<span style="color: #666666">.</span>Adam(<span style="color: #666666">1e-4</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>The first object, \( cross\_entropy \) is our loss function and the two others are
our optimizers. Notice we use the same learning rate for both \( g \) and \( d \). This
is because they need to improve their accuracy at approximately equal speeds to
get convergence (not necessarily exactly equal). Now we define our loss
functions
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">generator_loss</span>(fake_output):
    loss <span style="color: #666666">=</span> cross_entropy(tf<span style="color: #666666">.</span>ones_like(fake_output), fake_output)

    <span style="color: #008000; font-weight: bold">return</span> loss
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">discriminator_loss</span>(real_output, fake_output):
    real_loss <span style="color: #666666">=</span> cross_entropy(tf<span style="color: #666666">.</span>ones_like(real_output), real_output)
    fake_loss <span style="color: #666666">=</span> cross_entropy(tf<span style="color: #666666">.</span>zeros_liks(fake_output), fake_output)
    total_loss <span style="color: #666666">=</span> real_loss <span style="color: #666666">+</span> fake_loss

    <span style="color: #008000; font-weight: bold">return</span> total_loss
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Next we define a kind of seed to help us compare the learning process over
multiple training epochs.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">noise_dimension <span style="color: #666666">=</span> <span style="color: #666666">100</span>
n_examples_to_generate <span style="color: #666666">=</span> <span style="color: #666666">16</span>
seed_images <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal([n_examples_to_generate, noise_dimension])
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Now we have everything we need to define our training step, which we will apply
for every step in our training loop. Notice the @tf.function flag signifying
that the function is tensorflow 'compiled'. Removing this flag doubles the
computation time.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #AA22FF">@tf</span><span style="color: #666666">.</span>function
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">train_step</span>(images):
    noise <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal([BATCH_SIZE, noise_dimension])

    <span style="color: #008000; font-weight: bold">with</span> tf<span style="color: #666666">.</span>GradientTape() <span style="color: #008000; font-weight: bold">as</span> gen_tape, tf<span style="color: #666666">.</span>GradientTape() <span style="color: #008000; font-weight: bold">as</span> disc_tape:
        generated_images <span style="color: #666666">=</span> generator(noise, training<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)

        real_output <span style="color: #666666">=</span> discriminator(images, training<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
        fake_output <span style="color: #666666">=</span> discriminator(generated_images, training<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)

        gen_loss <span style="color: #666666">=</span> generator_loss(fake_output)
        disc_loss <span style="color: #666666">=</span> discriminator_loss(real_output, fake_output)

    gradients_of_generator <span style="color: #666666">=</span> gen_tape<span style="color: #666666">.</span>gradient(gen_loss,
                                            generator<span style="color: #666666">.</span>trainable_variables)
    gradients_of_discriminator <span style="color: #666666">=</span> disc_tape<span style="color: #666666">.</span>gradient(disc_loss,
                                            discriminator<span style="color: #666666">.</span>trainable_variables)
    generator_optimizer<span style="color: #666666">.</span>apply_gradients(<span style="color: #008000">zip</span>(gradients_of_generator,
                                            generator<span style="color: #666666">.</span>trainable_variables))
    discriminator_optimizer<span style="color: #666666">.</span>apply_gradients(<span style="color: #008000">zip</span>(gradients_of_discriminator,
                                            discriminator<span style="color: #666666">.</span>trainable_variables))

    <span style="color: #008000; font-weight: bold">return</span> gen_loss, disc_loss
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Next we define a helper function to produce an output over our training epochs
to see the predictive progression of our generator model. <b>Note</b>: I am including
this code here, but comment it out in the training loop.
</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">generate_and_save_images</span>(model, epoch, test_input):
    <span style="color: #408080; font-style: italic"># we&#39;re making inferences here</span>
    predictions <span style="color: #666666">=</span> model(test_input, training<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)

    fig <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">4</span>, <span style="color: #666666">4</span>))

    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(predictions<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>]):
        plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">4</span>, <span style="color: #666666">4</span>, i<span style="color: #666666">+1</span>)
        plt<span style="color: #666666">.</span>imshow(predictions[i, :, :, <span style="color: #666666">0</span>] <span style="color: #666666">*</span> <span style="color: #666666">127.5</span> <span style="color: #666666">+</span> <span style="color: #666666">127.5</span>, cmap<span style="color: #666666">=</span><span style="color: #BA2121">&#39;gray&#39;</span>)
        plt<span style="color: #666666">.</span>axis(<span style="color: #BA2121">&#39;off&#39;</span>)

    plt<span style="color: #666666">.</span>savefig(<span style="color: #BA2121">f&#39;./images_from_seed_images/image_at_epoch_</span><span style="color: #BB6688; font-weight: bold">{</span><span style="color: #008000">str</span>(epoch)<span style="color: #666666">.</span>zfill(<span style="color: #666666">3</span>)<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">.png&#39;</span>)
    plt<span style="color: #666666">.</span>close()
    <span style="color: #408080; font-style: italic">#plt.show()</span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Setting up checkpoints to periodically save our model during training so that
everything is not lost even if the program were to somehow terminate while
training.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># Setting up checkpoints to save model during training</span>
checkpoint_dir <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;./training_checkpoints&#39;</span>
checkpoint_prefix <span style="color: #666666">=</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(checkpoint_dir, <span style="color: #BA2121">&#39;ckpt&#39;</span>)
checkpoint <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>train<span style="color: #666666">.</span>Checkpoint(generator_optimizer<span style="color: #666666">=</span>generator_optimizer,
                            discriminator_optimizer<span style="color: #666666">=</span>discriminator_optimizer,
                            generator<span style="color: #666666">=</span>generator,
                            discriminator<span style="color: #666666">=</span>discriminator)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Now we define our training loop</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">train</span>(dataset, epochs):
    generator_loss_list <span style="color: #666666">=</span> []
    discriminator_loss_list <span style="color: #666666">=</span> []

    <span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(epochs):
        start <span style="color: #666666">=</span> time<span style="color: #666666">.</span>time()

        <span style="color: #008000; font-weight: bold">for</span> image_batch <span style="color: #AA22FF; font-weight: bold">in</span> dataset:
            gen_loss, disc_loss <span style="color: #666666">=</span> train_step(image_batch)
            generator_loss_list<span style="color: #666666">.</span>append(gen_loss<span style="color: #666666">.</span>numpy())
            discriminator_loss_list<span style="color: #666666">.</span>append(disc_loss<span style="color: #666666">.</span>numpy())

        <span style="color: #408080; font-style: italic">#generate_and_save_images(generator, epoch + 1, seed_images)</span>

        <span style="color: #008000; font-weight: bold">if</span> (epoch <span style="color: #666666">+</span> <span style="color: #666666">1</span>) <span style="color: #666666">%</span> <span style="color: #666666">15</span> <span style="color: #666666">==</span> <span style="color: #666666">0</span>:
            checkpoint<span style="color: #666666">.</span>save(file_prefix<span style="color: #666666">=</span>checkpoint_prefix)

        <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Time for epoch </span><span style="color: #BB6688; font-weight: bold">{</span>epoch<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> is </span><span style="color: #BB6688; font-weight: bold">{</span>time<span style="color: #666666">.</span>time() <span style="color: #666666">-</span> start<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)

    <span style="color: #408080; font-style: italic">#generate_and_save_images(generator, epochs, seed_images)</span>

    loss_file <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;./data/lossfile.txt&#39;</span>
    <span style="color: #008000; font-weight: bold">with</span> <span style="color: #008000">open</span>(loss_file, <span style="color: #BA2121">&#39;w&#39;</span>) <span style="color: #008000; font-weight: bold">as</span> outfile:
        outfile<span style="color: #666666">.</span>write(<span style="color: #008000">str</span>(generator_loss_list))
        outfile<span style="color: #666666">.</span>write(<span style="color: #BA2121">&#39;</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">&#39;</span>)
        outfile<span style="color: #666666">.</span>write(<span style="color: #BA2121">&#39;</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">&#39;</span>)
        outfile<span style="color: #666666">.</span>write(<span style="color: #008000">str</span>(discriminator_loss_list))
        outfile<span style="color: #666666">.</span>write(<span style="color: #BA2121">&#39;</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">&#39;</span>)
        outfile<span style="color: #666666">.</span>write(<span style="color: #BA2121">&#39;</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>To train simply call this function. <b>Warning</b>: this might take a long time so
there is a folder of a pretrained network already included in the repository.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">train(train_dataset, EPOCHS)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>And here is the result of training our model for 100 epochs</p>

<embed src="images_from_seed_images/generation.gif"  autoplay="false" loop="true"></embed>
<p><em></em></p>

<p>Now to avoid having to train and everything, which will take a while depending
on your computer setup we now load in the model which produced the above gif.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">checkpoint<span style="color: #666666">.</span>restore(tf<span style="color: #666666">.</span>train<span style="color: #666666">.</span>latest_checkpoint(checkpoint_dir))
restored_generator <span style="color: #666666">=</span> checkpoint<span style="color: #666666">.</span>generator
restored_discriminator <span style="color: #666666">=</span> checkpoint<span style="color: #666666">.</span>discriminator

<span style="color: #008000">print</span>(restored_generator)
<span style="color: #008000">print</span>(restored_discriminator)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h2 id="exploring-the-latent-space">Exploring the Latent Space </h2>

<p>So we have successfully loaded in our latest model. Let us now play around a bit
and see what kind of things we can learn about this model. Our generator takes
an array of 100 numbers. One idea can be to try to systematically change our
input. Let us try and see what we get
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">generate_latent_points</span>(number<span style="color: #666666">=100</span>, scale_means<span style="color: #666666">=1</span>, scale_stds<span style="color: #666666">=1</span>):
    latent_dim <span style="color: #666666">=</span> <span style="color: #666666">100</span>
    means <span style="color: #666666">=</span> scale_means <span style="color: #666666">*</span> tf<span style="color: #666666">.</span>linspace(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>, num<span style="color: #666666">=</span>latent_dim)
    stds <span style="color: #666666">=</span> scale_stds <span style="color: #666666">*</span> tf<span style="color: #666666">.</span>linspace(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>, num<span style="color: #666666">=</span>latent_dim)
    latent_space_value_range <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal([number, number],
                                                means,
                                                stds,
                                                dtype<span style="color: #666666">=</span>tf<span style="color: #666666">.</span>float64)

    <span style="color: #008000; font-weight: bold">return</span> latent_space_value_range

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">generate_images</span>(latent_points):
    <span style="color: #408080; font-style: italic"># notice we set training to false because we are making inferences</span>
    generated_images <span style="color: #666666">=</span> restored_generator(latent_space_value_range,
                                          training<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)

    <span style="color: #008000; font-weight: bold">return</span> generated_images
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">plot_result</span>(generated_images, number):
    <span style="color: #408080; font-style: italic"># obviously this assumes sqrt number is an int</span>
    fig, axs <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots(<span style="color: #008000">int</span>(np<span style="color: #666666">.</span>sqrt(number)), <span style="color: #008000">int</span>(np<span style="color: #666666">.</span>sqrt(number)),
                            figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>, <span style="color: #666666">10</span>))

    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">int</span>(np<span style="color: #666666">.</span>sqrt(number))):
        <span style="color: #008000; font-weight: bold">for</span> j <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">int</span>(np<span style="color: #666666">.</span>sqrt(number))):
            axs[i, j]<span style="color: #666666">.</span>imshow(generated_images[i<span style="color: #666666">*</span>j], cmap<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Greys&#39;</span>)
            axs[i, j]<span style="color: #666666">.</span>axis(<span style="color: #BA2121">&#39;off&#39;</span>)

    plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">generated_images <span style="color: #666666">=</span> generate_images(generate_latent_points())
plot_result(generated_images, number)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Interesting! We see that the generator generates images that look like MNIST
numbers: \( 1, 4, 7, 9 \). Let's try to tweak it a bit more to see if we are able
to generate a similar plot where we generate every MNIST number. Let us now try
to 'move' a bit around in the latent space. <b>Note</b>: decrease the plot number if
these following cells take too long to run on your computer.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">plot_number <span style="color: #666666">=</span> <span style="color: #666666">225</span>

generated_images <span style="color: #666666">=</span> generate_images(generate_latent_points(number<span style="color: #666666">=</span>plot_number,
                                                          scale_means<span style="color: #666666">=5</span>,
                                                          scale_stds<span style="color: #666666">=1</span>))
plot_result(generated_images, plot_number)

generated_images <span style="color: #666666">=</span> generate_images(generate_latent_points(number<span style="color: #666666">=</span>plot_number,
                                                          scale_means<span style="color: #666666">=-5</span>,
                                                          scale_stds<span style="color: #666666">=1</span>))
plot_result(generated_images, plot_number)

generated_images <span style="color: #666666">=</span> generate_images(generate_latent_points(number<span style="color: #666666">=</span>plot_number,
                                                          scale_means<span style="color: #666666">=1</span>,
                                                          scale_stds<span style="color: #666666">=5</span>))
plot_result(generated_images, plot_number)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Again, we have found something interesting. <em>Moving</em> around using our means
takes us from digit to digit, while <em>moving</em> around using our standard
deviations seem to increase the number of different digits! In the last image
above, we can barely make out every MNIST digit. Let us make on last plot using
this information by upping the standard deviation of our Gaussian noises.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">plot_number <span style="color: #666666">=</span> <span style="color: #666666">400</span>
generated_images <span style="color: #666666">=</span> generate_images(generate_latent_points(number<span style="color: #666666">=</span>plot_number,
                                                          scale_means<span style="color: #666666">=1</span>,
                                                          scale_stds<span style="color: #666666">=10</span>))
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>A pretty cool result! We see that our generator indeed has learned a
distribution which qualitatively looks a whole lot like the MNIST dataset.
</p>

<!-- ------------------- end of main content --------------- -->
<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2024, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</body>
</html>

