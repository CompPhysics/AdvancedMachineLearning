{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7921c09",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week8.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Advanced machine learning and data analysis for the physical sciences -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e312e",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Advanced machine learning and data analysis for the physical sciences\n",
    "**Morten Hjorth-Jensen**, Department of Physics and Center for Computing in Science Education, University of Oslo, Norway\n",
    "\n",
    "Date: **March 13, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb71ad2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Plans for the week March 10-14\n",
    "\n",
    "1. RNNs and discussion of Long-Short-Term memory\n",
    "\n",
    "2. Example of application of RNNs to differential equations\n",
    "\n",
    "3. Start discussion of Autoencoders (AEs)\n",
    "\n",
    "4. Links between Principal Component Analysis (PCA) and AE\n",
    "<!-- o Discussion of specific examples relevant for project 1, [see project from last year by Daniel and Keran](https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/Projects/2023/ProjectExamples/RNNs.pdf) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714db492",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reading recommendations:  RNNs and LSTMs\n",
    "\n",
    "1. For RNNs see Goodfellow et al chapter 10, see <https://www.deeplearningbook.org/contents/rnn.html>\n",
    "\n",
    "2. Reading suggestions for implementation of RNNs in PyTorch: Rashcka et al's text, chapter 15\n",
    "\n",
    "3. RNN video at URL\":https://youtu.be/PCgrgHgy26c?feature=shared\"\n",
    "\n",
    "4. New xLSTM, see Beck et al <https://arxiv.org/abs/2405.04517>. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7947c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reading recommendations: Autoencoders (AE)\n",
    "\n",
    "1. Goodfellow et al chapter 14, see <https://www.deeplearningbook.org/contents/autoencoders.html>\n",
    "\n",
    "2. Rashcka et al. Their chapter 17 contains a brief introduction only.\n",
    "\n",
    "3. Deep Learning Tutorial on AEs from Stanford University at <http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/>\n",
    "\n",
    "4. Building AEs in Keras at <https://blog.keras.io/building-autoencoders-in-keras.html>\n",
    "\n",
    "5. Introduction to AEs in TensorFlow at <https://www.tensorflow.org/tutorials/generative/autoencoder>\n",
    "\n",
    "6. Grosse, University of Toronto, Lecture on AEs at <http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec20.pdf>\n",
    "\n",
    "7. Bank et al on AEs at <https://arxiv.org/abs/2003.05991>  \n",
    "\n",
    "8. Baldi and Hornik, Neural networks and principal component analysis: Learning from examples without local minima, Neural Networks 2, 53 (1989)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2aa93",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gating mechanism: Long Short Term Memory (LSTM)\n",
    "\n",
    "Besides a simple recurrent neural network layer, as discussed above, there are two other\n",
    "commonly used types of recurrent neural network layers: Long Short\n",
    "Term Memory (LSTM) and Gated Recurrent Unit (GRU).  For a short\n",
    "introduction to these layers see <https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b>\n",
    "and <https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b>.\n",
    "\n",
    "LSTM uses a memory cell for \n",
    "modeling long-range dependencies and avoid vanishing gradient\n",
    " problems.\n",
    "Capable of modeling longer term dependencies by having\n",
    "memory cells and gates that controls the information flow along\n",
    "with the memory cells.\n",
    "\n",
    "1. Introduced by Hochreiter and Schmidhuber (1997) who solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).\n",
    "\n",
    "2. They designed a memory cell using logistic and linear units with multiplicative interactions.\n",
    "\n",
    "3. Information gets into the cell whenever its “write” gate is on.\n",
    "\n",
    "4. The information stays in the cell so long as its **keep** gate is on.\n",
    "\n",
    "5. Information can be read from the cell by turning on its **read** gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab201c1c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Implementing a memory cell in a neural network\n",
    "\n",
    "To preserve information for a long time in\n",
    "the activities of an RNN, we use a circuit\n",
    "that implements an analog memory cell.\n",
    "\n",
    "1. A linear unit that has a self-link with a weight of 1 will maintain its state.\n",
    "\n",
    "2. Information is stored in the cell by activating its write gate.\n",
    "\n",
    "3. Information is retrieved by activating the read gate.\n",
    "\n",
    "4. We can backpropagate through this circuit because logistics are have nice derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5672b24f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## LSTM details\n",
    "\n",
    "The LSTM is a unit cell that is made of three gates:\n",
    "1. the input gate,\n",
    "\n",
    "2. the forget gate,\n",
    "\n",
    "3. and the output gate.\n",
    "\n",
    "It also introduces a cell state $c$, which can be thought of as the\n",
    "long-term memory, and a hidden state $h$ which can be thought of as\n",
    "the short-term memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494680a5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Basic layout\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/lstm.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/lstm.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114c6fc",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More LSTM details\n",
    "\n",
    "The first stage is called the forget gate, where we combine the input\n",
    "at (say, time $t$), and the hidden cell state input at $t-1$, passing\n",
    "it through the Sigmoid activation function and then performing an\n",
    "element-wise multiplication, denoted by $\\otimes$.\n",
    "\n",
    "It follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0a6179",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{f}^{(t)} = \\sigma(W_f\\mathbf{x}^{(t)} + U_f\\mathbf{h}^{(t-1)} + \\mathbf{b}_f)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477b87db",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $W$ and $U$ are the weights respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab1d5d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The forget gate\n",
    "\n",
    "This is called the forget gate since the Sigmoid activation function's\n",
    "outputs are very close to $0$ if the argument for the function is very\n",
    "negative, and $1$ if the argument is very positive. Hence we can\n",
    "control the amount of information we want to take from the long-term\n",
    "memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48780944",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Input gate\n",
    "\n",
    "The next stage is the input gate, which consists of both a Sigmoid\n",
    "function ($\\sigma_i$), which decide what percentage of the input will\n",
    "be stored in the long-term memory, and the $\\tanh_i$ function, which\n",
    "decide what is the full memory that can be stored in the long term\n",
    "memory. When these results are calculated and multiplied together, it\n",
    "is added to the cell state or stored in the long-term memory, denoted\n",
    "as $\\oplus$. \n",
    "\n",
    "We have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55447c81",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{i}^{(t)} = \\sigma_g(W_i\\mathbf{x}^{(t)} + U_i\\mathbf{h}^{(t-1)} + \\mathbf{b}_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c3f28",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b2673b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{\\tilde{c}}^{(t)} = \\tanh(W_c\\mathbf{x}^{(t)} + U_c\\mathbf{h}^{(t-1)} + \\mathbf{b}_c),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d84104",
   "metadata": {
    "editable": true
   },
   "source": [
    "again the $W$ and $U$ are the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f0d2b7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Forget and input\n",
    "\n",
    "The forget gate and the input gate together also update the cell state with the following equation,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae36ff0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{c}^{(t)} = \\mathbf{f}^{(t)} \\otimes \\mathbf{c}^{(t-1)} + \\mathbf{i}^{(t)} \\otimes \\mathbf{\\tilde{c}}^{(t)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef889d9e",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $f^{(t)}$ and $i^{(t)}$ are the outputs of the forget gate and the input gate, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed9d97",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Output gate\n",
    "\n",
    "The final stage of the LSTM is the output gate, and its purpose is to\n",
    "update the short-term memory.  To achieve this, we take the newly\n",
    "generated long-term memory and process it through a hyperbolic tangent\n",
    "($\\tanh$) function creating a potential new short-term memory. We then\n",
    "multiply this potential memory by the output of the Sigmoid function\n",
    "($\\sigma_o$). This multiplication generates the final output as well\n",
    "as the input for the next hidden cell ($h^{\\langle t \\rangle}$) within\n",
    "the LSTM cell.\n",
    "\n",
    "We have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427b4cc1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{o}^{(t)} &= \\sigma_g(W_o\\mathbf{x}^{(t)} + U_o\\mathbf{h}^{(t-1)} + \\mathbf{b}_o), \\\\\n",
    "\\mathbf{h}^{(t)} &= \\mathbf{o}^{(t)} \\otimes \\sigma_h(\\mathbf{c}^{(t)}). \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f5d48",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\mathbf{W_o,U_o}$ are the weights of the output gate and $\\mathbf{b_o}$ is the bias of the output gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aaf820",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Example: Solving Differential equations\n",
    "\n",
    "The dynamics of a stable spiral evolve in such a way that the system's\n",
    "trajectory converges to a fixed point while spiraling inward. These\n",
    "oscillations around the fixed point are gradually dampened until the\n",
    "system reaches a steady state at a fixed point.  Suppose we have a\n",
    "two-dimensional system of coupled differential equations of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a914689",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{dx}{dt} &= ax + by \\notag,\\\\\n",
    "    \\frac{dy}{dt} &= cx + dy.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0005e",
   "metadata": {
    "editable": true
   },
   "source": [
    "The choice of $a,b,c,d \\in \\mathbb{R}$ completely determines the\n",
    "behavior of the solution, and for some of these values, albeit not\n",
    "all, the system is said to be a stable spiral. This condition is\n",
    "satisfied when the eigenvalues of the matrix formed by the\n",
    "coefficients are complex conjugates with a negative real part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b67c4b0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Lorenz attractor\n",
    "\n",
    "A Lorenz attractor presents some added complexity. It exhbits what is called a chaotic\n",
    "behavior and its behavior is extremely sensitive to initial conditions.\n",
    "\n",
    "The expression for the Lorenz attractor evolution consists of a set of three coupled nonlinear differential equations given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c1d4a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{dx}{dt} &= \\sigma (y-x), \\notag\\\\\n",
    "    \\frac{dy}{dt} &= x(\\rho -z) - y, \\notag\\\\\n",
    "    \\frac{dz}{dt} &= xy- \\beta z.  \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f28df9",
   "metadata": {
    "editable": true
   },
   "source": [
    "For this problem, $(x,y,z)$ are the variables that determine the state\n",
    "of the system in the space while $\\sigma, \\rho$ and $\\beta$ are,\n",
    "similarly to the constants $a,b,c,d$ of the stable spiral, parameters\n",
    "that influence largely how the system evolves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ac1cf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Generating data\n",
    "\n",
    "Both of the above-mentioned systems are governed by differential\n",
    "equations, and as such, they can be solved numerically through some\n",
    "integration scheme such as forward-Euler or fourth-order\n",
    "Runge-Kutta. \n",
    "\n",
    "We use the common choice of parameters $\\sigma =10$, $\\rho =28$,\n",
    "$\\beta =8/3$.  This choice generates complex and aesthetic\n",
    "trajectories that have been extensively investigated and benchmarked\n",
    "in the literature of numerical simulations.\n",
    "\n",
    "For the stable spiral, we employ $a = 0.2$, $b = -1.0$, $c = 1.0$, $d = 0.2$.\n",
    "This gives a good number of oscillations before reaching a steady state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46227e03",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Training and testing\n",
    "\n",
    "Training and testing procedures in recurrent neural networks follow\n",
    "what is usual for regular FNNs, but some special consideration needs\n",
    "to be taken into account due to the sequential character of the\n",
    "data. **Training and testing batches must not be randomly shuffled** for\n",
    "it would clearly decorrelate the time-series points and leak future\n",
    "information into present or past points of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b08ac6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Computationally expensive\n",
    "\n",
    "The training algorithm can become computationally\n",
    "costly, especially if the losses are evaluated for all previous time\n",
    "steps. While other architectures such as that of LSTMs can be used to\n",
    "mitigate that, it is also possible to introduce another hyperparameter\n",
    "responsible for controlling how much of the network will be unfolded\n",
    "in the training process, adjusting how much the network will remember\n",
    "from previous points in time . Similarly, the number of steps the network predicts\n",
    "in the future per iteration greatly influences the assessment of the\n",
    "loss function. ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fffa0c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Choice of training data\n",
    "\n",
    "The training and testing batches were separated into whole\n",
    "trajectories. This means that instead of training and testing on\n",
    "different fractions of the same trajectory, all trajectories that were\n",
    "tested had completely new initial conditions. In this sense, from a\n",
    "total of 10 initial conditions (independent trajectories), 9 were used\n",
    "for training and 1 for testing. Each trajectory consisted of 800\n",
    "points in each space coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ba314",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Cost/Loss function\n",
    "\n",
    "The problem we have is a time-series forecasting problem, so, we are\n",
    "free to choose the loss function amongst the big collection of\n",
    "regression losses. Using the mean-squared error of the predicted\n",
    "versus factual trajectories of the dynamic systems is a natural choice.\n",
    "\n",
    "It is a convex\n",
    "function, so given sufficient time and appropriate learning rates, it\n",
    "is guaranteed to converge to global minima irrespective of the\n",
    "weightss random initializations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e611f8a3",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}_{MSE} = \\frac{1}{N}\\sum_{i}^N (y(\\mathbf{x}_i) - \\hat{y}(\\mathbf{x}_i, \\mathbf{\\theta}))^2\n",
    "\\label{_auto1} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d674c",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\mathbf{\\theta}$ represents the set of all parameters of the network, and $\\mathbf{x}_i$ are the input values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6fabc8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Modifying the cost/loss function, adding more info\n",
    "\n",
    "A cost/loss function that is based on the observational and\n",
    "predicted data, is normally referred to as a purely data-driven approach.\n",
    "\n",
    "While this is a\n",
    "well-established way of assessing regressions, it does not make use of\n",
    "other intuitions we might have over the problem we are trying to\n",
    "solve. At the same time, it is a well-established fact that neural\n",
    "network models are data-greedy - they need large amounts of data to be\n",
    "able to generalize predictions outside the training set. One way to\n",
    "try to mitigate this is by using physics-informed neural networks\n",
    "(PINNs) when possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1315392f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Changing the function to optimize\n",
    "\n",
    "Trying to improve the performance of our model beyond training sets,\n",
    "PINNs then add physics-informed penalties to the loss function. In\n",
    "essence, this means that we add a worse evaluation score to\n",
    "predictions that do not respect physical laws we think our real data\n",
    "should obey. This procedure often has the advantage of trimming the\n",
    "parameter space without adding bias to the model if the constraints\n",
    "imposed are correct, but the choice of the physical laws can be a\n",
    "delicate one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb2848",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Adding more information to the loss function\n",
    "\n",
    "A general way of expressing this added penalty to the loss function is shown here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c38844a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathcal{L} = w_{MSE}\\mathcal{L}_{MSE} + w_{PI}\\mathcal{L}_{PI}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4752f3e",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, the weights $w_{MSE}$ and $w_{PI}$ explicitly mediate how much\n",
    "influence the specific parts of the total loss function should\n",
    "contribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82557ba5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Autoencoders: Overarching view\n",
    "\n",
    "Autoencoders are artificial neural networks capable of learning\n",
    "efficient representations of the input data (these representations are called codings)  without\n",
    "any supervision (i.e., the training set is unlabeled). These codings\n",
    "typically have a much lower dimensionality than the input data, making\n",
    "autoencoders useful for dimensionality reduction. \n",
    "\n",
    "Autoencoders learn to encode the\n",
    "input data into a lower-dimensional representation, and then decode it\n",
    "back to the original data. The goal of autoencoders is to minimize the\n",
    "reconstruction error, which measures how well the output matches the\n",
    "input. Autoencoders can be seen as a way of learning the latent\n",
    "features or hidden structure of the data, and they can be used for\n",
    "data compression, denoising, anomaly detection, and generative\n",
    "modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e47fe21",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Powerful detectors\n",
    "\n",
    "More importantly, autoencoders act as powerful feature detectors, and\n",
    "they can be used for unsupervised pretraining of deep neural networks.\n",
    "\n",
    "Lastly, they are capable of randomly generating new data that looks\n",
    "very similar to the training data; this is called a generative\n",
    "model. For example, you could train an autoencoder on pictures of\n",
    "faces, and it would then be able to generate new faces.  Surprisingly,\n",
    "autoencoders work by simply learning to copy their inputs to their\n",
    "outputs. This may sound like a trivial task, but we will see that\n",
    "constraining the network in various ways can make it rather\n",
    "difficult. For example, you can limit the size of the internal\n",
    "representation, or you can add noise to the inputs and train the\n",
    "network to recover the original inputs. These constraints prevent the\n",
    "autoencoder from trivially copying the inputs directly to the outputs,\n",
    "which forces it to learn efficient ways of representing the data. In\n",
    "short, the codings are byproducts of the autoencoder’s attempt to\n",
    "learn the identity function under some constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb01123",
   "metadata": {
    "editable": true
   },
   "source": [
    "## First introduction of AEs\n",
    "\n",
    "Autoencoders were first introduced by Rumelhart, Hinton, and Williams\n",
    "in 1986 with the goal of learning to reconstruct the input\n",
    "observations with the lowest error possible.\n",
    "\n",
    "Why would one want to learn to reconstruct the input observations? If\n",
    "you have problems imagining what that means, think of having a dataset\n",
    "made of images. An autoencoder would be an algorithm that can give as\n",
    "output an image that is as similar as possible to the input one. You\n",
    "may be confused, as there is no apparent reason of doing so. To better\n",
    "understand why autoencoders are useful we need a more informative\n",
    "(although not yet unambiguous) definition.\n",
    "\n",
    "An autoencoder is a type of algorithm with the primary purpose of learning an \"informative\" representation of the data that can be used for different applications ([see Bank, D., Koenigstein, N., and Giryes, R., Autoencoders](https://arxiv.org/abs/2003.05991)) by learning to reconstruct a set of input observations well enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6957d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Autoencoder structure\n",
    "\n",
    "Autoencoders are neural networks where the outputs are its own\n",
    "inputs. They are split into an **encoder part**\n",
    "which maps the input $\\boldsymbol{x}$ via a function $f(\\boldsymbol{x},\\boldsymbol{W})$ (this\n",
    "is the encoder part) to a **so-called code part** (or intermediate part)\n",
    "with the result $\\boldsymbol{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2291f4d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{h} = f(\\boldsymbol{x},\\boldsymbol{W})),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a5288",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\boldsymbol{W}$ are the weights to be determined.  The **decoder** parts maps, via its own parameters (weights given by the matrix $\\boldsymbol{V}$ and its own biases) to \n",
    "the final ouput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf898e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{x}} = g(\\boldsymbol{h},\\boldsymbol{V})).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3c5747",
   "metadata": {
    "editable": true
   },
   "source": [
    "The goal is to minimize the construction error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac9743",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Schematic image of an Autoencoder\n",
    "\n",
    "<!-- dom:FIGURE: [figures/ae1.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/ae1.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36008e49",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More on the structure\n",
    "\n",
    "In most typical architectures, the encoder and the decoder are neural networks\n",
    "since they can be easily trained with existing software libraries such as TensorFlow or PyTorch with back propagation.\n",
    "\n",
    "In general, the encoder can be written as a function $g$ that will depend on some parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ee37e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{h}_{i} = g(\\mathbf{x}_{i}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94042ead",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\mathbf{h}_{i}\\in\\mathbb{R}^{q}$  (the latent feature representation) is the output of the encoder block where we evaluate\n",
    "it using the input $\\mathbf{x}_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d50aa5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Decoder part\n",
    "\n",
    "Note that we have $g:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{q}$\n",
    "The decoder and the output of the network $\\tilde{\\mathbf{x}}_{i}$ can be written then as a second generic function\n",
    "of the latent features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d3278",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\mathbf{x}}_{i} = f\\left(\\mathbf{h}_{i}\\right) = f\\left(g\\left(\\mathbf{x}_{i}\\right)\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecedfbc1",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\tilde{\\mathbf{x}}_{i}\\mathbf{\\in }\\mathbb{R}^{n}$.\n",
    "\n",
    "Training an autoencoder simply means finding the functions $g(\\cdot)$ and $f(\\cdot)$\n",
    "that satisfy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfca39",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\textrm{arg}\\min_{f,g}<\\left[\\Delta (\\mathbf{x}_{i}, f(g\\left(\\mathbf{x}_{i}\\right))\\right]>.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a2059",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Typical AEs\n",
    "\n",
    "The standard setup is done via a standard feed forward neural network (FFNN), or what is called a Feed Forward Autoencoder.\n",
    "\n",
    "A typical FFNN architecture has an odd number of layers and is symmetrical with respect to the middle layer.\n",
    "\n",
    "Typically, the first layer has a number of neurons $n_{1} = n$ which equals the size of the input observation $\\mathbf{x}_{\\mathbf{i}}$.\n",
    "\n",
    "As we move toward the center of the network, the number of neurons in each layer drops in some measure.\n",
    "The middle layer usually has the smallest number of neurons.\n",
    "The fact that the number of neurons in this layer is smaller than the size of the input, is often called the **bottleneck**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da504c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Feed Forward Autoencoder\n",
    "\n",
    "<!-- dom:FIGURE: [figures/ae2.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/ae2.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51480216",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mirroring\n",
    "\n",
    "In almost all practical applications,\n",
    "the layers after the middle one are a mirrored version of the layers before the middle one.\n",
    "For example, an autoencoder with three layers could have the following numbers of neurons:\n",
    "\n",
    "$n_{1} = 10$, $n_{2} = 5$ and then $n_{3} = n_{1} = 10$ where the input dimension is equal to ten.\n",
    "\n",
    "All the layers up to and including the middle one, make what is called the encoder, and all the layers from and including\n",
    "the middle one (up to the output) make what is called the decoder.\n",
    "\n",
    "If the FFNN training is successful, the result will\n",
    "be a good approximation of the input $\\tilde{\\mathbf{x}}_{i}\\approx\\mathbf{x}_{i}$.\n",
    "\n",
    "What is essential to notice is that the decoder can reconstruct the\n",
    "input by using only a much smaller number of features than the input\n",
    "observations initially have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90e83d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Output of middle layer\n",
    "\n",
    "The output of the middle layer\n",
    "$\\mathbf{h}_{\\mathbf{i}}$ are also called a **learned representation** of the input observation $\\mathbf{x}_{i}$.\n",
    "\n",
    "The encoder can reduce the number of dimensions of the input\n",
    "observation and create a learned representation\n",
    "$\\mathbf{h}_{\\mathbf{i}}\\mathbf{) }$ of the input that has a smaller\n",
    "dimension $q<n$.\n",
    "\n",
    "This learned representation is enough for the decoder to reconstruct\n",
    "the input accurately (if the autoencoder training was successful as\n",
    "intended)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94abcece",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Activation Function of the Output Layer\n",
    "\n",
    "In autoencoders based on neural networks, the output layer's\n",
    "activation function plays a particularly important role.  The most\n",
    "used functions are ReLU and Sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf0fb0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## ReLU\n",
    "\n",
    "The  ReLU activation function can assume all values in the range $\\left[0,\\infty\\right]$. As a remainder, its formula is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc6317",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\textrm{ReLU}\\left(x\\right) = \\max\\left(0,x\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f0f58",
   "metadata": {
    "editable": true
   },
   "source": [
    "This choice is good when the input observations \\(\\mathbf{x}_{i}\\) assume a wide range of positive values.\n",
    "If the input $\\mathbf{x}_{i}$ can assume negative values, the ReLU is, of course, a terrible choice, and the identity function is a much better choice. It is then common to replace to the ReLU with the so-called **Leaky ReLu** or just modified ReLU.\n",
    "\n",
    "The ReLU activation function for the output layer is well suited for cases when the input observations \\(\\mathbf{x}_{i}\\) assume a wide range of positive real values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13db21c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Sigmoid\n",
    "\n",
    "The sigmoid function $\\sigma$ can assume all values in the range $[0,1]$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb79da4",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\sigma\\left(x\\right) =\\frac{1}{1+e^{-x}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8f80a",
   "metadata": {
    "editable": true
   },
   "source": [
    "This activation function can only be used if the input observations\n",
    "$\\mathbf{x}_{i}$ are all in the range $[0,1]$  or if you have\n",
    "normalized them to be in that range. Consider as an example the MNIST\n",
    "dataset. Each value of the input observation $\\mathbf{x}_{i}$ (one\n",
    "image) is the gray values of the pixels that can assume any value from\n",
    "0 to 255. Normalizing the data by dividing the pixel values by 255\n",
    "would make each observation (each image) have only pixel values\n",
    "between 0 and 1. In this case, the sigmoid would be a good choice for\n",
    "the output layer's activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5628bf8a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Cost/Loss Function\n",
    "\n",
    "If an autoencoder is trying to solve a regression problem, the most\n",
    "common choice as a loss function is the Mean Square Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642edaf",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "L_{\\textrm{MSE}} = \\textrm{MSE} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left\\vert\\vert\\mathbf{x}_{i}-\\tilde{\\mathbf{x}}_{i}\\right\\vert\\vert^{2}_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e506c30",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Binary Cross-Entropy\n",
    "\n",
    "If the activation function of the output layer of the AE is a sigmoid\n",
    "function, thus limiting neuron outputs to be between 0 and 1, and the\n",
    "input features are normalized to be between 0 and 1 we can use as loss\n",
    "function the binary cross-entropy. This cots/loss function is\n",
    "typically used in classification problems, but it works well for\n",
    "autoencoders. The formula for it is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9807e6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "L_{\\textrm{CE}} = -\\frac{1}{n}\\sum_{i = 1}^{n}\\sum_{j = 1}^{p}[x_{j,i} \\log\\tilde{x}_{j,i}+\\left(1-x_{j,i}\\right)\\log (1-\\tilde{x}_{j,i})].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c6b3ac",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reconstruction Error\n",
    "\n",
    "The reconstruction error (RE) is a metric that gives you an indication of how good (or bad) the autoencoder was able to reconstruct\n",
    "the input observation $\\mathbf{x}_{i}$. The most typical RE used is the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc5cc9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\textrm{RE}\\equiv \\textrm{MSE} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left\\vert\\vert\\mathbf{x}_{i}-\\tilde{\\mathbf{x}}_{i}\\right\\vert\\vert^{2}_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74ed85",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Dimensionality reduction and links with Principal component analysis\n",
    "\n",
    "The hope is that the training of the autoencoder can unravel some\n",
    "useful properties of the function $f$. They are often trained with\n",
    "only single-layer neural networks (although deep networks can improve\n",
    "the training) and are essentially given by feed forward neural\n",
    "networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd26c82",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linear functions\n",
    "\n",
    "If the function $f$ and $g$ are given by a linear dependence on the\n",
    "weight matrices $\\boldsymbol{W}$ and $\\boldsymbol{V}$, we can show that for a\n",
    "regression case, by miminizing the mean squared error between $\\boldsymbol{x}$\n",
    "and $\\tilde{\\boldsymbol{x}}$, the autoencoder learns the same subspace as the\n",
    "standard principal component analysis (PCA).\n",
    "\n",
    "In order to see this, we define then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb8bbd",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{h} = f(\\boldsymbol{x},\\boldsymbol{W}))=\\boldsymbol{W}\\boldsymbol{x},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351e81d",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd23ba0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{x}} = g(\\boldsymbol{h},\\boldsymbol{V}))=\\boldsymbol{V}\\boldsymbol{h}=\\boldsymbol{V}\\boldsymbol{W}\\boldsymbol{x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe60901",
   "metadata": {
    "editable": true
   },
   "source": [
    "## AE mean-squared error\n",
    "\n",
    "With the above linear dependence we can in turn define our\n",
    "optimization problem in terms of the optimization of the mean-squared\n",
    "error, that is we wish to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e3c49",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\min_{\\boldsymbol{W},\\boldsymbol{V}\\in {\\mathbb{R}}}\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(x_i-\\tilde{x}_i\\right)^2=\\frac{1}{n}\\vert\\vert \\boldsymbol{x}-\\boldsymbol{V}\\boldsymbol{W}\\boldsymbol{x}\\vert\\vert_2^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512b643",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have used the definition of  a norm-2 vector, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba1ac6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert\\vert \\boldsymbol{x}\\vert\\vert_2 = \\sqrt{\\sum_i x_i^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d46d75",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "This is equivalent to our functions learning the same subspace as\n",
    "the PCA method. This means that we can interpret AEs as a\n",
    "dimensionality reduction method.  To see this, we need to remind\n",
    "ourselves about the PCA method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc263ba2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## What is the Principal Component Analysis (PCA)?\n",
    "\n",
    "PCA is a linear transformation that finds the directions of maximum\n",
    "variance in the data, and projects the data onto a lower-dimensional\n",
    "space. These directions are called principal components, and they are\n",
    "orthogonal to each other. PCA can be seen as a way of compressing the\n",
    "data by discarding the components that have low variance and retain\n",
    "the most important ones. PCA can be applied to both supervised and\n",
    "unsupervised learning problems, and it is often used for data\n",
    "visualization, feature extraction, and noise reduction.\n",
    "\n",
    "A linear autoencoder can be shown to be equal to the PCA.\n",
    "In this lectures we will try to expose these ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd40a4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Basic ideas of the PCA\n",
    "\n",
    "The principal component analysis deals with the problem of fitting a\n",
    "low-dimensional affine subspace $S$ of dimension $d$ much smaller than\n",
    "the total dimension $D$ of the problem at hand (our data\n",
    "set). Mathematically it can be formulated as a statistical problem or\n",
    "a geometric problem.  In our discussion of the theorem for the\n",
    "classical PCA, we will stay with a statistical approach. \n",
    "Historically, the PCA was first formulated in a statistical setting in order to estimate the principal component of a multivariate random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d2607",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Ingredients of the PCA\n",
    "\n",
    "We have a data set defined by a design/feature matrix $\\boldsymbol{X}$ (see below for its definition) \n",
    "1. Each data point is determined by $p$ extrinsic (measurement) variables\n",
    "\n",
    "2. We may want to ask the following question: Are there fewer intrinsic variables (say $d << p$) that still approximately describe the data?\n",
    "\n",
    "3. If so, these intrinsic variables may tell us something important and finding these intrinsic variables is what dimension reduction methods do. \n",
    "\n",
    "A good read is for example [Vidal, Ma and Sastry](https://www.springer.com/gp/book/9780387878102)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431e10d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Introducing the Covariance and Correlation functions\n",
    "\n",
    "Before we discuss the PCA theorem, we need to remind ourselves about\n",
    "the definition of the covariance and the correlation function. These are quantities \n",
    "\n",
    "Suppose we have defined two vectors\n",
    "$\\hat{x}$ and $\\hat{y}$ with $n$ elements each. The covariance matrix $\\boldsymbol{C}$ is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f3fca",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{x},\\boldsymbol{y}] = \\begin{bmatrix} \\mathrm{cov}[\\boldsymbol{x},\\boldsymbol{x}] & \\mathrm{cov}[\\boldsymbol{x},\\boldsymbol{y}] \\\\\n",
    "                              \\mathrm{cov}[\\boldsymbol{y},\\boldsymbol{x}] & \\mathrm{cov}[\\boldsymbol{y},\\boldsymbol{y}] \\\\\n",
    "             \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f3da0b",
   "metadata": {
    "editable": true
   },
   "source": [
    "where for example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ff428",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathrm{cov}[\\boldsymbol{x},\\boldsymbol{y}] =\\frac{1}{n} \\sum_{i=0}^{n-1}(x_i- \\overline{x})(y_i- \\overline{y}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f02e1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Covariance matrix\n",
    "\n",
    "With this definition and recalling that the variance is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020602be",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathrm{var}[\\boldsymbol{x}]=\\frac{1}{n} \\sum_{i=0}^{n-1}(x_i- \\overline{x})^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3164715",
   "metadata": {
    "editable": true
   },
   "source": [
    "we can rewrite the covariance matrix as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92814982",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{x},\\boldsymbol{y}] = \\begin{bmatrix} \\mathrm{var}[\\boldsymbol{x}] & \\mathrm{cov}[\\boldsymbol{x},\\boldsymbol{y}] \\\\\n",
    "                              \\mathrm{cov}[\\boldsymbol{x},\\boldsymbol{y}] & \\mathrm{var}[\\boldsymbol{y}] \\\\\n",
    "             \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2821e5d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More on the covariance\n",
    "The covariance takes values between zero and infinity and may thus\n",
    "lead to problems with loss of numerical precision for particularly\n",
    "large values. It is common to scale the covariance matrix by\n",
    "introducing instead the correlation matrix defined via the so-called\n",
    "correlation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe34628",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathrm{corr}[\\boldsymbol{x},\\boldsymbol{y}]=\\frac{\\mathrm{cov}[\\boldsymbol{x},\\boldsymbol{y}]}{\\sqrt{\\mathrm{var}[\\boldsymbol{x}] \\mathrm{var}[\\boldsymbol{y}]}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fdd7df",
   "metadata": {
    "editable": true
   },
   "source": [
    "The correlation function is then given by values $\\mathrm{corr}[\\boldsymbol{x},\\boldsymbol{y}]\n",
    "\\in [-1,1]$. This avoids eventual problems with too large values. We\n",
    "can then define the correlation matrix for the two vectors $\\boldsymbol{x}$\n",
    "and $\\boldsymbol{y}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8056e8bf",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{K}[\\boldsymbol{x},\\boldsymbol{y}] = \\begin{bmatrix} 1 & \\mathrm{corr}[\\boldsymbol{x},\\boldsymbol{y}] \\\\\n",
    "                              \\mathrm{corr}[\\boldsymbol{y},\\boldsymbol{x}] & 1 \\\\\n",
    "             \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64026703",
   "metadata": {
    "editable": true
   },
   "source": [
    "In the above example this is the function we constructed using **pandas**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630426b5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reminding ourselves about Linear Regression\n",
    "In our derivation of the various regression algorithms like **Ordinary Least Squares** or **Ridge regression**\n",
    "we defined the design/feature matrix $\\boldsymbol{X}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c8feb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\\begin{bmatrix}\n",
    "x_{0,0} & x_{0,1} & x_{0,2}& \\dots & \\dots x_{0,p-1}\\\\\n",
    "x_{1,0} & x_{1,1} & x_{1,2}& \\dots & \\dots x_{1,p-1}\\\\\n",
    "x_{2,0} & x_{2,1} & x_{2,2}& \\dots & \\dots x_{2,p-1}\\\\\n",
    "\\dots & \\dots & \\dots & \\dots \\dots & \\dots \\\\\n",
    "x_{n-2,0} & x_{n-2,1} & x_{n-2,2}& \\dots & \\dots x_{n-2,p-1}\\\\\n",
    "x_{n-1,0} & x_{n-1,1} & x_{n-1,2}& \\dots & \\dots x_{n-1,p-1}\\\\\n",
    "\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3070ab",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\boldsymbol{X}\\in {\\mathbb{R}}^{n\\times p}$, with the predictors/features $p$  refering to the column numbers and the\n",
    "entries $n$ being the row elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5fee2d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Rewriting the matrix $\\boldsymbol{X}$\n",
    "\n",
    "We can rewrite the design/feature matrix in terms of its column vectors as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd2235",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\\begin{bmatrix} \\boldsymbol{x}_0 & \\boldsymbol{x}_1 & \\boldsymbol{x}_2 & \\dots & \\dots & \\boldsymbol{x}_{p-1}\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005d4b2d",
   "metadata": {
    "editable": true
   },
   "source": [
    "with a given vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc0e4ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{x}_i^T = \\begin{bmatrix}x_{0,i} & x_{1,i} & x_{2,i}& \\dots & \\dots x_{n-1,i}\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b87b2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple Example\n",
    "\n",
    "With these definitions, we can now rewrite our $2\\times 2$\n",
    "correlation/covariance matrix in terms of a moe general design/feature\n",
    "matrix $\\boldsymbol{X}\\in {\\mathbb{R}}^{n\\times p}$. This leads to a $p\\times p$\n",
    "covariance matrix for the vectors $\\boldsymbol{x}_i$ with $i=0,1,\\dots,p-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56896c72",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{x}] = \\begin{bmatrix}\n",
    "\\mathrm{var}[\\boldsymbol{x}_0] & \\mathrm{cov}[\\boldsymbol{x}_0,\\boldsymbol{x}_1]  & \\mathrm{cov}[\\boldsymbol{x}_0,\\boldsymbol{x}_2] & \\dots & \\dots & \\mathrm{cov}[\\boldsymbol{x}_0,\\boldsymbol{x}_{p-1}]\\\\\n",
    "\\mathrm{cov}[\\boldsymbol{x}_1,\\boldsymbol{x}_0] & \\mathrm{var}[\\boldsymbol{x}_1]  & \\mathrm{cov}[\\boldsymbol{x}_1,\\boldsymbol{x}_2] & \\dots & \\dots & \\mathrm{cov}[\\boldsymbol{x}_1,\\boldsymbol{x}_{p-1}]\\\\\n",
    "\\mathrm{cov}[\\boldsymbol{x}_2,\\boldsymbol{x}_0]   & \\mathrm{cov}[\\boldsymbol{x}_2,\\boldsymbol{x}_1] & \\mathrm{var}[\\boldsymbol{x}_2] & \\dots & \\dots & \\mathrm{cov}[\\boldsymbol{x}_2,\\boldsymbol{x}_{p-1}]\\\\\n",
    "\\dots & \\dots & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "\\dots & \\dots & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "\\mathrm{cov}[\\boldsymbol{x}_{p-1},\\boldsymbol{x}_0]   & \\mathrm{cov}[\\boldsymbol{x}_{p-1},\\boldsymbol{x}_1] & \\mathrm{cov}[\\boldsymbol{x}_{p-1},\\boldsymbol{x}_{2}]  & \\dots & \\dots  & \\mathrm{var}[\\boldsymbol{x}_{p-1}]\\\\\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59e2be",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Correlation Matrix\n",
    "\n",
    "The correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2830fbc1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{K}[\\boldsymbol{x}] = \\begin{bmatrix}\n",
    "1 & \\mathrm{corr}[\\boldsymbol{x}_0,\\boldsymbol{x}_1]  & \\mathrm{corr}[\\boldsymbol{x}_0,\\boldsymbol{x}_2] & \\dots & \\dots & \\mathrm{corr}[\\boldsymbol{x}_0,\\boldsymbol{x}_{p-1}]\\\\\n",
    "\\mathrm{corr}[\\boldsymbol{x}_1,\\boldsymbol{x}_0] & 1  & \\mathrm{corr}[\\boldsymbol{x}_1,\\boldsymbol{x}_2] & \\dots & \\dots & \\mathrm{corr}[\\boldsymbol{x}_1,\\boldsymbol{x}_{p-1}]\\\\\n",
    "\\mathrm{corr}[\\boldsymbol{x}_2,\\boldsymbol{x}_0]   & \\mathrm{corr}[\\boldsymbol{x}_2,\\boldsymbol{x}_1] & 1 & \\dots & \\dots & \\mathrm{corr}[\\boldsymbol{x}_2,\\boldsymbol{x}_{p-1}]\\\\\n",
    "\\dots & \\dots & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "\\dots & \\dots & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "\\mathrm{corr}[\\boldsymbol{x}_{p-1},\\boldsymbol{x}_0]   & \\mathrm{corr}[\\boldsymbol{x}_{p-1},\\boldsymbol{x}_1] & \\mathrm{corr}[\\boldsymbol{x}_{p-1},\\boldsymbol{x}_{2}]  & \\dots & \\dots  & 1\\\\\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96a08b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Numpy Functionality\n",
    "\n",
    "The Numpy function **np.cov** calculates the covariance elements using\n",
    "the factor $1/(n-1)$ instead of $1/n$ since it assumes we do not have\n",
    "the exact mean values.  The following simple function uses the\n",
    "**np.vstack** function which takes each vector of dimension $1\\times n$\n",
    "and produces a $2\\times n$ matrix $\\boldsymbol{W}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75431bb0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{W}^T = \\begin{bmatrix} x_0 & y_0 \\\\\n",
    "                          x_1 & y_1 \\\\\n",
    "                          x_2 & y_2\\\\\n",
    "                          \\dots & \\dots \\\\\n",
    "                          x_{n-2} & y_{n-2}\\\\\n",
    "                          x_{n-1} & y_{n-1} & \n",
    "             \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d2838",
   "metadata": {
    "editable": true
   },
   "source": [
    "which in turn is converted into into the $2\\times 2$ covariance matrix\n",
    "$\\boldsymbol{C}$ via the Numpy function **np.cov()**. We note that we can also calculate\n",
    "the mean value of each set of samples $\\boldsymbol{x}$ etc using the Numpy\n",
    "function **np.mean(x)**. We can also extract the eigenvalues of the\n",
    "covariance matrix through the **np.linalg.eig()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35261333",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importing various packages\n",
    "import numpy as np\n",
    "n = 100\n",
    "x = np.random.normal(size=n)\n",
    "print(np.mean(x))\n",
    "y = 4+3*x+np.random.normal(size=n)\n",
    "print(np.mean(y))\n",
    "W = np.vstack((x, y))\n",
    "C = np.cov(W)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aed417",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Correlation Matrix again\n",
    "\n",
    "The previous example can be converted into the correlation matrix by\n",
    "simply scaling the matrix elements with the variances.  We should also\n",
    "subtract the mean values for each column. This leads to the following\n",
    "code which sets up the correlations matrix for the previous example in\n",
    "a more brute force way. Here we scale the mean values for each column of the design matrix, calculate the relevant mean values and variances and then finally set up the $2\\times 2$ correlation matrix (since we have only two vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1499a259",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "# define two vectors                                                                                           \n",
    "x = np.random.random(size=n)\n",
    "y = 4+3*x+np.random.normal(size=n)\n",
    "#scaling the x and y vectors                                                                                   \n",
    "x = x - np.mean(x)\n",
    "y = y - np.mean(y)\n",
    "variance_x = np.sum(x@x)/n\n",
    "variance_y = np.sum(y@y)/n\n",
    "print(variance_x)\n",
    "print(variance_y)\n",
    "cov_xy = np.sum(x@y)/n\n",
    "cov_xx = np.sum(x@x)/n\n",
    "cov_yy = np.sum(y@y)/n\n",
    "C = np.zeros((2,2))\n",
    "C[0,0]= cov_xx/variance_x\n",
    "C[1,1]= cov_yy/variance_y\n",
    "C[0,1]= cov_xy/np.sqrt(variance_y*variance_x)\n",
    "C[1,0]= C[0,1]\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f9092e",
   "metadata": {
    "editable": true
   },
   "source": [
    "We see that the matrix elements along the diagonal are one as they\n",
    "should be and that the matrix is symmetric. Furthermore, diagonalizing\n",
    "this matrix we easily see that it is a positive definite matrix.\n",
    "\n",
    "The above procedure with **numpy** can be made more compact if we use **pandas**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e4b3bb",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using Pandas\n",
    "\n",
    "We whow here how we can set up the correlation matrix using **pandas**, as done in this simple code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76cc0ba2",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "n = 10\n",
    "x = np.random.normal(size=n)\n",
    "x = x - np.mean(x)\n",
    "y = 4+3*x+np.random.normal(size=n)\n",
    "y = y - np.mean(y)\n",
    "X = (np.vstack((x, y))).T\n",
    "print(X)\n",
    "Xpd = pd.DataFrame(X)\n",
    "print(Xpd)\n",
    "correlation_matrix = Xpd.corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43594126",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Links with the Design Matrix\n",
    "\n",
    "We can rewrite the covariance matrix in a more compact form in terms of the design/feature matrix $\\boldsymbol{X}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f2af3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{x}] = \\frac{1}{n}\\boldsymbol{X}^T\\boldsymbol{X}= \\mathbb{E}[\\boldsymbol{X}^T\\boldsymbol{X}].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df13eb9a",
   "metadata": {
    "editable": true
   },
   "source": [
    "To see this let us simply look at a design matrix $\\boldsymbol{X}\\in {\\mathbb{R}}^{2\\times 2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee5aae",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\\begin{bmatrix}\n",
    "x_{00} & x_{01}\\\\\n",
    "x_{10} & x_{11}\\\\\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "\\boldsymbol{x}_{0} & \\boldsymbol{x}_{1}\\\\\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f4c1a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Computing the Expectation Values\n",
    "\n",
    "If we then compute the expectation value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d24c124",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbb{E}[\\boldsymbol{X}^T\\boldsymbol{X}] = \\frac{1}{n}\\boldsymbol{X}^T\\boldsymbol{X}=\\begin{bmatrix}\n",
    "x_{00}^2+x_{01}^2 & x_{00}x_{10}+x_{01}x_{11}\\\\\n",
    "x_{10}x_{00}+x_{11}x_{01} & x_{10}^2+x_{11}^2\\\\\n",
    "\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae872d",
   "metadata": {
    "editable": true
   },
   "source": [
    "which is just"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee0e5b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{x}_0,\\boldsymbol{x}_1] = \\boldsymbol{C}[\\boldsymbol{x}]=\\begin{bmatrix} \\mathrm{var}[\\boldsymbol{x}_0] & \\mathrm{cov}[\\boldsymbol{x}_0,\\boldsymbol{x}_1] \\\\\n",
    "                              \\mathrm{cov}[\\boldsymbol{x}_1,\\boldsymbol{x}_0] & \\mathrm{var}[\\boldsymbol{x}_1] \\\\\n",
    "             \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90435387",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we wrote $$\\boldsymbol{C}[\\boldsymbol{x}_0,\\boldsymbol{x}_1] = \\boldsymbol{C}[\\boldsymbol{x}]$$ to indicate that this the covariance of the vectors $\\boldsymbol{x}$ of the design/feature matrix $\\boldsymbol{X}$.\n",
    "\n",
    "It is easy to generalize this to a matrix $\\boldsymbol{X}\\in {\\mathbb{R}}^{n\\times p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8262464",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Towards the PCA theorem\n",
    "\n",
    "We have that the covariance matrix (the correlation matrix involves a simple rescaling) is given as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb7ede",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{x}] = \\frac{1}{n}\\boldsymbol{X}^T\\boldsymbol{X}= \\mathbb{E}[\\boldsymbol{X}^T\\boldsymbol{X}].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c7eda2",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let us now assume that we can perform a series of orthogonal transformations where we employ some orthogonal matrices $\\boldsymbol{S}$.\n",
    "These matrices are defined as $\\boldsymbol{S}\\in {\\mathbb{R}}^{p\\times p}$ and obey the orthogonality requirements $\\boldsymbol{S}\\boldsymbol{S}^T=\\boldsymbol{S}^T\\boldsymbol{S}=\\boldsymbol{I}$. The matrix can be written out in terms of the column vectors $\\boldsymbol{s}_i$ as $\\boldsymbol{S}=[\\boldsymbol{s}_0,\\boldsymbol{s}_1,\\dots,\\boldsymbol{s}_{p-1}]$ and $\\boldsymbol{s}_i \\in {\\mathbb{R}}^{p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f550f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More details\n",
    "\n",
    "Assume also that there is a transformation $\\boldsymbol{S}^T\\boldsymbol{C}[\\boldsymbol{x}]\\boldsymbol{S}=\\boldsymbol{C}[\\boldsymbol{y}]$ such that the new matrix $\\boldsymbol{C}[\\boldsymbol{y}]$ is diagonal with elements $[\\lambda_0,\\lambda_1,\\lambda_2,\\dots,\\lambda_{p-1}]$.  \n",
    "\n",
    "That is we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14488a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{y}] = \\mathbb{E}[\\boldsymbol{S}^T\\boldsymbol{X}^T\\boldsymbol{X}T\\boldsymbol{S}]=\\boldsymbol{S}^T\\boldsymbol{C}[\\boldsymbol{x}]\\boldsymbol{S},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf97a5",
   "metadata": {
    "editable": true
   },
   "source": [
    "since the matrix $\\boldsymbol{S}$ is not a data dependent matrix.   Multiplying with $\\boldsymbol{S}$ from the left we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c1b4b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{S}\\boldsymbol{C}[\\boldsymbol{y}] = \\boldsymbol{C}[\\boldsymbol{x}]\\boldsymbol{S},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4a252",
   "metadata": {
    "editable": true
   },
   "source": [
    "and since $\\boldsymbol{C}[\\boldsymbol{y}]$ is diagonal we have for a given eigenvalue $i$ of the covariance matrix that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e9f52",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{S}_i\\lambda_i = \\boldsymbol{C}[\\boldsymbol{x}]\\boldsymbol{S}_i.\n",
    "$$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
