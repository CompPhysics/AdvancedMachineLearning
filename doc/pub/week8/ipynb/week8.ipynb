{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0afe3fa3",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week8.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: March 4-8: Advanced machine learning and data analysis for the physical sciences -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28735fc1",
   "metadata": {
    "editable": true
   },
   "source": [
    "# March 4-8: Advanced machine learning and data analysis for the physical sciences\n",
    "**Morten Hjorth-Jensen**, Department of Physics and Center for Computing in Science Education, University of Oslo, Norway and Department of Physics and Astronomy and Facility for Rare Isotope Beams, Michigan State University, East Lansing, Michigan, USA\n",
    "\n",
    "Date: **March 5, 2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306211b8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Plans for the week March 4-8\n",
    "\n",
    "1. RNNs and discussion of Long-Short-Term memory\n",
    "\n",
    "2. Discussion of specific examples relevant for project 1, [see project from last year by Daniel and Keran](https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/Projects/2023/ProjectExamples/RNNs.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d08486",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reading recommendations\n",
    "\n",
    "1. For RNNs see Goodfellow et al chapter 10.\n",
    "\n",
    "2. Reading suggestions for implementation of RNNs in PyTorch: Rashcka et al's text, chapter 15\n",
    "\n",
    "3. Reading suggestions for implementation of RNNs in TensorFlow: [Aurelien Geron's chapter 14](https://github.com/CompPhysics/MachineLearning/blob/master/doc/Textbooks/TensorflowML.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68788b17",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs\n",
    "\n",
    "Recurrent neural networks (RNNs) have in general no probabilistic component\n",
    "in a model. With a given fixed input and target from data, the RNNs learn the intermediate\n",
    "association between various layers.\n",
    "The inputs, outputs, and internal representation (hidden states) are all\n",
    "real-valued vectors.\n",
    "\n",
    "In a  traditional NN, it is assumed that every input is\n",
    "independent of each other.  But with sequential data, the input at a given stage $t$ depends on the input from the previous stage $t-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d0eaf0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Basic layout,  [Figures from Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch](https://sebastianraschka.com/blog/2022/ml-pytorch-book.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN1.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN1.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6070a2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- ===== Basic layout  ===== -->\n",
    "<!-- FIGURE: [figslides/rnnsetup.pdf, width=900 frac=1.2] -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59887518",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Solving differential equations with RNNs\n",
    "\n",
    "To gain some intuition on how we can use RNNs for time series, let us\n",
    "tailor the representation of the solution of a differential equation\n",
    "as a time series.\n",
    "\n",
    "Consider the famous differential equation (Newton's equation of motion for damped harmonic oscillations, scaled in terms of dimensionless time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea3bda",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{d^2x}{dt^2}+\\eta\\frac{dx}{dt}+x(t)=F(t),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa506f",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\eta$ is a constant used in scaling time into a dimensionless variable and $F(t)$ is an external force acting on the system.\n",
    "The constant $\\eta$ is a so-called damping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac37f91a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Two first-order differential equations\n",
    "\n",
    "In solving the above second-order equation, it is common to rewrite it in terms of two coupled first-order equations\n",
    "with the velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a83f1c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v(t)=\\frac{dx}{dt},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e340e624",
   "metadata": {
    "editable": true
   },
   "source": [
    "and the acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44999c09",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{dv}{dt}=F(t)-\\eta v(t)-x(t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20247e38",
   "metadata": {
    "editable": true
   },
   "source": [
    "With the initial conditions $v_0=v(t_0)$ and $x_0=x(t_0)$ defined, we can integrate these equations and find their respective solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f4225",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Velocity only\n",
    "\n",
    "Let us focus on the velocity only. Discretizing and using the simplest\n",
    "possible approximation for the derivative, we have Euler's forward\n",
    "method for the updated velocity at a time step $i+1$ given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e8e040",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v_{i+1}=v_i+\\Delta t \\frac{dv}{dt}_{\\vert_{v=v_i}}=v_i+\\Delta t\\left(F_i-\\eta v_i-x_i\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f94767",
   "metadata": {
    "editable": true
   },
   "source": [
    "Defining a function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d2e862",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "h_i(x_i,v_i,F_i)=v_i+\\Delta t\\left(F_i-\\eta v_i-x_i\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4fecb2",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469095a7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v_{i+1}=h_i(x_i,v_i,F_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc78e7b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linking with RNNs\n",
    "\n",
    "The equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d05a67c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v_{i+1}=h_i(x_i,v_i,F_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aaaadb",
   "metadata": {
    "editable": true
   },
   "source": [
    "can be used to train a feed-forward neural network with inputs $v_i$ and outputs $v_{i+1}$ at a time $t_i$. But we can think of this also as a recurrent neural network\n",
    "with inputs $v_i$, $x_i$ and $F_i$ at each time step $t_i$, and producing an output $v_{i+1}$.\n",
    "\n",
    "Noting that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04398796",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v_{i}=v_{i-1}+\\Delta t\\left(F_{i-1}-\\eta v_{i-1}-x_{i-1}\\right)=h_{i-1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf3eb5",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894dc226",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v_{i}=h_{i-1}(x_{i-1},v_{i-1},F_{i-1}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81406c58",
   "metadata": {
    "editable": true
   },
   "source": [
    "and we can rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7cfe74",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v_{i+1}=h_i(x_i,h_{i-1},F_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3317ac",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Minor rewrite\n",
    "\n",
    "We can thus set up a recurring series which depends on the inputs $x_i$ and $F_i$ and the previous values $h_{i-1}$.\n",
    "We assume now that the inputs at each step (or time $t_i$) is given by $x_i$ only and we denote the outputs for $\\tilde{y}_i$ instead of $v_{i_1}$, we have then the compact equation for our outputs at each step $t_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12220ef",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "y_{i}=h_i(x_i,h_{i-1}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c010a23",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can think of this as an element in a recurrent network where our\n",
    "network (our model) produces an output $y_i$ which is then compared\n",
    "with a target value through a given cost/loss function that we\n",
    "optimize. The target values at a given step $t_i$ could be the results\n",
    "of a measurement or simply the analytical results of a differential\n",
    "equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a34b73",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN2.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN2.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b5582",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 2\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN3.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN3.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12350730",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 3\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN4.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN4.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d504e8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 4\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN5.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN5.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117455a9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 5\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN6.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN6.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af65449",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 6\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN7.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN7.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce946c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 7\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN8.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN8.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03217d3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Backpropagation through time\n",
    "\n",
    "We can think of the recurrent net as a layered, feed-forward\n",
    "net with shared weights and then train the feed-forward net\n",
    "with weight constraints.\n",
    "\n",
    "We can also think of this training algorithm in the time domain:\n",
    "1. The forward pass builds up a stack of the activities of all the units at each time step.\n",
    "\n",
    "2. The backward pass peels activities off the stack to compute the error derivatives at each time step.\n",
    "\n",
    "3. After the backward pass we add together the derivatives at all the different times for each weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3056d5a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The backward pass is linear\n",
    "\n",
    "1. There is a big difference between the forward and backward passes.\n",
    "\n",
    "2. In the forward pass we use squashing functions (like the logistic) to prevent the activity vectors from exploding.\n",
    "\n",
    "3. The backward pass, is completely linear. If you double the error derivatives at the final layer, all the error derivatives will double.\n",
    "\n",
    "The forward pass determines the slope of the linear function used for\n",
    "backpropagating through each neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb411bc0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The problem of exploding or vanishing gradients\n",
    "* What happens to the magnitude of the gradients as we backpropagate through many layers?\n",
    "\n",
    "a. If the weights are small, the gradients shrink exponentially.\n",
    "\n",
    "b. If the weights are big the gradients grow exponentially.\n",
    "\n",
    "* Typical feed-forward neural nets can cope with these exponential effects because they only have a few hidden layers.\n",
    "\n",
    "* In an RNN trained on long sequences (e.g. 100 time steps) the gradients can easily explode or vanish.\n",
    "\n",
    "a. We can avoid this by initializing the weights very carefully.\n",
    "\n",
    "* Even with good initial weights, its very hard to detect that the current target output depends on an input from many time-steps ago.\n",
    "\n",
    "RNNs have difficulty dealing with long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4184c52a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematical setup\n",
    "\n",
    "The expression for the simplest Recurrent network resembles that of a\n",
    "regular feed-forward neural network, but now with\n",
    "the concept of temporal dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66766eaa",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{a}^{(t)} & = U * \\mathbf{x}^{(t)} + W * \\mathbf{h}^{(t-1)} + \\mathbf{b}, \\notag \\\\\n",
    "    \\mathbf{h}^{(t)} &= \\sigma_h(\\mathbf{a}^{(t)}), \\notag\\\\\n",
    "    \\mathbf{y}^{(t)} &= V * \\mathbf{h}^{(t)} + \\mathbf{c}, \\notag\\\\\n",
    "    \\mathbf{\\hat{y}}^{(t)} &= \\sigma_y(\\mathbf{y}^{(t)}).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613e3a9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time through figures, part 1\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN9.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN9.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf49dc",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time, part 2\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN10.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN10.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258349c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time, part 3\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN11.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN11.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6daf3bd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time, part 4\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN12.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN12.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb419f9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back propagation in time in equations\n",
    "\n",
    "To derive the expression of the gradients of $\\mathcal{L}$ for\n",
    "the RNN, we need to start recursively from the nodes closer to the\n",
    "output layer in the temporal unrolling scheme - such as $\\mathbf{y}$\n",
    "and $\\mathbf{h}$ at final time $t = \\tau$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74b70a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    (\\nabla_{ \\mathbf{y}^{(t)}} \\mathcal{L})_{i} &= \\frac{\\partial \\mathcal{L}}{\\partial L^{(t)}}\\frac{\\partial L^{(t)}}{\\partial y_{i}^{(t)}}, \\notag\\\\\n",
    "    \\nabla_{\\mathbf{h}^{(\\tau)}} \\mathcal{L} &= \\mathbf{V}^\\mathsf{T}\\nabla_{ \\mathbf{y}^{(\\tau)}} \\mathcal{L}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc667d74",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Chain rule again\n",
    "For the following hidden nodes, we have to iterate through time, so by the chain rule,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce70fab",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla_{\\mathbf{h}^{(t)}} \\mathcal{L} &= \\left(\\frac{\\partial\\mathbf{h}^{(t+1)}}{\\partial\\mathbf{h}^{(t)}}\\right)^\\mathsf{T}\\nabla_{\\mathbf{h}^{(t+1)}}\\mathcal{L} + \\left(\\frac{\\partial\\mathbf{y}^{(t)}}{\\partial\\mathbf{h}^{(t)}}\\right)^\\mathsf{T}\\nabla_{ \\mathbf{y}^{(t)}} \\mathcal{L}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae592746",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gradients of loss functions\n",
    "Similarly, the gradients of $\\mathcal{L}$ with respect to the weights and biases follow,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29deaf57",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:rnn_gradients3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla_{\\mathbf{c}} \\mathcal{L} &=\\sum_{t}\\left(\\frac{\\partial \\mathbf{y}^{(t)}}{\\partial \\mathbf{c}}\\right)^\\mathsf{T} \\nabla_{\\mathbf{y}^{(t)}} \\mathcal{L} \\notag\\\\\n",
    "    \\nabla_{\\mathbf{b}} \\mathcal{L} &=\\sum_{t}\\left(\\frac{\\partial \\mathbf{h}^{(t)}}{\\partial \\mathbf{b}}\\right)^\\mathsf{T}        \\nabla_{\\mathbf{h}^{(t)}} \\mathcal{L} \\notag\\\\\n",
    "    \\nabla_{\\mathbf{V}} \\mathcal{L} &=\\sum_{t}\\sum_{i}\\left(\\frac{\\partial \\mathcal{L}}{\\partial y_i^{(t)} }\\right)\\nabla_{\\mathbf{V}^{(t)}}y_i^{(t)} \\notag\\\\\n",
    "    \\nabla_{\\mathbf{W}} \\mathcal{L} &=\\sum_{t}\\sum_{i}\\left(\\frac{\\partial \\mathcal{L}}{\\partial h_i^{(t)}}\\right)\\nabla_{\\mathbf{w}^{(t)}} h_i^{(t)} \\notag\\\\\n",
    "    \\nabla_{\\mathbf{U}} \\mathcal{L} &=\\sum_{t}\\sum_{i}\\left(\\frac{\\partial \\mathcal{L}}{\\partial h_i^{(t)}}\\right)\\nabla_{\\mathbf{U}^{(t)}}h_i^{(t)}.\n",
    "    \\label{eq:rnn_gradients3} \\tag{1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f582e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Summary of RNNs\n",
    "\n",
    "Recurrent neural networks (RNNs) have in general no probabilistic component\n",
    "in a model. With a given fixed input and target from data, the RNNs learn the intermediate\n",
    "association between various layers.\n",
    "The inputs, outputs, and internal representation (hidden states) are all\n",
    "real-valued vectors.\n",
    "\n",
    "In a  traditional NN, it is assumed that every input is\n",
    "independent of each other.  But with sequential data, the input at a given stage $t$ depends on the input from the previous stage $t-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25bfd6a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Summary of a  typical RNN\n",
    "\n",
    "1. Weight matrices $U$, $W$ and $V$ that connect the input layer at a stage $t$ with the hidden layer $h_t$, the previous hidden layer $h_{t-1}$ with $h_t$ and the hidden layer $h_t$ connecting with the output layer at the same stage and producing an output $\\tilde{y}_t$, respectively.\n",
    "\n",
    "2. The output from the hidden layer $h_t$ is oftem modulated by a $\\tanh{}$ function $h_t=\\sigma_h(x_t,h_{t-1})=\\tanh{(Ux_t+Wh_{t-1}+b)}$ with $b$ a bias value\n",
    "\n",
    "3. The output from the hidden layer produces $\\tilde{y}_t=\\sigma_y(Vh_t+c)$ where $c$ is a new bias parameter.\n",
    "\n",
    "4. The output from the training at a given stage is in turn compared with the observation $y_t$ thorugh a chosen cost function.\n",
    "\n",
    "The function $g$ can any of the standard activation functions, that is a Sigmoid, a Softmax, a ReLU and other.\n",
    "The parameters are trained through the so-called back-propagation through time (BPTT) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae3f1c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Four effective ways to learn an RNN and preparing for next week\n",
    "1. Long Short Term Memory Make the RNN out of little modules that are designed to remember values for a long time.\n",
    "\n",
    "2. Hessian Free Optimization: Deal with the vanishing gradients problem by using a fancy optimizer that can detect directions with a tiny gradient but even smaller curvature.\n",
    "\n",
    "3. Echo State Networks: Initialize the input a hidden and hidden-hidden and output-hidden connections very carefully so that the hidden state has a huge reservoir of weakly coupled oscillators which can be selectively driven by the input.\n",
    "\n",
    "  * ESNs only need to learn the hidden-output connections.\n",
    "\n",
    "4. Good initialization with momentum Initialize like in Echo State Networks, but then learn all of the connections using momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee8241",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gating mechanism: Long Short Term Memory (LSTM)\n",
    "\n",
    "Besides a simple recurrent neural network layer, as discussed above, there are two other\n",
    "commonly used types of recurrent neural network layers: Long Short\n",
    "Term Memory (LSTM) and Gated Recurrent Unit (GRU).  For a short\n",
    "introduction to these layers see <https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b>\n",
    "and <https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b>.\n",
    "\n",
    "LSTM uses a memory cell for \n",
    "modeling long-range dependencies and avoid vanishing gradient\n",
    " problems.\n",
    "Capable of modeling longer term dependencies by having\n",
    "memory cells and gates that controls the information flow along\n",
    "with the memory cells.\n",
    "\n",
    "1. Introduced by Hochreiter and Schmidhuber (1997) who solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).\n",
    "\n",
    "2. They designed a memory cell using logistic and linear units with multiplicative interactions.\n",
    "\n",
    "3. Information gets into the cell whenever its “write” gate is on.\n",
    "\n",
    "4. The information stays in the cell so long as its **keep** gate is on.\n",
    "\n",
    "5. Information can be read from the cell by turning on its **read** gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead7ddf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Implementing a memory cell in a neural network\n",
    "\n",
    "To preserve information for a long time in\n",
    "the activities of an RNN, we use a circuit\n",
    "that implements an analog memory cell.\n",
    "\n",
    "1. A linear unit that has a self-link with a weight of 1 will maintain its state.\n",
    "\n",
    "2. Information is stored in the cell by activating its write gate.\n",
    "\n",
    "3. Information is retrieved by activating the read gate.\n",
    "\n",
    "4. We can backpropagate through this circuit because logistics are have nice derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5d6da",
   "metadata": {
    "editable": true
   },
   "source": [
    "## LSTM details\n",
    "\n",
    "The LSTM is a unit cell that is made of three gates:\n",
    "1. the input gate,\n",
    "\n",
    "2. the forget gate,\n",
    "\n",
    "3. and the output gate.\n",
    "\n",
    "It also introduces a cell state $c$, which can be thought of as the\n",
    "long-term memory, and a hidden state $h$ which can be thought of as\n",
    "the short-term memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adead78e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Basic layout\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/lstm.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/lstm.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c80483",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More LSTM details\n",
    "\n",
    "The first stage is called the forget gate, where we combine the input\n",
    "at (say, time $t$), and the hidden cell state input at $t-1$, passing\n",
    "it through the Sigmoid activation function and then performing an\n",
    "element-wise multiplication, denoted by $\\otimes$.\n",
    "\n",
    "It follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e73479",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{f}^{(t)} = \\sigma(W_f\\mathbf{x}^{(t)} + U_f\\mathbf{h}^{(t-1)} + \\mathbf{b}_f)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781c667",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $W$ and $U$ are the weights respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c09bf1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The forget gate\n",
    "\n",
    "This is called the forget gate since the Sigmoid activation function's\n",
    "outputs are very close to $0$ if the argument for the function is very\n",
    "negative, and $1$ if the argument is very positive. Hence we can\n",
    "control the amount of information we want to take from the long-term\n",
    "memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85fae36",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Input gate\n",
    "\n",
    "The next stage is the input gate, which consists of both a Sigmoid\n",
    "function ($\\sigma_i$), which decide what percentage of the input will\n",
    "be stored in the long-term memory, and the $\\tanh_i$ function, which\n",
    "decide what is the full memory that can be stored in the long term\n",
    "memory. When these results are calculated and multiplied together, it\n",
    "is added to the cell state or stored in the long-term memory, denoted\n",
    "as $\\oplus$. \n",
    "\n",
    "We have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e702ea9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{i}^{(t)} = \\sigma_g(W_i\\mathbf{x}^{(t)} + U_i\\mathbf{h}^{(t-1)} + \\mathbf{b}_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbb2f1b",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f768cf7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{\\tilde{c}}^{(t)} = \\tanh(W_c\\mathbf{x}^{(t)} + U_c\\mathbf{h}^{(t-1)} + \\mathbf{b}_c),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496e20b",
   "metadata": {
    "editable": true
   },
   "source": [
    "again the $W$ and $U$ are the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa282cb",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Forget and input\n",
    "\n",
    "The forget gate and the input gate together also update the cell state with the following equation,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6cfe8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{c}^{(t)} = \\mathbf{f}^{(t)} \\otimes \\mathbf{c}^{(t-1)} + \\mathbf{i}^{(t)} \\otimes \\mathbf{\\tilde{c}}^{(t)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f296733",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $f^{(t)}$ and $i^{(t)}$ are the outputs of the forget gate and the input gate, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f92e60",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Output gate\n",
    "\n",
    "The final stage of the LSTM is the output gate, and its purpose is to\n",
    "update the short-term memory.  To achieve this, we take the newly\n",
    "generated long-term memory and process it through a hyperbolic tangent\n",
    "($\\tanh$) function creating a potential new short-term memory. We then\n",
    "multiply this potential memory by the output of the Sigmoid function\n",
    "($\\sigma_o$). This multiplication generates the final output as well\n",
    "as the input for the next hidden cell ($h^{\\langle t \\rangle}$) within\n",
    "the LSTM cell.\n",
    "\n",
    "We have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbfc00",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{o}^{(t)} &= \\sigma_g(W_o\\mathbf{x}^{(t)} + U_o\\mathbf{h}^{(t-1)} + \\mathbf{b}_o), \\\\\n",
    "\\mathbf{h}^{(t)} &= \\mathbf{o}^{(t)} \\otimes \\sigma_h(\\mathbf{c}^{(t)}). \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62071652",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\mathbf{W_o,U_o}$ are the weights of the output gate and $\\mathbf{b_o}$ is the bias of the output gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74705ba6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Differential equations\n",
    "\n",
    "[The examples here are taken from the project of Keran Chen and Daniel Haas Beccatini Lima from 2023.](https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/Projects/2023/ProjectExamples/RNNs.pdf)\n",
    "\n",
    "The dynamics of a stable spiral evolve in such a way that the system's\n",
    "trajectory converges to a fixed point while spiraling inward. These\n",
    "oscillations around the fixed point are gradually dampened until the\n",
    "system reaches a steady state at a fixed point.  Suppose we have a\n",
    "two-dimensional system of coupled differential equations of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f81ee0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{dx}{dt} &= ax + by \\notag,\\\\\n",
    "    \\frac{dy}{dt} &= cx + dy.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48980954",
   "metadata": {
    "editable": true
   },
   "source": [
    "The choice of $a,b,c,d \\in \\mathbb{R}$ completely determines the\n",
    "behavior of the solution, and for some of these values, albeit not\n",
    "all, the system is said to be a stable spiral. This condition is\n",
    "satisfied when the eigenvalues of the matrix formed by the\n",
    "coefficients are complex conjugates with a negative real part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567feb3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Lorenz attractor\n",
    "\n",
    "A Lorenz attractor presents some added complexity. It exhbits what is called a chaotic\n",
    "behavior and its behavior is extremely sensitive to initial conditions.\n",
    "\n",
    "The expression for the Lorenz attractor evolution consists of a set of three coupled nonlinear differential equations given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6721fad3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{dx}{dt} &= \\sigma (y-x), \\notag\\\\\n",
    "    \\frac{dy}{dt} &= x(\\rho -z) - y, \\notag\\\\\n",
    "    \\frac{dz}{dt} &= xy- \\beta z.  \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be834f",
   "metadata": {
    "editable": true
   },
   "source": [
    "For this problem, ${x,y,z}$ are the variables that determine the state\n",
    "of the system in the space while $\\sigma, \\rho$ and $\\beta$ are,\n",
    "similarly to the constants $a,b,c,d$ of the stable spiral, parameters\n",
    "that influence largely how the system evolves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81be49f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Generating data\n",
    "\n",
    "Both of the above-mentioned systems are governed by differential\n",
    "equations, and as such, they can be solved numerically through some\n",
    "integration scheme such as forward-Euler or fourth-order\n",
    "Runge-Kutta. \n",
    "\n",
    "We use the common choice of parameters $\\sigma =10$, $\\rho =28$,\n",
    "$\\beta =8/3$.  This choice generates complex and aesthetic\n",
    "trajectories that have been extensively investigated and benchmarked\n",
    "in the literature of numerical simulations.\n",
    "\n",
    "For the stable spiral, we employ $a = 0.2$, $b = -1.0$, $c = 1.0$, $d = 0.2$.\n",
    "This gives a good number of oscillations before reaching a steady state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4638a1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Training and testing\n",
    "\n",
    "Training and testing procedures in recurrent neural networks follow\n",
    "what is usual for regular FNNs, but some special consideration needs\n",
    "to be taken into account due to the sequential character of the\n",
    "data. **Training and testing batches must not be randomly shuffled** for\n",
    "it would clearly decorrelate the time-series points and leak future\n",
    "information into present or past points of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c74ed",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Computationally expensive\n",
    "\n",
    "The training algorithm can become computationally\n",
    "costly, especially if the losses are evaluated for all previous time\n",
    "steps. While other architectures such as that of LSTMs can be used to\n",
    "mitigate that, it is also possible to introduce another hyperparameter\n",
    "responsible for controlling how much of the network will be unfolded\n",
    "in the training process, adjusting how much the network will remember\n",
    "from previous points in time . Similarly, the number of steps the network predicts\n",
    "in the future per iteration greatly influences the assessment of the\n",
    "loss function. ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4265c885",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Choice of training data\n",
    "\n",
    "The training and testing batches were separated into whole\n",
    "trajectories. This means that instead of training and testing on\n",
    "different fractions of the same trajectory, all trajectories that were\n",
    "tested had completely new initial conditions. In this sense, from a\n",
    "total of 10 initial conditions (independent trajectories), 9 were used\n",
    "for training and 1 for testing. Each trajectory consisted of 800\n",
    "points in each space coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dfa6d2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Cost/Loss function\n",
    "\n",
    "The problem we have is a time-series forecasting problem, so, we are\n",
    "free to choose the loss function amongst the big collection of\n",
    "regression losses. Using the mean-squared error of the predicted\n",
    "versus factual trajectories of the dynamic systems is a natural choice.\n",
    "\n",
    "It is a convex\n",
    "function, so given sufficient time and appropriate learning rates, it\n",
    "is guaranteed to converge to global minima irrespective of the\n",
    "weightss random initializations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6799dd",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}_{MSE} = \\frac{1}{N}\\sum_{i}^N (y(\\mathbf{x}_i) - \\hat{y}(\\mathbf{x}_i, \\mathbf{\\theta}))^2\n",
    "\\label{_auto1} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c1c2bf",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\mathbf{\\theta}$ represents the set of all parameters of the network, and $\\mathbf{x}_i$ are the input values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b6c116",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Modifying the cost/loss function, adding more info\n",
    "\n",
    "A cost/loss function that is based on the observational and\n",
    "predicted data, is normally referred to as a purely data-driven approach.\n",
    "\n",
    "While this is a\n",
    "well-established way of assessing regressions, it does not make use of\n",
    "other intuitions we might have over the problem we are trying to\n",
    "solve. At the same time, it is a well-established fact that neural\n",
    "network models are data-greedy - they need large amounts of data to be\n",
    "able to generalize predictions outside the training set. One way to\n",
    "try to mitigate this is by using physics-informed neural networks\n",
    "(PINNs) when possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd6f85",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Changing the function to optimize\n",
    "\n",
    "Trying to improve the performance of our model beyond training sets,\n",
    "PINNs then add physics-informed penalties to the loss function. In\n",
    "essence, this means that we add a worse evaluation score to\n",
    "predictions that do not respect physical laws we think our real data\n",
    "should obey. This procedure often has the advantage of trimming the\n",
    "parameter space without adding bias to the model if the constraints\n",
    "imposed are correct, but the choice of the physical laws can be a\n",
    "delicate one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f53814e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Adding more information to the loss function\n",
    "\n",
    "A general way of expressing this added penalty to the loss function is shown here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3dbe5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathcal{L} = w_{MSE}\\mathcal{L}_{MSE} + w_{PI}\\mathcal{L}_{PI}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f0907",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, the weights $w_{MSE}$ and $w_{PI}$ explicitly mediate how much\n",
    "influence the specific parts of the total loss function should\n",
    "contribute. See the above project link for more details."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
