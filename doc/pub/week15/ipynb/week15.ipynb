{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577a152a",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week15.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Advanced machine learning and data analysis for the physical sciences -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f785e",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Advanced machine learning and data analysis for the physical sciences\n",
    "**Morten Hjorth-Jensen**, Department of Physics and Center for Computing in Science Education, University of Oslo, Norway and Department of Physics and Astronomy and Facility for Rare Isotope Beams, Michigan State University, East Lansing, Michigan, USA\n",
    "\n",
    "Date: **April 30, 2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b016d97",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Plans for the week of April 29- May 3, 2024\n",
    "\n",
    "**Deep generative models.**\n",
    "\n",
    "1. Summary of Variational Autoencoders\n",
    "\n",
    "2. Generative Adversarial Networks (GANs)\n",
    "\n",
    "3. Start discussion of diffusion models\n",
    "<!-- o [Video of lecture](https://youtu.be/rw-NBN293o4) -->\n",
    "<!-- o [Whiteboard notes](https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/HandwrittenNotes/2024/NotesApril30.pdf) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de7163c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Readings\n",
    "\n",
    "1. Reading recommendation: Goodfellow et al, for GANs see sections 20.10-20.11\n",
    "\n",
    "2. For codes and background, see Raschka et al, Machine with PyTorch and Scikit-Learn, chapter 17, see <https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch17> for codes\n",
    "\n",
    "3. Babcock and Bali, Generative AI with Python and TensorFlow2, chapter 6 and codes at <https://github.com/raghavbali/generative_ai_with_tensorflow/blob/master/Chapter_6/conditional_gan.ipynb>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be1c5d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Summary of Variational Autoencoders (VAEs)\n",
    "\n",
    "In our short summary of VAes, we will also remind you about the\n",
    "mathematics of Boltzmann machines and the Kullback-Leibler divergence,\n",
    "leading to used ways to optimize the probability\n",
    "distributions, namely what is called \n",
    "* Contrastive optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e969fb5f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Boltzmann machines and energy-based models and contrastive optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2085c06",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Energy models\n",
    "\n",
    "For Boltzmann machines  we defined a domain $\\boldsymbol{X}$ of stochastic variables $\\boldsymbol{X}= \\{x_0,x_1, \\dots , x_{n-1}\\}$ with a pertinent probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd81511",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{X})=\\prod_{x_i\\in \\boldsymbol{X}}p(x_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888472d",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have assumed that the random varaibles $x_i$ are all independent and identically distributed (iid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a97aa",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Probability model\n",
    "\n",
    "We defined a probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35f74a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(x_i,h_j;\\boldsymbol{\\Theta}) = \\frac{f(x_i,h_j;\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18152b5",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $f(x_i,h_j;\\boldsymbol{\\Theta})$ is a function which we assume is larger or\n",
    "equal than zero and obeys all properties required for a probability\n",
    "distribution and $Z(\\boldsymbol{\\Theta})$ is a normalization constant. Inspired by\n",
    "statistical mechanics, we call it often for the partition function.\n",
    "It is defined as (assuming that we have discrete probability distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839e45b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "Z(\\boldsymbol{\\Theta})=\\sum_{x_i\\in \\boldsymbol{X}}\\sum_{h_j\\in \\boldsymbol{H}} f(x_i,h_j;\\boldsymbol{\\Theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f221eff2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Marginal and conditional probabilities\n",
    "\n",
    "We can in turn define the marginal probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3cac9b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(x_i;\\boldsymbol{\\Theta}) = \\frac{\\sum_{h_j\\in \\boldsymbol{H}}f(x_i,h_j;\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c818a0f9",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2147c3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(h_i;\\boldsymbol{\\Theta}) = \\frac{\\sum_{x_i\\in \\boldsymbol{X}}f(x_i,h_j;\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f572f677",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Partition function\n",
    "\n",
    "**Note the change to a vector notation**. A variable like $\\boldsymbol{x}$\n",
    "represents now a specific **configuration**. We can generate an infinity\n",
    "of such configurations. The final partition function is then the sum\n",
    "over all such possible configurations, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113ce65",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "Z(\\boldsymbol{\\Theta})=\\sum_{x_i\\in \\boldsymbol{X}}\\sum_{h_j\\in \\boldsymbol{H}} f(x_i,h_j;\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd4a772",
   "metadata": {
    "editable": true
   },
   "source": [
    "changes to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9540f1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "Z(\\boldsymbol{\\Theta})=\\sum_{\\boldsymbol{x}}\\sum_{\\boldsymbol{h}} f(\\boldsymbol{x},\\boldsymbol{h};\\boldsymbol{\\Theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c912c12",
   "metadata": {
    "editable": true
   },
   "source": [
    "If we have a binary set of variable $x_i$ and $h_j$ and $M$ values of $x_i$ and $N$ values of $h_j$ we have in total $2^M$ and $2^N$ possible $\\boldsymbol{x}$ and $\\boldsymbol{h}$ configurations, respectively.\n",
    "\n",
    "We see that even for the modest binary case, we can easily approach a\n",
    "number of configuration which is not possible to deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beba968",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Optimization problem\n",
    "\n",
    "At the end, we are not interested in the probabilities of the hidden variables. The probability we thus want to optimize is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5dbc1a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{X};\\boldsymbol{\\Theta})=\\prod_{x_i\\in \\boldsymbol{X}}p(x_i;\\boldsymbol{\\Theta})=\\prod_{x_i\\in \\boldsymbol{X}}\\left(\\frac{\\sum_{h_j\\in \\boldsymbol{H}}f(x_i,h_j;\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})}\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ae569",
   "metadata": {
    "editable": true
   },
   "source": [
    "which we rewrite as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897daba",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{X};\\boldsymbol{\\Theta})=\\frac{1}{Z(\\boldsymbol{\\Theta})}\\prod_{x_i\\in \\boldsymbol{X}}\\left(\\sum_{h_j\\in \\boldsymbol{H}}f(x_i,h_j;\\boldsymbol{\\Theta})\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a76b6a6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Further simplifications\n",
    "\n",
    "We simplify further by rewriting it as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a21fc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{X};\\boldsymbol{\\Theta})=\\frac{1}{Z(\\boldsymbol{\\Theta})}\\prod_{x_i\\in \\boldsymbol{X}}f(x_i;\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dfaafc",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we used $p(x_i;\\boldsymbol{\\Theta}) = \\sum_{h_j\\in \\boldsymbol{H}}f(x_i,h_j;\\boldsymbol{\\Theta})$.\n",
    "The optimization problem is then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070fde8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\mathrm{arg} \\hspace{0.1cm}\\max_{\\boldsymbol{\\boldsymbol{\\Theta}}\\in {\\mathbb{R}}^{p}}} \\hspace{0.1cm}p(\\boldsymbol{X};\\boldsymbol{\\Theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c402d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Optimizing the logarithm instead\n",
    "\n",
    "Computing the derivatives with respect to the parameters $\\boldsymbol{\\Theta}$ is\n",
    "easier (and equivalent) with taking the logarithm of the\n",
    "probability. We will thus optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630dcb70",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\mathrm{arg} \\hspace{0.1cm}\\max_{\\boldsymbol{\\boldsymbol{\\Theta}}\\in {\\mathbb{R}}^{p}}} \\hspace{0.1cm}\\log{p(\\boldsymbol{X};\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea16c1",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023960cd",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{p(\\boldsymbol{X};\\boldsymbol{\\Theta})}=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c04ca4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Expression for the gradients\n",
    "\n",
    "This leads to the following equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f215e9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{p(\\boldsymbol{X};\\boldsymbol{\\Theta})}=\\nabla_{\\boldsymbol{\\Theta}}\\left(\\sum_{x_i\\in \\boldsymbol{X}}\\log{f(x_i;\\boldsymbol{\\Theta})}\\right)-\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ba0e9",
   "metadata": {
    "editable": true
   },
   "source": [
    "The first term is called the positive phase and we assume that we have a model for the function $f$ from which we can sample values. Below we will develop an explicit model for this.\n",
    "The second term is called the negative phase and is the one which leads to more difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1cbab4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Contrastive optimization\n",
    "The evaluation of these two terms leads to what in the literature is called contrastive optimization.\n",
    "\n",
    "If we optimize the negative **log** of the PDF, the aboves phases simply change sign.\n",
    "\n",
    "For a further discussion of energy-based models, see the notes by Philip Lippe at <https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50dbd3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The derivative of the partition function\n",
    "\n",
    "The partition function, defined above as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee1b8e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "Z(\\boldsymbol{\\Theta})=\\sum_{x_i\\in \\boldsymbol{X}}\\sum_{h_j\\in \\boldsymbol{H}} f(x_i,h_j;\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd7d619",
   "metadata": {
    "editable": true
   },
   "source": [
    "is in general the most problematic term. In principle both $x$ and $h$ can span large degrees of freedom, if not even infinitely many ones, and computing the partition function itself is often not desirable or even feasible. The above derivative of the partition function can however be written in terms of an expectation value which is in turn evaluated  using Monte Carlo sampling and the theory of Markov chains, popularly shortened to MCMC (or just MC$^2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a1a41",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Explicit expression for the derivative\n",
    "We can rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ce4ef",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\frac{\\nabla_{\\boldsymbol{\\Theta}}Z(\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b188116",
   "metadata": {
    "editable": true
   },
   "source": [
    "which reads in more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d23c0b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\frac{\\nabla_{\\boldsymbol{\\Theta}} \\sum_{x_i\\in \\boldsymbol{X}}f(x_i;\\boldsymbol{\\Theta})   }{Z(\\boldsymbol{\\Theta})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e40951",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can rewrite the function $f$ (we have assumed that is larger or\n",
    "equal than zero) as $f=\\exp{\\log{f}}$. We can then reqrite the last\n",
    "equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca2756",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\frac{ \\sum_{x_i\\in \\boldsymbol{X}} \\nabla_{\\boldsymbol{\\Theta}}\\exp{\\log{f(x_i;\\boldsymbol{\\Theta})}}   }{Z(\\boldsymbol{\\Theta})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23e42f5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Final expression\n",
    "\n",
    "Taking the derivative gives us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bf7382",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\frac{ \\sum_{x_i\\in \\boldsymbol{X}}f(x_i;\\boldsymbol{\\Theta}) \\nabla_{\\boldsymbol{\\Theta}}\\log{f(x_i;\\boldsymbol{\\Theta})}   }{Z(\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d63919a",
   "metadata": {
    "editable": true
   },
   "source": [
    "which is the expectation value of $\\log{f}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e226ca8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\sum_{x_i\\in \\boldsymbol{X}}p(x_i;\\boldsymbol{\\Theta}) \\nabla_{\\boldsymbol{\\Theta}}\\log{f(x_i;\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eead37",
   "metadata": {
    "editable": true
   },
   "source": [
    "that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f9eed0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\mathbb{E}(\\log{f(x_i;\\boldsymbol{\\Theta})}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923a636",
   "metadata": {
    "editable": true
   },
   "source": [
    "This quantity is evaluated using Monte Carlo sampling, with Gibbs\n",
    "sampling as the standard sampling rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce94d5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Generative model,  basic overview (Borrowed from Rashcka et al)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/figure1.png, width=900 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/figure1.png\" width=\"900\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f54fccf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reminder on VAEs\n",
    "\n",
    "Mathematically, we can imagine the latent variables and the data we\n",
    "observe as modeled by a joint distribution $p(\\boldsymbol{x}, \\boldsymbol{h};\\boldsymbol{\\Theta})$.  Recall one\n",
    "approach of generative modeling, termed likelihood-based, is to\n",
    "learn a model to maximize the likelihood $p(\\boldsymbol{x};\\boldsymbol{\\Theta})$ of all observed\n",
    "$\\boldsymbol{x}$.  There are two ways we can manipulate this joint distribution\n",
    "to recover the likelihood of purely our observed data $p(\\boldsymbol{x};\\boldsymbol{\\Theta})$; we can\n",
    "explicitly marginalize\n",
    "out the latent variable $\\boldsymbol{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21377fc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x}) = \\int p(\\boldsymbol{x}, \\boldsymbol{h})d\\boldsymbol{h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7aefda",
   "metadata": {
    "editable": true
   },
   "source": [
    "or, we could also appeal to the chain rule of probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df9e10",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x}) = \\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{p(\\boldsymbol{h}|\\boldsymbol{x})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679bef5f",
   "metadata": {
    "editable": true
   },
   "source": [
    "We suppress here the dependence\ton the optimization parameters $\\boldsymbol{\\Theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5b979",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Evidence Lower Bound\n",
    "Directly computing and maximizing the likelihood $p(\\boldsymbol{x})$ is\n",
    "difficult because it either involves integrating out all latent\n",
    "variables $\\boldsymbol{h}$, which is intractable for\n",
    "complex models, or it involves having access to a ground truth latent\n",
    "encoder $p(\\boldsymbol{h}|\\boldsymbol{x})$.\n",
    "\n",
    "Using the last  two equations, we can derive a term called the \\textbf{E}vidence\n",
    "\\textbf{L}ower \\textbf{Bo}und (ELBO), which as its name suggests, is a lower\n",
    "  bound of the evidence.  The evidence is quantified in this case as\n",
    "the log likelihood of the observed data.  Then, maximizing the ELBO\n",
    "becomes a proxy objective with which to optimize a latent variable\n",
    "model; in the best case, when the ELBO is powerfully parameterized and\n",
    "perfectly optimized, it becomes exactly equivalent to the evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4991c012",
   "metadata": {
    "editable": true
   },
   "source": [
    "## ELBO equations\n",
    "Formally, the equation of the ELBO is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b8d53",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78603f8e",
   "metadata": {
    "editable": true
   },
   "source": [
    "To make the relationship with the evidence explicit, we can mathematically write:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b7ca7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\log p(\\boldsymbol{x}) \\geq \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09a099b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Introducing the encoder function\n",
    "\n",
    "Here, $q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})$ is a flexible approximate\n",
    "variational distribution with parameters $\\boldsymbol{\\phi}$ that we seek to\n",
    "optimize.  Intuitively, it can be thought of as a parameterizable\n",
    "model that is learned to estimate the true distribution over latent\n",
    "variables for given observations $\\boldsymbol{x}$; in other words, it seeks to\n",
    "approximate true posterior $p(\\boldsymbol{h}|\\boldsymbol{x})$.  As we saw last week when we\n",
    "explored Variational Autoencoders, as we increase the lower bound\n",
    "by tuning the parameters $\\boldsymbol{\\phi}$ to maximize the ELBO, we gain\n",
    "access to components that can be used to model the true data\n",
    "distribution and sample from it, thus learning a generative model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb1c58",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The derivation from last week\n",
    "\n",
    "To better understand the relationship between the evidence and the ELBO, let us perform another derivation, this time using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ffc565",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\log p(\\boldsymbol{x}) & = \\log p(\\boldsymbol{x}) \\int q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})dz && \\text{(Multiply by $1 = \\int q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})d\\boldsymbol{h}$)}\\\\\n",
    "          & = \\int q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})(\\log p(\\boldsymbol{x}))dz && \\text{(Bring evidence into integral)}\\\\\n",
    "          & = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log p(\\boldsymbol{x})\\right] && \\text{(Definition of Expectation)}\\\\\n",
    "          & = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{p(\\boldsymbol{h}|\\boldsymbol{x})}\\right]&& \\\\\n",
    "          & = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}{p(\\boldsymbol{h}|\\boldsymbol{x})q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right]&& \\text{(Multiply by $1 = \\frac{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}$)}\\\\\n",
    "          & = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right] + \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}{p(\\boldsymbol{h}|\\boldsymbol{x})}\\right] && \\text{(Split the Expectation)}\\\\\n",
    "          & = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right] +\n",
    "\t  KL(q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})\\vert\\vert p(\\boldsymbol{h}|\\boldsymbol{x}))  && \\text{(Definition of KL Divergence)}\\\\\n",
    "          & \\geq \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right]  && \\text{(KL Divergence always $\\geq 0$)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f823e8b4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Analysis\n",
    "\n",
    "From this derivation, we clearly observe from the last equation\n",
    "that the evidence is equal to the ELBO plus the KL Divergence between\n",
    "the approximate posterior $q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})$ and the true\n",
    "posterior $p(\\boldsymbol{h}|\\boldsymbol{x})$.  Understanding this term is the\n",
    "key to understanding not only the relationship between the ELBO and\n",
    "the evidence, but also the reason why optimizing the ELBO is an\n",
    "appropriate objective at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842168f3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The VAE\n",
    "\n",
    "In the default formulation of the VAE by Kingma and Welling (2015), we directly maximize the ELBO.  This\n",
    "approach is \\textit{variational}, because we optimize for the best\n",
    "$q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})$ amongst a family of potential posterior\n",
    "distributions parameterized by $\\boldsymbol{\\phi}$.  It is called an\n",
    "\\textit{autoencoder} because it is reminiscent of a traditional\n",
    "autoencoder model, where input data is trained to predict itself after\n",
    "undergoing an intermediate bottlenecking representation step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7425433",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Dissecting the equations\n",
    "To make\n",
    "this connection explicit, let us dissect the ELBO term further:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0a4ad",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "{\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right]}\n",
    "&= {\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{h})p(\\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right]}         && {\\text{(Chain Rule of Probability)}}\\\\\n",
    "&= {\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{h})\\right] + \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right]}         && {\\text{(Split the Expectation)}}\\\\\n",
    "&= \\underbrace{{\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{h})\\right]}}_\\text{reconstruction term} - \\underbrace{{KL(q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\vert\\vert{p(\\boldsymbol{h}))}}_\\text{prior matching term} && {\\text{(Definition of KL Divergence)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedc1e37",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Bottlenecking distribution\n",
    "\n",
    "In this case, we learn an intermediate bottlenecking distribution\n",
    "$q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})$ that can be treated as\n",
    "an \\textit{encoder}; it transforms inputs into a distribution over\n",
    "possible latents.  Simultaneously, we learn a deterministic function\n",
    "$p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{h})$ to convert a given latent vector\n",
    "$\\boldsymbol{h}$ into an observation $\\boldsymbol{x}$, which can be interpreted as\n",
    "a \\textit{decoder}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4cc30b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Decoder and encoder\n",
    "The two terms in the last equation each have intuitive descriptions: the first\n",
    "term measures the reconstruction likelihood of the decoder from our\n",
    "variational distribution; this ensures that the learned distribution\n",
    "is modeling effective latents that the original data can be\n",
    "regenerated from.  The second term measures how similar the learned\n",
    "variational distribution is to a prior belief held over latent\n",
    "variables.  Minimizing this term encourages the encoder to actually\n",
    "learn a distribution rather than collapse into a Dirac delta function.\n",
    "Maximizing the ELBO is thus equivalent to maximizing its first term\n",
    "and minimizing its second term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd295453",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Defining feature of VAEs\n",
    "\n",
    "A defining feature of the VAE is how the ELBO is optimized jointly over parameters $\\boldsymbol{\\phi}$ and $\\boldsymbol{\\theta}$.  The encoder of the VAE is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior is often selected to be a standard multivariate Gaussian:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445166d6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x}) &= N(\\boldsymbol{h}; \\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\boldsymbol{x}), \\boldsymbol{\\sigma}_{\\boldsymbol{\\phi}}^2(\\boldsymbol{x})\\textbf{I})\\\\\n",
    "    p(\\boldsymbol{h}) &= N(\\boldsymbol{h}; \\boldsymbol{0}, \\textbf{I})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fb079",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Analytical evaluation\n",
    "\n",
    "Then, the KL divergence term of the ELBO can be computed analytically, and the reconstruction term can be approximated using a Monte Carlo estimate.  Our objective can then be rewritten as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ad2c7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "  argmax_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}} \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{h})\\right] - KL(q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})\\vert\\vert p(\\boldsymbol{h})) \\approx argmax_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}} \\sum_{l=1}^{L}\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{h}^{(l)}) - KL(q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})\\vert\\vert p(\\boldsymbol{h}))\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b040d5",
   "metadata": {
    "editable": true
   },
   "source": [
    "where latents $\\{\\boldsymbol{h}^{(l)}\\}_{l=1}^L$ are sampled from $q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})$, for every observation $\\boldsymbol{x}$ in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d2f29",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reparameterization trick\n",
    "\n",
    "However, a problem arises in this default setup: each $\\boldsymbol{h}^{(l)}$\n",
    "that our loss is computed on is generated by a stochastic sampling\n",
    "procedure, which is generally non-differentiable.  Fortunately, this\n",
    "can be addressed via the \\textit{reparameterization trick} when\n",
    "$q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})$ is designed to model certain\n",
    "distributions, including the multivariate Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eb4062",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Actual implementation\n",
    "\n",
    "The reparameterization trick rewrites a random variable as a\n",
    "deterministic function of a noise variable; this allows for the\n",
    "optimization of the non-stochastic terms through gradient descent.\n",
    "For example, samples from a normal distribution\n",
    "$x \\sim N(x;\\mu, \\sigma^2)$ with arbitrary mean $\\mu$ and\n",
    "variance $\\sigma^2$ can be rewritten as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fcdaed",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    x &= \\mu + \\sigma\\epsilon \\quad \\text{with } \\epsilon \\sim N(\\epsilon; 0, \\boldsymbol{I})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8bebd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Interpretation\n",
    "An arbitrary Gaussian distributions can be interpreted as\n",
    "standard Gaussians (of which $\\epsilon$ is a sample) that have their\n",
    "mean shifted from zero to the target mean $\\mu$ by addition, and their\n",
    "variance stretched by the target variance $\\sigma^2$.  Therefore, by\n",
    "the reparameterization trick, sampling from an arbitrary Gaussian\n",
    "distribution can be performed by sampling from a standard Gaussian,\n",
    "scaling the result by the target standard deviation, and shifting it\n",
    "by the target mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae4c7b8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Deterministic function\n",
    "\n",
    "In a VAE, each $\\boldsymbol{h}$ is thus computed as a deterministic function of input $\\boldsymbol{x}$ and auxiliary noise variable $\\boldsymbol{\\epsilon}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e41352",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{h} &= \\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\boldsymbol{x}) + \\boldsymbol{\\sigma}_{\\boldsymbol{\\phi}}(\\boldsymbol{x})\\odot\\boldsymbol{\\epsilon} \\quad \\text{with } \\boldsymbol{\\epsilon} \\sim N(\\boldsymbol{\\epsilon};\\boldsymbol{0}, \\textbf{I})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa23ac7f",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\odot$ represents an element-wise product.  Under this\n",
    "reparameterized version of $\\boldsymbol{h}$, gradients can then be computed\n",
    "with respect to $\\boldsymbol{\\phi}$ as desired, to optimize\n",
    "$\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}$ and $\\boldsymbol{\\sigma}_{\\boldsymbol{\\phi}}$.  The VAE\n",
    "therefore utilizes the reparameterization trick and Monte Carlo\n",
    "estimates to optimize the ELBO jointly over $\\boldsymbol{\\phi}$ and\n",
    "$\\boldsymbol{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee357fd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## After training\n",
    "\n",
    "After training a VAE, generating new data can be performed by sampling\n",
    "directly from the latent space $p(\\boldsymbol{h})$ and then running it through\n",
    "the decoder.  Variational Autoencoders are particularly interesting\n",
    "when the dimensionality of $\\boldsymbol{h}$ is less than that of input\n",
    "$\\boldsymbol{x}$, as we might then be learning compact, useful\n",
    "representations.  Furthermore, when a semantically meaningful latent\n",
    "space is learned, latent vectors can be edited before being passed to\n",
    "the decoder to more precisely control the data generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3875016",
   "metadata": {
    "editable": true
   },
   "source": [
    "## What is a GAN?\n",
    "\n",
    "A GAN is a deep neural network which consists of two networks, a\n",
    "so-called generator network and a discriminating network, or just\n",
    "discriminator. Through several iterations of generation and\n",
    "discrimination, the idea is that these networks will train each other,\n",
    "while also trying to outsmart each other.\n",
    "\n",
    "In its simplest version, the two networks could be two standard neural networks with a given number of hidden of hidden layers and parameters to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3bfd91",
   "metadata": {
    "editable": true
   },
   "source": [
    "## What is a generator network?\n",
    "\n",
    "A generator network is often a deep network which uses existing data\n",
    "to generate new data (from for example simulations of physical\n",
    "systems, imagesm video, audio and more) from randomly generated\n",
    "inputs, the so-called latent space. Training the network allows us to\n",
    "generate say new data, images etc. As an example a generator network\n",
    "could for example be a Boltzmann machine as discussed earlier. This\n",
    "machine is trained to produce for example a quantum mechanical\n",
    "probability distribution.\n",
    "\n",
    "It can be a simple neural network with an input layer and an output layer and a given number of hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460118a9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## And what is a discriminator network?\n",
    "\n",
    "A discriminator tries to distinguish between real data and those generated by the abovementioned generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f41ad",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Appplications of GANs\n",
    "\n",
    "There are exteremely many applications of GANs\n",
    "1. Image generation\n",
    "\n",
    "2. Text-to-image analysis\n",
    "\n",
    "3. Face-aging\n",
    "\n",
    "4. Image-to-image translation\n",
    "\n",
    "5. Video synthesis\n",
    "\n",
    "6. High-resolution image generation\n",
    "\n",
    "7. Completing missing parts of images and much more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8be37d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Discriminator versus generator  (Borrowed from Rashcka et al)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/figure2.png, width=900 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/figure2.png\" width=\"900\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fedf1ce",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Generative Adversarial Networks\n",
    "\n",
    "**Generative Adversarial Networks** are a type of unsupervised machine learning\n",
    "algorithm proposed by Goodfellow et. al, see <https://arxiv.org/pdf/1406.2661.pdf>\n",
    "in 2014 (Read the paper first it's only 6 pages). The simplest formulation of\n",
    "the model is based on a game theoretic approach, *zero sum game*, where we pit\n",
    "two neural networks against one another. We define two rival networks, one\n",
    "generator $g$, and one discriminator $d$. The generator directly produces\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768de371",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "x = g(z; \\theta^{(g)}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8545eb3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Discriminator\n",
    "\n",
    "The discriminator attempts to distinguish between samples drawn from the\n",
    "training data and samples drawn from the generator. In other words, it tries to\n",
    "tell the difference between the fake data produced by $g$ and the actual data\n",
    "samples we want to do prediction on. The discriminator outputs a probability\n",
    "value given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47708b6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "d(x; \\theta^{(d)}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b52e4",
   "metadata": {
    "editable": true
   },
   "source": [
    "indicating the probability that $x$ is a real training example rather than a\n",
    "fake sample the generator has generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec34e1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Zero-sum game\n",
    "\n",
    "The simplest way to formulate the\n",
    "learning process in a generative adversarial network is a zero-sum game, in\n",
    "which a function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28e77d8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v(\\theta^{(g)}, \\theta^{(d)}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad49aef",
   "metadata": {
    "editable": true
   },
   "source": [
    "determines the reward for the discriminator, while the generator gets the\n",
    "conjugate reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99600bbb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-v(\\theta^{(g)}, \\theta^{(d)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77fe58",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Maximizing reward\n",
    "\n",
    "During learning both of the networks maximize their own reward function, so that\n",
    "the generator gets better and better at tricking the discriminator, while the\n",
    "discriminator gets better and better at telling the difference between the fake\n",
    "and real data. The generator and discriminator alternate on which one trains at\n",
    "one time (i.e. for one epoch). In other words, we keep the generator constant\n",
    "and train the discriminator, then we keep the discriminator constant to train\n",
    "the generator and repeat. It is this back and forth dynamic which lets GANs\n",
    "tackle otherwise intractable generative problems. As the generator improves with\n",
    " training, the discriminator's performance gets worse because it cannot easily\n",
    " tell the difference between real and fake. If the generator ends up succeeding\n",
    " perfectly, the the discriminator will do no better than random guessing i.e.\n",
    " 50\\%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d2bd80",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Progression in training\n",
    "\n",
    "This progression in the training poses a problem for the convergence\n",
    " criteria for GANs. The discriminator feedback gets less meaningful over time,\n",
    " if we continue training after this point then the generator is effectively\n",
    " training on junk data which can undo the learning up to that point. Therefore,\n",
    " we stop training when the discriminator starts outputting $1/2$ everywhere.\n",
    " At convergence we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138391e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "g^* = \\underset{g}{\\mathrm{argmin}}\\hspace{2pt}\n",
    "          \\underset{d}{\\mathrm{max}}v(\\theta^{(g)}, \\theta^{(d)}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681cf86e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Deafault choice\n",
    "The default choice for $v$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac77c2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v(\\theta^{(g)}, \\theta^{(d)}) = \\mathbb{E}_{x\\sim p_\\mathrm{data}}\\log d(x)\n",
    "                                  + \\mathbb{E}_{x\\sim p_\\mathrm{model}}\n",
    "                                  \\log (1 - d(x)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc1074",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Design of GANs\n",
    "The main motivation for the design of GANs is that the learning process requires\n",
    "neither approximate inference (variational autoencoders for example) nor\n",
    "approximation of a partition function. In the case where"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27174f9c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\underset{d}{\\mathrm{max}}v(\\theta^{(g)}, \\theta^{(d)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852c00b",
   "metadata": {
    "editable": true
   },
   "source": [
    "is convex in $\\theta^{(g)}$ then the procedure is guaranteed to converge and is\n",
    "asymptotically consistent\n",
    "( [Seth Lloyd on QuGANs](https://arxiv.org/pdf/1804.09139.pdf)  ). This is in\n",
    "general not the case and it is possible to get situations where the training\n",
    "process never converges because the generator and discriminator chase one\n",
    "another around in the parameter space indefinitely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad257d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Steps in building a GAN  (Borrowed from Rashcka et al)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/figure3.png, width=900 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/figure3.png\" width=\"900\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b808f53",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More references\n",
    "\n",
    "A much deeper discussion on\n",
    "the currently open research problem of GAN convergence is available\n",
    "from <https://www.deeplearningbook.org/contents/generative_models.html>. To\n",
    "anyone interested in learning more about GANs it is a highly recommended read.\n",
    "Direct quote: **In this best-performing formulation, the generator aims to\n",
    "increase the log probability that the discriminator makes a mistake, rather than\n",
    "aiming to decrease the log probability that the discriminator makes the correct\n",
    "prediction.** Another interesting read can be found at <https://arxiv.org/abs/1701.00160>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645edce6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Writing Our First Generative Adversarial Network\n",
    "\n",
    "This part is best seen using the jupyter-notebook. We follow here\n",
    "closely the code developed by Raschka et al from chapter 17 of their\n",
    "textbook, see <https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch17> for codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a8a49",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Implementing the networks   (Borrowed from Rashcka et al)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/figure4.png, width=900 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/figure4.png\" width=\"900\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da053473",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Code elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240348d6",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35aa94f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Setting up the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0c26c62",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## define a function for the generator:\n",
    "def make_generator_network(\n",
    "        input_size=20,\n",
    "        num_hidden_layers=1,\n",
    "        num_hidden_units=100,\n",
    "        num_output_units=784):\n",
    "    model = nn.Sequential()\n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add_module(f'fc_g{i}', \n",
    "                         nn.Linear(input_size, \n",
    "                                   num_hidden_units)) \n",
    "        model.add_module(f'relu_g{i}', \n",
    "                         nn.LeakyReLU())     \n",
    "        input_size = num_hidden_units\n",
    "    model.add_module(f'fc_g{num_hidden_layers}', \n",
    "                    nn.Linear(input_size, num_output_units))   \n",
    "    model.add_module('tanh_g', nn.Tanh())      \n",
    "    return model\n",
    "\n",
    "## define a function for the discriminator:\n",
    "def make_discriminator_network(\n",
    "        input_size,\n",
    "        num_hidden_layers=1,\n",
    "        num_hidden_units=100,\n",
    "        num_output_units=1):\n",
    "    model = nn.Sequential()\n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add_module(f'fc_d{i}', \n",
    "                 nn.Linear(input_size, \n",
    "                           num_hidden_units, bias=False)) \n",
    "        model.add_module(f'relu_d{i}', \n",
    "                         nn.LeakyReLU())  \n",
    "        model.add_module('dropout', nn.Dropout(p=0.5))\n",
    "        input_size = num_hidden_units\n",
    "    model.add_module(f'fc_d{num_hidden_layers}', \n",
    "                     nn.Linear(input_size, num_output_units))   \n",
    "    model.add_module('sigmoid', nn.Sigmoid())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d664b6a7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Printing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38edb40a",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image_size = (28, 28)\n",
    "z_size = 20\n",
    "\n",
    "gen_hidden_layers = 1\n",
    "gen_hidden_size = 100\n",
    "disc_hidden_layers = 1\n",
    "disc_hidden_size = 100\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "gen_model = make_generator_network(\n",
    "    input_size=z_size,\n",
    "    num_hidden_layers=gen_hidden_layers, \n",
    "    num_hidden_units=gen_hidden_size,\n",
    "    num_output_units=np.prod(image_size))\n",
    " \n",
    "print(gen_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613f9679",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "disc_model = make_discriminator_network(\n",
    "    input_size=np.prod(image_size),\n",
    "    num_hidden_layers=disc_hidden_layers,\n",
    "    num_hidden_units=disc_hidden_size)\n",
    "\n",
    "print(disc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8210a270",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Defining the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff1006f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torchvision \n",
    "from torchvision import transforms \n",
    "\n",
    "\n",
    "image_path = './'\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5), std=(0.5)),\n",
    "])\n",
    "mnist_dataset = torchvision.datasets.MNIST(root=image_path, \n",
    "                                           train=True, \n",
    "                                           transform=transform, \n",
    "                                           download=True)\n",
    "\n",
    "example, label = next(iter(mnist_dataset))\n",
    "print(f'Min: {example.min()} Max: {example.max()}')\n",
    "print(example.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f1e0d3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Defining the training set, part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b928eaa8",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_noise(batch_size, z_size, mode_z):\n",
    "    if mode_z == 'uniform':\n",
    "        input_z = torch.rand(batch_size, z_size)*2 - 1 \n",
    "    elif mode_z == 'normal':\n",
    "        input_z = torch.randn(batch_size, z_size)\n",
    "    return input_z\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(mnist_dataset, batch_size, shuffle=False)\n",
    "input_real, label = next(iter(dataloader))\n",
    "input_real = input_real.view(batch_size, -1)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "mode_z = 'uniform'  # 'uniform' vs. 'normal'\n",
    "input_z = create_noise(batch_size, z_size, mode_z)\n",
    "\n",
    "print('input-z -- shape:', input_z.shape)\n",
    "print('input-real -- shape:', input_real.shape)\n",
    "\n",
    "g_output = gen_model(input_z)\n",
    "print('Output of G -- shape:', g_output.shape)\n",
    "\n",
    "d_proba_real = disc_model(input_real)\n",
    "d_proba_fake = disc_model(g_output)\n",
    "print('Disc. (real) -- shape:', d_proba_real.shape)\n",
    "print('Disc. (fake) -- shape:', d_proba_fake.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0dcb02",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ab896c",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "## Loss for the Generator\n",
    "g_labels_real = torch.ones_like(d_proba_fake)\n",
    "g_loss = loss_fn(d_proba_fake, g_labels_real)\n",
    "print(f'Generator Loss: {g_loss:.4f}')\n",
    "\n",
    "## Loss for the Discriminator\n",
    "d_labels_real = torch.ones_like(d_proba_real)\n",
    "d_labels_fake = torch.zeros_like(d_proba_fake)\n",
    "\n",
    "d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
    "d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n",
    "print(f'Discriminator Losses: Real {d_loss_real:.4f} Fake {d_loss_fake:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6146dc",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f802bfbf",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "## Set up the dataset\n",
    "mnist_dl = DataLoader(mnist_dataset, batch_size=batch_size, \n",
    "                      shuffle=True, drop_last=True)\n",
    " \n",
    "## Set up the models\n",
    "gen_model = make_generator_network(\n",
    "    input_size=z_size,\n",
    "    num_hidden_layers=gen_hidden_layers, \n",
    "    num_hidden_units=gen_hidden_size,\n",
    "    num_output_units=np.prod(image_size)).to(device)\n",
    " \n",
    "disc_model = make_discriminator_network(\n",
    "    input_size=np.prod(image_size),\n",
    "    num_hidden_layers=disc_hidden_layers,\n",
    "    num_hidden_units=disc_hidden_size).to(device)\n",
    " \n",
    "## Loss function and optimizers:\n",
    "loss_fn = nn.BCELoss()\n",
    "g_optimizer = torch.optim.Adam(gen_model.parameters())\n",
    "d_optimizer = torch.optim.Adam(disc_model.parameters())\n",
    "\n",
    "## Train the discriminator\n",
    "def d_train(x):\n",
    "    disc_model.zero_grad()\n",
    "\n",
    "    # Train discriminator with a real batch\n",
    "    batch_size = x.size(0)\n",
    "    x = x.view(batch_size, -1).to(device)\n",
    "    d_labels_real = torch.ones(batch_size, 1, device=device)\n",
    "\n",
    "    d_proba_real = disc_model(x)\n",
    "    d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
    "\n",
    "    # Train discriminator on a fake batch\n",
    "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
    "    g_output = gen_model(input_z)\n",
    "    \n",
    "    d_proba_fake = disc_model(g_output)\n",
    "    d_labels_fake = torch.zeros(batch_size, 1, device=device)\n",
    "    d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n",
    "\n",
    "    # gradient backprop & optimize ONLY D's parameters\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "  \n",
    "    return d_loss.data.item(), d_proba_real.detach(), d_proba_fake.detach()\n",
    "\n",
    "## Train the generator\n",
    "def g_train(x):\n",
    "    gen_model.zero_grad()\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
    "    g_labels_real = torch.ones(batch_size, 1, device=device)\n",
    "\n",
    "    g_output = gen_model(input_z)\n",
    "    d_proba_fake = disc_model(g_output)\n",
    "    g_loss = loss_fn(d_proba_fake, g_labels_real)\n",
    "\n",
    "    # gradient backprop & optimize ONLY G's parameters\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "        \n",
    "    return g_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99469899",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fixed_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
    "\n",
    "def create_samples(g_model, input_z):\n",
    "    g_output = g_model(input_z)\n",
    "    images = torch.reshape(g_output, (batch_size, *image_size))    \n",
    "    return (images+1)/2.0\n",
    "\n",
    "epoch_samples = []\n",
    "\n",
    "all_d_losses = []\n",
    "all_g_losses = []\n",
    "\n",
    "all_d_real = []\n",
    "all_d_fake = []\n",
    "\n",
    "num_epochs = 100\n",
    "torch.manual_seed(1)\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    d_losses, g_losses = [], []\n",
    "    d_vals_real, d_vals_fake = [], []\n",
    "    for i, (x, _) in enumerate(mnist_dl):\n",
    "        d_loss, d_proba_real, d_proba_fake = d_train(x)\n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_train(x))\n",
    "        \n",
    "        d_vals_real.append(d_proba_real.mean().cpu())\n",
    "        d_vals_fake.append(d_proba_fake.mean().cpu())\n",
    "        \n",
    "    all_d_losses.append(torch.tensor(d_losses).mean())\n",
    "    all_g_losses.append(torch.tensor(g_losses).mean())\n",
    "    all_d_real.append(torch.tensor(d_vals_real).mean())\n",
    "    all_d_fake.append(torch.tensor(d_vals_fake).mean())\n",
    "    print(f'Epoch {epoch:03d} | Avg Losses >>'\n",
    "          f' G/D {all_g_losses[-1]:.4f}/{all_d_losses[-1]:.4f}'\n",
    "          f' [D-Real: {all_d_real[-1]:.4f} D-Fake: {all_d_fake[-1]:.4f}]')\n",
    "    epoch_samples.append(\n",
    "        create_samples(gen_model, fixed_z).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c136e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66d43d70",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "## Plotting the losses\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    " \n",
    "plt.plot(all_g_losses, label='Generator loss')\n",
    "half_d_losses = [all_d_loss/2 for all_d_loss in all_d_losses]\n",
    "plt.plot(half_d_losses, label='Discriminator loss')\n",
    "plt.legend(fontsize=20)\n",
    "ax.set_xlabel('Iteration', size=15)\n",
    "ax.set_ylabel('Loss', size=15)\n",
    "\n",
    "## Plotting the outputs of the discriminator\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "plt.plot(all_d_real, label=r'Real: $D(\\mathbf{x})$')\n",
    "plt.plot(all_d_fake, label=r'Fake: $D(G(\\mathbf{z}))$')\n",
    "plt.legend(fontsize=20)\n",
    "ax.set_xlabel('Iteration', size=15)\n",
    "ax.set_ylabel('Discriminator output', size=15)\n",
    "\n",
    "#plt.savefig('figures/ch17-gan-learning-curve.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d6b8eeb",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "selected_epochs = [1, 2, 4, 10, 50, 100]\n",
    "fig = plt.figure(figsize=(10, 14))\n",
    "for i,e in enumerate(selected_epochs):\n",
    "    for j in range(5):\n",
    "        ax = fig.add_subplot(6, 5, i*5+j+1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if j == 0:\n",
    "            ax.text(\n",
    "                -0.06, 0.5, f'Epoch {e}',\n",
    "                rotation=90, size=18, color='red',\n",
    "                horizontalalignment='right',\n",
    "                verticalalignment='center', \n",
    "                transform=ax.transAxes)\n",
    "        \n",
    "        image = epoch_samples[e-1][j]\n",
    "        ax.imshow(image, cmap='gray_r')\n",
    "    \n",
    "#plt.savefig('figures/ch17-vanila-gan-samples.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f66d1a0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Calculating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba85ca3f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def distance(X, Y, sqrt):\n",
    "    nX = X.size(0)\n",
    "    nY = Y.size(0)\n",
    "    X = X.view(nX,-1).cuda()\n",
    "    X2 = (X*X).sum(1).resize_(nX,1)\n",
    "    Y = Y.view(nY,-1).cuda()\n",
    "    Y2 = (Y*Y).sum(1).resize_(nY,1)\n",
    "\n",
    "    M = torch.zeros(nX, nY)\n",
    "    M.copy_(X2.expand(nX,nY) + Y2.expand(nY,nX).transpose(0,1) - 2*torch.mm(X,Y.transpose(0,1)))\n",
    "\n",
    "    del X, X2, Y, Y2\n",
    "    \n",
    "    if sqrt:\n",
    "        M = ((M+M.abs())/2).sqrt()\n",
    "    \n",
    "    return M\n",
    "\n",
    "def mmd(Mxx, Mxy, Myy, sigma) :\n",
    "    scale = Mxx.mean()\n",
    "    Mxx = torch.exp(-Mxx/(scale*2*sigma*sigma))\n",
    "    Mxy = torch.exp(-Mxy/(scale*2*sigma*sigma))\n",
    "    Myy = torch.exp(-Myy/(scale*2*sigma*sigma))\n",
    "    a = Mxx.mean()+Myy.mean()-2*Mxy.mean()\n",
    "    mmd = math.sqrt(max(a, 0))\n",
    "\n",
    "    return mmd\n",
    "\n",
    "def compute_score(fake, real , k=1, sigma=1, sqrt=True):\n",
    "\n",
    "    Mxx = distance(real, real, False)\n",
    "    Mxy = distance(real, fake, False)\n",
    "    Myy = distance(fake, fake, False)\n",
    "\n",
    " \n",
    "    print(mmd(Mxx, Mxy, Myy, sigma))\n",
    "\n",
    "whole_dl = DataLoader(mnist_dataset, batch_size=10000, \n",
    "                      shuffle=True, drop_last=True) \n",
    "real_image = next(iter(whole_dl))[0]\n",
    "compute_score(torch.from_numpy(epoch_samples[-1]), real_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd6ed7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Diffusion models, basics\n",
    "\n",
    "Diffusion models are inspired by non-equilibrium thermodynamics. They\n",
    "define a Markov chain of diffusion steps to slowly add random noise to\n",
    "data and then learn to reverse the diffusion process to construct\n",
    "desired data samples from the noise. Unlike VAE or flow models,\n",
    "diffusion models are learned with a fixed procedure and the latent\n",
    "variable has high dimensionality (same as the original data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fee89b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematics of diffusion models\n",
    "\n",
    "See whiteboard notes"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
