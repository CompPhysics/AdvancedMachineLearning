<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week15-reveal.html week15-reveal reveal --html_slide_theme=beige
-->
<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Advanced machine learning and data analysis for the physical sciences">
<title>Advanced machine learning and data analysis for the physical sciences</title>

<!-- reveal.js: https://lab.hakim.se/reveal-js/ -->

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<!--
<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/beigesmall.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/serif.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/moon.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/sky.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/darkgray.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/cbc.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simula.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/league.css" id="theme">
-->

<!-- For syntax highlighting -->
<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>

<style type="text/css">
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
.reveal .alert-text-small   { font-size: 80%;  }
.reveal .alert-text-large   { font-size: 130%; }
.reveal .alert-text-normal  { font-size: 90%;  }
.reveal .alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:5px solid #bababa;
  -webkit-border-radius: 14px; -moz-border-radius: 14px;
  border-radius:14px;
  background-position: 10px 10px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 30px; /* 55px; if icon */
}
.reveal .alert-block {padding-top:14px; padding-bottom:14px}
.reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
/*.reveal .alert li {margin-top: 1em}*/
.reveal .alert-block p+p {margin-top:5px}
/*.reveal .alert-notice { background-image: url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_notice.png); }
.reveal .alert-summary  { background-image:url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_summary.png); }
.reveal .alert-warning { background-image: url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_warning.png); }
.reveal .alert-question {background-image:url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_question.png); } */
/* Override reveal.js table border */
.reveal table td {
  border: 0;
}

<style type="text/css">
/* Override h1, h2, ... styles */
h1 { font-size: 2.8em; }
h2 { font-size: 1.5em; }
h3 { font-size: 1.4em; }
h4 { font-size: 1.3em; }
h1, h2, h3, h4 { font-weight: bold; line-height: 1.2; }
body { overflow: auto; } /* vertical scrolling */
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
.slide .alert-text-small   { font-size: 80%;  }
.slide .alert-text-large   { font-size: 130%; }
.slide .alert-text-normal  { font-size: 90%;  }
.slide .alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:5px solid #bababa;
    -webkit-border-radius:14px; -moz-border-radius:14px;
  border-radius:14px
  background-position: 10px 10px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 30px; /* 55px; if icon */
}
.slide .alert-block {padding-top:14px; padding-bottom:14px}
.slide .alert-block > p, .alert-block > ul {margin-bottom:0}
/*.slide .alert li {margin-top: 1em}*/
.deck .alert-block p+p {margin-top:5px}
/*.slide .alert-notice { background-image: url(https://hplgit.github.io/doconce/
bundled/html_images//small_gray_notice.png); }
.slide .alert-summary  { background-image:url(https://hplgit.github.io/doconce/
bundled/html_images//small_gray_summary.png); }
.slide .alert-warning { background-image: url(https://hplgit.github.io/doconce/
bundled/html_images//small_gray_warning.png); }
.slide .alert-question {background-image:url(https://hplgit.github.io/doconce/
bundled/html_images/small_gray_question.png); } */
.dotable table, .dotable th, .dotable tr, .dotable tr td {
  border: 2px solid black;
  border-collapse: collapse;
  padding: 2px;
}
</style>


<!-- Styles for table layout of slides -->
<style type="text/css">
td.padding {
  padding-top:20px;
  padding-bottom:20px;
  padding-right:50px;
  padding-left:50px;
}
</style>

</head>


<body>
<div class="reveal">
<div class="slides">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




<section>
<!-- ------------------- main content ---------------------- -->
<center>
<h1 style="text-align: center;">Advanced machine learning and data analysis for the physical sciences</h1>
</center>  <!-- document title -->

<!-- author(s): Morten Hjorth-Jensen -->
<center>
<b>Morten Hjorth-Jensen</b> 
</center>
<!-- institution -->
<center>
<b>Department of Physics and Center for Computing in Science Education, University of Oslo, Norway</b>
</center>
<br>
<center>
<h4>May 8, 2025</h4>
</center> <!-- date -->
<br>


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2025, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</section>

<section>
<h2 id="plans-for-the-week-of-may-5-9-2025">Plans for the week of May 5-9, 2025  </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Deep generative models</b>
<p>
<ol>
<p><li> Mathematics of diffusion models and selected examples</li>
</ol>
</div>
</section>

<section>
<h2 id="readings">Readings </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Reading on diffusion models</b>
<p>
<ol>
<p><li> A central paper is the one by Sohl-Dickstein et al, Deep Unsupervised Learning using Nonequilibrium Thermodynamics, <a href="https://arxiv.org/abs/1503.03585" target="_blank"><tt>https://arxiv.org/abs/1503.03585</tt></a></li>
<p><li> Calvin Luo at <a href="https://arxiv.org/abs/2208.11970" target="_blank"><tt>https://arxiv.org/abs/2208.11970</tt></a></li>
<p><li> See also Diederik P. Kingma, Tim Salimans, Ben Poole, Jonathan Ho, Variational Diffusion Models, <a href="https://arxiv.org/abs/2107.00630" target="_blank"><tt>https://arxiv.org/abs/2107.00630</tt></a></li>
<p><li> See also David Foster <em>Generative Deep Learning</em>, chapter 8 on diffusion models.</li>
</ol>
</div>
</section>

<section>
<h2 id="diffusion-models-basics">Diffusion models, basics </h2>

<p>Diffusion models are inspired by non-equilibrium thermodynamics. They
define a Markov chain of diffusion steps to slowly add random noise to
data and then learn to reverse the diffusion process to construct
desired data samples from the noise. Unlike VAE or flow models,
diffusion models are learned with a fixed procedure and the latent
variable has high dimensionality (same as the original data).
</p>
</section>

<section>
<h2 id="why-diffusion-models">Why diffusion models? </h2>

<p>Diffusion models are prominent in generating high-quality images,
video, sound, etc. They are named for their similarity to the natural
diffusion process in physics, which describes how molecules move from
high-concentration to low-concentration areas. In the context of
machine learning, diffusion models generate new data by reversing a
diffusion process, that is information loss due to noise
intervention. The main idea here is to add random noise to data and
then undo the process to get the original data distribution from the
noisy data.
</p>

<p>The famous DALL-E 2, Midjourney, and open-source Stable Diffusion that
create realistic images based on the user's text input are all
examples of diffusion models.
</p>
</section>

<section>
<h2 id="what-are-diffusion-models">What are diffusion models? </h2>

<p>Diffusion models are advanced machine learning algorithms that
uniquely generate high-quality data by progressively adding noise to a
dataset and then learning to reverse this process. This innovative
approach enables them to create remarkably accurate and detailed
outputs, from lifelike images to coherent text sequences. Central to
their function is the concept of gradually degrading data quality,
only to reconstruct it to its original form or transform it into
something new. This technique enhances the fidelity of generated data
and offers new possibilities in areas like medical imaging, autonomous
vehicles, and personalized AI assistants.
</p>
</section>

<section>
<h2 id="problems-with-probabilistic-models">Problems with probabilistic models </h2>

<p>Historically, probabilistic models suffer from a tradeoff between two
conflicting objectives: <em>tractability</em> and
<em>flexibility</em>. Models that are <em>tractable</em> can be
analytically evaluated and easily fit to data (e.g. a Gaussian or
Laplace). However, these models are unable to aptly describe structure
in rich datasets. On the other hand, models that are <em>flexible</em>
can be molded to fit structure in arbitrary data. For example, we can
define models in terms of any (non-negative) function \( \phi(\boldsymbol{x}) \)
yielding the flexible distribution
</p>
<p>&nbsp;<br>
$$
p\left(\boldsymbol{x}\right) =\frac{\phi\left(\boldsymbol{x} \right)}{Z},
$$
<p>&nbsp;<br>

<p>where \( Z \) is a normalization
constant. However, computing this normalization constant is generally
intractable. Evaluating, training, or drawing samples from such
flexible models typically requires a very expensive Monte Carlo
process.
</p>
</section>

<section>
<h2 id="diffusion-models">Diffusion models </h2>
<p>Diffusion models have several interesting features</p>
<ul>
<p><li> extreme flexibility in model structure,</li>
<p><li> exact sampling,</li>
<p><li> easy multiplication with other distributions, e.g. in order to compute a posterior, and</li>
<p><li> the model log likelihood, and the probability of individual states, to be cheaply evaluated.</li>
</ul>
</section>

<section>
<h2 id="original-idea">Original idea </h2>

<p>In the original formulation, one uses a Markov chain to gradually
convert one distribution into another, an idea used in non-equilibrium
statistical physics and sequential Monte Carlo. Diffusion models build
a generative Markov chain which converts a simple known distribution
(e.g. a Gaussian) into a target (data) distribution using a diffusion
process. Rather than use this Markov chain to approximately evaluate a
model which has been otherwise defined, one can  explicitly define the
probabilistic model as the endpoint of the Markov chain. Since each
step in the diffusion chain has an analytically evaluable probability,
the full chain can also be analytically evaluated.
</p>
</section>

<section>
<h2 id="diffusion-learning">Diffusion learning </h2>

<p>Learning in this framework involves estimating small perturbations to
a diffusion process. Estimating small, analytically tractable,
perturbations is more tractable than explicitly describing the full
distribution with a single, non-analytically-normalizable, potential
function.  Furthermore, since a diffusion process exists for any
smooth target distribution, this method can capture data distributions
of arbitrary form.
</p>
</section>

<section>
<h2 id="how-diffusion-models-work">How diffusion models work </h2>

<p>Diffusion models work in a dual-phase mechanism: They first train a
neural network to introduce noise into the dataset(a staple in the
forward diffusion process) and then methodically reverse this
process. 
</p>
</section>

<section>
<h2 id="data-preprocessing">Data preprocessing </h2>

<p>Before the diffusion process begins, data needs to be appropriately
formatted for model training. This process involves data cleaning to
remove outliers, data normalization to scale features consistently,
and data augmentation to increase dataset diversity, especially in the
case of image data. Standardization is also applied to achieve normal
data distribution, which is important for handling noisy image
data. Different data types, such as text or images, may require
specific preprocessing steps, like addressing class-imbalance
issues. Well-executed data processing ensures high-quality training
data and contributes to the model's ability to learn meaningful
patterns and generate high-quality images (or other data types) during
inference.
</p>
</section>

<section>
<h2 id="mathematics-of-diffusion-models">Mathematics of diffusion models </h2>

<p>Let us go back our discussions of the variational autoencoders from the lecture of April 24, see
<a href="https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week14/ipynb/week14.ipynb" target="_blank"><tt>https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week14/ipynb/week14.ipynb</tt></a>. As
a first attempt at understanding diffusion models, we can think of
these as stacked VAEs, or better, recursive VAEs.
</p>

<p>Let us try to see why. As an intermediate step, we consider so-called
hierarchical VAEs, which can be seen as a generalization of VAEs that
include multiple hierarchies of latent spaces.
</p>

<p><b>Note</b>: Many of the derivations and figures here are inspired and borrowed from the excellent exposition of diffusion models by Calvin Luo at <a href="https://arxiv.org/abs/2208.11970" target="_blank"><tt>https://arxiv.org/abs/2208.11970</tt></a>. </p>
</section>

<section>
<h2 id="chains-of-vaes">Chains of VAEs </h2>

<p>Markovian
VAEs represent a  generative process where we use  Markov chain to build a hierarchy of VAEs.
</p>

<p>Each transition down the hierarchy is Markovian, where we decode each
latent set of variables \( \boldsymbol{h}_t \) in terms of the previous latent variable \( \boldsymbol{h}_{t-1} \).
Intuitively, and visually, this can be seen as simply stacking VAEs on
top of each other (see figure next slide).
</p>

<p>One can think of such a model as a recursive VAE.</p>
</section>

<section>
<h2 id="mathematical-representation">Mathematical representation </h2>

<p>Mathematically, we represent the joint distribution and the posterior
of a Markovian VAE as
</p>
<p>&nbsp;<br>
$$
\begin{align*}
    p(\boldsymbol{x}, \boldsymbol{h}_{1:T}) &= p(\boldsymbol{h}_T)p_{\boldsymbol{\theta}}(\boldsymbol{x}|\boldsymbol{h}_1)\prod_{t=2}^{T}p_{\boldsymbol{\theta}}(\boldsymbol{h}_{t-1}|\boldsymbol{h}_{t})\\
    q_{\boldsymbol{\phi}}(\boldsymbol{h}_{1:T}|\boldsymbol{x}) &= q_{\boldsymbol{\phi}}(\boldsymbol{h}_1|\boldsymbol{x})\prod_{t=2}^{T}q_{\boldsymbol{\phi}}(\boldsymbol{h}_{t}|\boldsymbol{h}_{t-1})
\end{align*}
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="back-to-the-marginal-probability">Back to the marginal probability </h2>

<p>We can then define the marginal probability we want to optimize as</p>
$$
\begin{align*}
\log p(\boldsymbol{x}) &= \log \int p(\boldsymbol{x}, \boldsymbol{h}_{1:T}) d\boldsymbol{h}_{1:T}  \\
&= \log \int \frac{p(\boldsymbol{x}, \boldsymbol{h}_{1:T})q_{\boldsymbol{\phi}}(\boldsymbol{h}_{1:T}|\boldsymbol{x})}{q_{\boldsymbol{\phi}}(\boldsymbol{h}_{1:T}|\boldsymbol{x})} d\boldsymbol{h}_{1:T}         && \text{(Multiply by 1 = $\frac{q_{\boldsymbol{\phi}}(\boldsymbol{h}_{1:T}|\boldsymbol{x})}{q_{\boldsymbol{\phi}}(\boldsymbol{h}_{1:T}|\boldsymbol{x})}$)}\\
&= \log \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{h}_{1:T}|\boldsymbol{x})}\left[\frac{p(\boldsymbol{x}, \boldsymbol{h}_{1:T})}{q_{\boldsymbol{\phi}}(\boldsymbol{h}_{1:T}|\boldsymbol{x})}\right]         && \text{(Definition of Expectation)}\\
&\geq \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{h}_{1:T}|\boldsymbol{x})}\left[\log \frac{p(\boldsymbol{x}, \boldsymbol{h}_{1:T})}{q_{\boldsymbol{\phi}}(\boldsymbol{h}_{1:T}|\boldsymbol{x})}\right]         && \text{(Discussed last week)}
\end{align*}
$$
</section>

<section>
<h2 id="diffusion-models-for-hierarchical-vae-from-url-https-arxiv-org-abs-2208-11970">Diffusion models for hierarchical VAE, from <a href="https://arxiv.org/abs/2208.11970" target="_blank"><tt>https://arxiv.org/abs/2208.11970</tt></a>  </h2>

<p>A Markovian hierarchical Variational Autoencoder with \( T \) hierarchical
latents.  The generative process is modeled as a Markov chain, where
each latent \( \boldsymbol{h}_t \) is generated only from the previous latent
\( \boldsymbol{h}_{t+1} \). Here \( \boldsymbol{z} \) is our latent variable \( \boldsymbol{h} \).
</p>

<br/><br/>
<center>
<p><img src="figures/figure1.png" width="800" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="equation-for-the-markovian-hierarchical-vae">Equation for the Markovian hierarchical VAE </h2>

<p>We obtain then</p>
<p>&nbsp;<br>
$$
\begin{align*}
\mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{h}_{1:T}|\boldsymbol{x})}\left[\log \frac{p(\boldsymbol{x}, \boldsymbol{h}_{1:T})}{q_{\boldsymbol{\phi}}(\boldsymbol{h}_{1:T}|\boldsymbol{x})}\right]
&= \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{h}_{1:T}|\boldsymbol{x})}\left[\log \frac{p(\boldsymbol{h}_T)p_{\boldsymbol{\theta}}(\boldsymbol{x}|\boldsymbol{h}_1)\prod_{t=2}^{T}p_{\boldsymbol{\theta}}(\boldsymbol{h}_{t-1}|\boldsymbol{h}_{t})}{q_{\boldsymbol{\phi}}(\boldsymbol{h}_1|\boldsymbol{x})\prod_{t=2}^{T}q_{\boldsymbol{\phi}}(\boldsymbol{h}_{t}|\boldsymbol{h}_{t-1})}\right]
\end{align*}
$$
<p>&nbsp;<br>

<p>We will modify this equation when we discuss what are normally called Variational Diffusion Models.</p>
</section>

<section>
<h2 id="variational-diffusion-models">Variational Diffusion Models </h2>

<p>The easiest way to think of a Variational Diffusion Model (VDM) is as a Markovian Hierarchical Variational Autoencoder with three key restrictions:</p>

<ol>
<p><li> The latent dimension is exactly equal to the data dimension</li>
<p><li> The structure of the latent encoder at each timestep is not learned; it is pre-defined as a linear Gaussian model.  In other words, it is a Gaussian distribution centered around the output of the previous timestep</li>
<p><li> The Gaussian parameters of the latent encoders vary over time in such a way that the distribution of the latent at final timestep \( T \) is a standard Gaussian</li>
</ol>
<p>
<p>The VDM posterior is</p>
<p>&nbsp;<br>
$$
\begin{align*}
    q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0) = \prod_{t = 1}^{T}q(\boldsymbol{x}_{t}|\boldsymbol{x}_{t-1})
\end{align*}
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="second-assumption">Second assumption </h2>

<p>The distribution of each latent variable in the encoder is a Gaussian centered around its previous hierarchical latent.
Here then, the structure of the encoder at each timestep \( t \) is not learned; it
is fixed as a linear Gaussian model, where the mean and standard
deviation can be set beforehand as hyperparameters, or learned as
parameters.
</p>
</section>

<section>
<h2 id="parameterizing-gaussian-encoder">Parameterizing Gaussian encoder </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>We parameterize the Gaussian encoder with mean \( \boldsymbol{\mu}_t(\boldsymbol{x}_t) =
\sqrt{\alpha_t} \boldsymbol{x}_{t-1} \), and variance \( \boldsymbol{\Sigma}_t(\boldsymbol{x}_t) =
(1 - \alpha_t) \textbf{I} \), where the form of the coefficients are
chosen such that the variance of the latent variables stay at a
similar scale; in other words, the encoding process is
variance-preserving.
</p>
</div>


<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>Note that alternate Gaussian parameterizations
are allowed, and lead to similar derivations.  The main takeaway is
that \( \alpha_t \) is a (potentially learnable) coefficient that can vary
with the hierarchical depth \( t \), for flexibility.
</p>
</div>
</section>

<section>
<h2 id="encoder-transitions">Encoder transitions </h2>

<p>Mathematically, the encoder transitions are defined as</p>
<p>&nbsp;<br>
$$
\begin{align*}
    q(\boldsymbol{x}_{t}|\boldsymbol{x}_{t-1}) = \mathcal{N}(\boldsymbol{x}_{t} ; \sqrt{\alpha_t} \boldsymbol{x}_{t-1}, (1 - \alpha_t) \textbf{I}) \tag{1}
\end{align*}
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="third-assumption">Third assumption </h2>

<p>From the third assumption, we know that \( \alpha_t \) evolves over time
according to a fixed or learnable schedule structured such that the
distribution of the final latent \( p(\boldsymbol{x}_T) \) is a standard Gaussian.
We can then update the joint distribution of a Markovian VAE to write
the joint distribution for a VDM as
</p>

<p>&nbsp;<br>
$$
\begin{align*}
p(\boldsymbol{x}_{0:T}) &= p(\boldsymbol{x}_T)\prod_{t=1}^{T}p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t) \\
\text{where,}&\nonumber\\
p(\boldsymbol{x}_T) &= \mathcal{N}(\boldsymbol{x}_T; \boldsymbol{0}, \textbf{I})
\end{align*}
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="noisification">Noisification </h2>

<p>Collectively, what this set of assumptions describes is a steady
noisification of an image input over time. We progressively corrupt an
image by adding Gaussian noise until eventually it becomes completely
identical to pure Gaussian noise.  See figure on next slide.
</p>
</section>

<section>
<h2 id="diffusion-models-from-url-https-arxiv-org-abs-2208-11970">Diffusion models, from <a href="https://arxiv.org/abs/2208.11970" target="_blank"><tt>https://arxiv.org/abs/2208.11970</tt></a>  </h2>

<br/><br/>
<center>
<p><img src="figures/figure2.png" width="800" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="gaussian-modeling">Gaussian modeling </h2>

<p>Note that our encoder distributions \( q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1}) \) are no
longer parameterized by \( \boldsymbol{\phi} \), as they are completely modeled as
Gaussians with defined mean and variance parameters at each timestep.
Therefore, in a VDM, we are only interested in learning conditionals
\( p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t-1}|\boldsymbol{x}_{t}) \), so that we can simulate
new data.  After optimizing the VDM, the sampling procedure is as
simple as sampling Gaussian noise from \( p(\boldsymbol{x}_T) \) and iteratively
running the denoising transitions
\( p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t-1}|\boldsymbol{x}_{t}) \) for \( T \) steps to generate a
novel \( \boldsymbol{x}_0 \).
</p>
</section>

<section>
<h2 id="optimizing-the-variational-diffusion-model">Optimizing the variational diffusion model </h2>

<p>&nbsp;<br>
$$
\begin{align*}
\log p(\boldsymbol{x})
&= \log \int p(\boldsymbol{x}_{0:T}) d\boldsymbol{x}_{1:T}\\
&= \log \int \frac{p(\boldsymbol{x}_{0:T})q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} d\boldsymbol{x}_{1:T}\\
&= \log \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[\frac{p(\boldsymbol{x}_{0:T})}{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\right]\\
&\geq {\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[\log \frac{p(\boldsymbol{x}_{0:T})}{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\right]}\\
&= {\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[\log \frac{p(\boldsymbol{x}_T)\prod_{t=1}^{T}p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t)}{\prod_{t = 1}^{T}q(\boldsymbol{x}_{t}|\boldsymbol{x}_{t-1})}\right]}\\
&= {\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[\log \frac{p(\boldsymbol{x}_T)p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)\prod_{t=2}^{T}p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t)}{q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})\prod_{t = 1}^{T-1}q(\boldsymbol{x}_{t}|\boldsymbol{x}_{t-1})}\right]}\\
&= {\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[\log \frac{p(\boldsymbol{x}_T)p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)\prod_{t=1}^{T-1}p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t}|\boldsymbol{x}_{t+1})}{q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})\prod_{t = 1}^{T-1}q(\boldsymbol{x}_{t}|\boldsymbol{x}_{t-1})}\right]}\\
&= {\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[\log \frac{p(\boldsymbol{x}_T)p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)}{q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})}\right] + \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[\log \prod_{t = 1}^{T-1}\frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t}|\boldsymbol{x}_{t+1})}{q(\boldsymbol{x}_{t}|\boldsymbol{x}_{t-1})}\right]}\\
\end{align*}
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="continues">Continues </h2>

<p>&nbsp;<br>
$$
\begin{align*}
\log p(\boldsymbol{x})
&= {\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[\log \frac{p(\boldsymbol{x}_T)p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)}{q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})}\right] + \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[\log \prod_{t = 1}^{T-1}\frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t}|\boldsymbol{x}_{t+1})}{q(\boldsymbol{x}_{t}|\boldsymbol{x}_{t-1})}\right]}\\
&= {\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)\right] + \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[\log \frac{p(\boldsymbol{x}_T)}{q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})}\right] + \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[ \sum_{t=1}^{T-1} \log \frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t}|\boldsymbol{x}_{t+1})}{q(\boldsymbol{x}_{t}|\boldsymbol{x}_{t-1})}\right]}\\
&= {\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)\right] + \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[\log \frac{p(\boldsymbol{x}_T)}{q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})}\right] + \sum_{t=1}^{T-1}\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[ \log \frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t}|\boldsymbol{x}_{t+1})}{q(\boldsymbol{x}_{t}|\boldsymbol{x}_{t-1})}\right]}\\
&= {\mathbb{E}_{q(\boldsymbol{x}_{1}|\boldsymbol{x}_0)}\left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)\right] + \mathbb{E}_{q(\boldsymbol{x}_{T-1}, \boldsymbol{x}_T|\boldsymbol{x}_0)}\left[\log \frac{p(\boldsymbol{x}_T)}{q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})}\right] + \sum_{t=1}^{T-1}\mathbb{E}_{q(\boldsymbol{x}_{t-1}, \boldsymbol{x}_t, \boldsymbol{x}_{t+1}|\boldsymbol{x}_0)}\left[\log \frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t}|\boldsymbol{x}_{t+1})}{q(\boldsymbol{x}_{t}|\boldsymbol{x}_{t-1})}\right]}\\
\end{align*}
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="interpretations">Interpretations </h2>

<p>These equations can be interpreted as</p>

<ul>
<p><li> \( \mathbb{E}_{q(\boldsymbol{x}_{1}|\boldsymbol{x}_0)}\left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)\right] \) can be interpreted as a <b>reconstruction term</b>, predicting the log probability of the original data sample given the first-step latent.  This term also appears in a vanilla VAE, and can be trained similarly.</li>
<p><li> \( \mathbb{E}_{q(\boldsymbol{x}_{T-1}|\boldsymbol{x}_0)}\left[D_{KL}(q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})\vert\vert p(\boldsymbol{x}_T))\right] \) is a <b>prior matching term</b>; it is minimized when the final latent distribution matches the Gaussian prior.  This term requires no optimization, as it has no trainable parameters; furthermore, as we have assumed a large enough \( T \) such that the final distribution is Gaussian, this term effectively becomes zero.</li>
</ul>
</section>

<section>
<h2 id="the-last-term">The last term </h2>

<ul>
<p><li> \( \mathbb{E}_{q(\boldsymbol{x}_{t-1}, \boldsymbol{x}_{t+1}|\boldsymbol{x}_0)}\left[D_{KL}(q(\boldsymbol{x}_{t}|\boldsymbol{x}_{t-1})\vert\vert p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t}|\boldsymbol{x}_{t+1}))\right] \) is a <em>consistency term</em>; it attempts to make the distribution at \( \boldsymbol{x}_t \) consistent, from both forward and backward processes.  That is, a denoising step from a noisier image should match the corresponding noising step from a cleaner image, for every intermediate timestep; this is reflected mathematically by the KL Divergence.  This term is minimized when we train \( p_{\theta}(\boldsymbol{x}_t|\boldsymbol{x}_{t+1}) \) to match the Gaussian distribution \( q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1}) \).</li>
</ul>
</section>

<section>
<h2 id="diffusion-models-part-2-from-url-https-arxiv-org-abs-2208-11970">Diffusion models, part 2, from <a href="https://arxiv.org/abs/2208.11970" target="_blank"><tt>https://arxiv.org/abs/2208.11970</tt></a>  </h2>

<br/><br/>
<center>
<p><img src="figures/figure3.png" width="800" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="optimization-cost">Optimization cost </h2>

<p>The cost of optimizing a VDM is primarily dominated by the third term, since we must optimize over all timesteps \( t \).</p>

<p>Under this derivation, all three terms are computed as expectations,
and can therefore be approximated using Monte Carlo estimates.
However, actually optimizing the ELBO using the terms we just derived
might be suboptimal; because the consistency term is computed as an
expectation over two random variables \( \left\{\boldsymbol{x}_{t-1},
\boldsymbol{x}_{t+1}\right\} \) for every timestep, the variance of its Monte
Carlo estimate could potentially be higher than a term that is
estimated using only one random variable per timestep.  As it is
computed by summing up \( T-1 \) consistency terms, the final estimated
value may have high variance for large \( T \) values.
</p>
</section>

<section>
<h2 id="image-quality">Image quality </h2>

<p>An advantage of diffusion models over for example VAEs (and also GANs
to be discussed next time) is the ease of training with simple and
efficient loss functions and their ability to generate highly
realistic images. They excel at closely matching the distribution of
real images, outperforming GANs in this aspect. This proficiency is
due to the distinct mechanisms in diffusion models, allowing for more
precise replication of real-world imagery.
</p>
</section>

<section>
<h2 id="training-stability">Training stability </h2>

<p>Regarding training stability, generative diffusion models have an edge
over GANs. GANs often struggle with <em>mode collapse</em>, which is  a limitation
where they produce a limited output variety. Diffusion models
effectively avoid this issue through their gradual data smoothing
process, leading to a more diverse range of generated images.
</p>
</section>

<section>
<h2 id="input-types">Input types </h2>

<p>It is also important to mention that diffusion models handle various
input types. They perform diverse generative tasks like text-to-image
synthesis, layout-to-image generation, inpainting, and
super-resolution tasks.
</p>
</section>

<section>
<h2 id="denoising-diffusion-probabilistic-models-ddpms">Denoising diffusion probabilistic models (DDPMs) </h2>

<p>Denoising diffusion probabilistic models (DDPMs) are a specific type
of diffusion model that focuses on probabilistically removing noise
from data. During training, they learn how noise is added to data over
time and how to reverse this process to recover the original
data. This involves using probabilities to make educated guesses about
what the data looked like before noise was added. This approach is
essential for the model's capability to accurately reconstruct data,
ensuring the outputs aren&#8217;t just noise-free but also closely resemble
the original data.
</p>
</section>

<section>
<h2 id="techniques-for-speeding-up-diffusion-models">Techniques for speeding up diffusion models </h2>

<p>Generating a sample from DDPM using the reverse diffusion process is
quite slow because it involves many steps, possibly up to a
thousand. For instance, according to Song et al. (2020), it takes
about 20 hours to generate 50,000 small images with a DDPM, while a
GAN can create the same amount in less than a minute using an Nvidia
2080 Ti GPU.
</p>

<p>There is an alternative method called Denoising Diffusion Implicit
Model (DDIM) that stands out for its efficiency and quality. Unlike
traditional models, DDIM needs fewer steps to create clear images from
noisy data.
</p>
</section>

<section>
<h2 id="applications-of-diffusion-models">Applications of diffusion models </h2>

<p>There are very diverse applications of diffusion models, one of the most exciting being digital art creation.
The document at <a href="https://www.superannotate.com/blog/diffusion-models#:~:text=A%20primary%20advantage%20of%20diffusion,to%20generate%20highly%20realistic%20images" target="_blank"><tt>https://www.superannotate.com/blog/diffusion-models#:~:text=A%20primary%20advantage%20of%20diffusion,to%20generate%20highly%20realistic%20images</tt></a> gives many nice examples of applications.
.
</p>
</section>

<section>
<h2 id="pytorch-implementation-of-a-denoising-diffusion-probabilistic-model-ddpm-trained-on-the-mnist-dataset">PyTorch implementation of a Denoising Diffusion Probabilistic Model (DDPM) trained on the MNIST dataset </h2>

<p>The code covers:</p>
<ol>
<p><li> Model definition (a simple U-Net-style convolutional network)</li>
<p><li> Forward diffusion (adding noise over \( T \) timesteps)</li>
<p><li> Reverse denoising process</li>
<p><li> Training loop</li>
<p><li> Sampling from the trained model</li>
</ol>
<p>
<p>This example is adapted from several open-source tutorials and
implementations, demonstrating how to build a diffusion model from
scratch in under 200 lines of PyTorch.
I have borrowed extensively from 
</p>
<ol>
<p><li> Jackson-Kang&#8217;s PyTorch diffusion tutorial, see <a href="https://github.com/Jackson-Kang/Pytorch-Diffusion-Model-Tutorial" target="_blank"><tt>https://github.com/Jackson-Kang/Pytorch-Diffusion-Model-Tutorial</tt></a>  and</li>
<p><li> awjuliani&#8217;s PyTorch DDPM implementation, see <a href="https://github.com/awjuliani/pytorch-diffusion" target="_blank"><tt>https://github.com/awjuliani/pytorch-diffusion</tt></a></li>  
</ol>
</section>

<section>
<h2 id="what-is-a-u-net">What is a U-net? </h2>

<p>UNet is a type of convolutional neural network (CNN) Architecture that is primarily used for image segmentation tasks. See <a href="https://en.wikipedia.org/wiki/U-Net" target="_blank"><tt>https://en.wikipedia.org/wiki/U-Net</tt></a> or <a href="https://www.sciencedirect.com/topics/computer-science/u-net" target="_blank"><tt>https://www.sciencedirect.com/topics/computer-science/u-net</tt></a></p>
</section>

<section>
<h2 id="problem-with-diffusion-models">Problem with diffusion models </h2>

<p>Diffusion models gradually corrupt data by adding Gaussian noise over
a sequence of timesteps and then learn to reverse this noising process
with a neural network.
</p>

<p>The corruption schedule is typically linear or cosine in variance.</p>

<p>During training, the network is optimized to predict the original
noise added at each timestep, using a mean-squared error loss.
</p>

<p>At inference, one starts from random noise and iteratively applies the
learned denoising steps to generate new samples.
</p>
</section>

<section>
<h2 id="imports-and-utilities">Imports and Utilities </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">torch</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">torch.nn</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">nn</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">torch.nn.functional</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">F</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">torchvision</span> <span style="color: #8B008B; font-weight: bold">import</span> datasets, transforms
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">torch.utils.data</span> <span style="color: #8B008B; font-weight: bold">import</span> DataLoader
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">math</span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="hyperparameters-and-schedules">Hyperparameters and schedules </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;">device = <span style="color: #CD5555">&#39;cuda&#39;</span> <span style="color: #8B008B; font-weight: bold">if</span> torch.cuda.is_available() <span style="color: #8B008B; font-weight: bold">else</span> <span style="color: #CD5555">&#39;cpu&#39;</span>

<span style="color: #228B22"># Training settings</span>
batch_size = <span style="color: #B452CD">128</span>
epochs     = <span style="color: #B452CD">5</span>
lr         = <span style="color: #B452CD">2e-4</span>
img_size   = <span style="color: #B452CD">28</span>
channels   = <span style="color: #B452CD">1</span>

<span style="color: #228B22"># Diffusion hyperparameters</span>
T = <span style="color: #B452CD">300</span>  <span style="color: #228B22"># number of diffusion steps  [oai_citation:5‡Medium](https://papers-100-lines.medium.com/diffusion-models-from-scratch-mnist-data-tutorial-in-100-lines-of-pytorch-code-a609e1558cee?utm_source=chatgpt.com)</span>
beta_start, beta_end = <span style="color: #B452CD">1e-4</span>, <span style="color: #B452CD">0.02</span>
betas = torch.linspace(beta_start, beta_end, T, device=device)  <span style="color: #228B22"># linear schedule  [oai_citation:6‡Medium](https://medium.com/data-science/diffusion-model-from-scratch-in-pytorch-ddpm-9d9760528946?utm_source=chatgpt.com)</span>
alphas = <span style="color: #B452CD">1.</span> - betas
alphas_cumprod = torch.cumprod(alphas, dim=<span style="color: #B452CD">0</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="data-loading">Data Loading </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;">transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((<span style="color: #B452CD">0.5</span>,), (<span style="color: #B452CD">0.5</span>,)),
])

train_ds = datasets.MNIST(<span style="color: #CD5555">&#39;.&#39;</span>, train=<span style="color: #8B008B; font-weight: bold">True</span>, download=<span style="color: #8B008B; font-weight: bold">True</span>, transform=transform)
train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=<span style="color: #8B008B; font-weight: bold">True</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="model-definition">Model definition </h2>

<p>We present a  lightweight U-Net inspired model for noise prediction:</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">class</span> <span style="color: #008b45; font-weight: bold">SimpleUNet</span>(nn.Module):
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__init__</span>(<span style="color: #658b00">self</span>, c):
        <span style="color: #658b00">super</span>().<span style="color: #008b45">__init__</span>()
        <span style="color: #658b00">self</span>.enc1 = nn.Conv2d(c, <span style="color: #B452CD">64</span>, <span style="color: #B452CD">3</span>, padding=<span style="color: #B452CD">1</span>)
        <span style="color: #658b00">self</span>.enc2 = nn.Conv2d(<span style="color: #B452CD">64</span>, <span style="color: #B452CD">128</span>, <span style="color: #B452CD">3</span>, padding=<span style="color: #B452CD">1</span>)
        <span style="color: #658b00">self</span>.dec1 = nn.ConvTranspose2d(<span style="color: #B452CD">128</span>, <span style="color: #B452CD">64</span>, <span style="color: #B452CD">3</span>, padding=<span style="color: #B452CD">1</span>)
        <span style="color: #658b00">self</span>.dec2 = nn.ConvTranspose2d(<span style="color: #B452CD">64</span>, c, <span style="color: #B452CD">3</span>, padding=<span style="color: #B452CD">1</span>)
        <span style="color: #658b00">self</span>.act  = nn.ReLU()
        <span style="color: #228B22"># timestep embedding to condition on t</span>
        <span style="color: #658b00">self</span>.time_mlp = nn.Sequential(nn.Linear(<span style="color: #B452CD">1</span>, <span style="color: #B452CD">128</span>), nn.ReLU(),nn.Linear(<span style="color: #B452CD">128</span>, <span style="color: #B452CD">128</span>))

    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">forward</span>(<span style="color: #658b00">self</span>, x, t):
        <span style="color: #228B22"># x: [B, C, H, W], t: [B]</span>
        h = <span style="color: #658b00">self</span>.act(<span style="color: #658b00">self</span>.enc1(x))
        h = <span style="color: #658b00">self</span>.act(<span style="color: #658b00">self</span>.enc2(h))
        <span style="color: #228B22"># add time embedding</span>
        t = t.unsqueeze(-<span style="color: #B452CD">1</span>)                             
        temb = <span style="color: #658b00">self</span>.time_mlp(t)
        temb = temb.view(-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">128</span>, <span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>)
        h = h + temb
        h = <span style="color: #658b00">self</span>.act(<span style="color: #658b00">self</span>.dec1(h))
        <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #658b00">self</span>.dec2(h)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="forward-diffusion-q-x-t-vert-x-0">Forward Diffusion \( q(x_t\vert x_0) \) </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">q_sample</span>(x0, t, noise=<span style="color: #8B008B; font-weight: bold">None</span>):
    <span style="color: #CD5555">&quot;&quot;&quot;Add noise to x0 at timestep t.&quot;&quot;&quot;</span>
    <span style="color: #8B008B; font-weight: bold">if</span> noise <span style="color: #8B008B">is</span> <span style="color: #8B008B; font-weight: bold">None</span>:
        noise = torch.randn_like(x0)
    sqrt_acp = alphas_cumprod[t]**<span style="color: #B452CD">0.5</span>
    sqrt_1macp = (<span style="color: #B452CD">1</span> - alphas_cumprod[t])**<span style="color: #B452CD">0.5</span>
    <span style="color: #8B008B; font-weight: bold">return</span> sqrt_acp.view(-<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>)*x0 + sqrt_1macp.view(-<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>)*noise
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="cost-loss-function">Cost/Loss function </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">diffusion_loss</span>(model, x0):
    <span style="color: #CD5555">&quot;&quot;&quot;Compute MSE between predicted noise and true noise.&quot;&quot;&quot;</span>
    B = x0.size(<span style="color: #B452CD">0</span>)
    t = torch.randint(<span style="color: #B452CD">0</span>, T, (B,), device=device).long()
    noise = torch.randn_like(x0)
    x_noisy = q_sample(x0, t, noise)
    pred_noise = model(x_noisy, t.float()/T)
    <span style="color: #8B008B; font-weight: bold">return</span> F.mse_loss(pred_noise, noise)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="training-loop">Training loop </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;">model = SimpleUNet(channels).to(device)
opt   = torch.optim.Adam(model.parameters(), lr=lr)

<span style="color: #8B008B; font-weight: bold">for</span> epoch <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(epochs):
    total_loss = <span style="color: #B452CD">0</span>
    <span style="color: #8B008B; font-weight: bold">for</span> x, _ <span style="color: #8B008B">in</span> train_loader:
        x = x.to(device)
        loss = diffusion_loss(model, x)
        opt.zero_grad()
        loss.backward()
        opt.step()
        total_loss += loss.item()
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">f&quot;Epoch {</span>epoch+<span style="color: #B452CD">1</span><span style="color: #CD5555">}/{</span>epochs<span style="color: #CD5555">}, Loss: {</span>total_loss/<span style="color: #658b00">len</span>(train_loader)<span style="color: #CD5555">:.4f}&quot;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="sampling-reverse-diffusion">Sampling (Reverse Diffusion) </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #707a7c">@torch</span>.no_grad()
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">p_sample_loop</span>(model, shape):
    x = torch.randn(shape, device=device)
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">reversed</span>(<span style="color: #658b00">range</span>(T)):
        t = torch.full((shape[<span style="color: #B452CD">0</span>],), i, device=device).float()/T
        eps_pred = model(x, t)
        beta_t = betas[i]
        alpha_t = alphas[i]
        acp_t   = alphas_cumprod[i]
        coef1 = <span style="color: #B452CD">1</span> / alpha_t.sqrt()
        coef2 = beta_t / ( (<span style="color: #B452CD">1</span> - acp_t).sqrt() )
        x = coef1*(x - coef2*eps_pred)
        <span style="color: #8B008B; font-weight: bold">if</span> i &gt; <span style="color: #B452CD">0</span>:
            z = torch.randn_like(x)
            sigma = beta_t.sqrt()
            x = x + sigma*z
    <span style="color: #8B008B; font-weight: bold">return</span> x

<span style="color: #228B22"># Generate samples</span>
samples = p_sample_loop(model, (<span style="color: #B452CD">16</span>, channels, img_size, img_size))
samples = samples.clamp(-<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>).cpu()
grid = torchvision.utils.make_grid(samples, nrow=<span style="color: #B452CD">4</span>, normalize=<span style="color: #8B008B; font-weight: bold">True</span>)
plt.figure(figsize=(<span style="color: #B452CD">5</span>,<span style="color: #B452CD">5</span>))
plt.imshow(grid.permute(<span style="color: #B452CD">1</span>,<span style="color: #B452CD">2</span>,<span style="color: #B452CD">0</span>))
plt.axis(<span style="color: #CD5555">&#39;off&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="more-details-and-plans-for-next-week">More details and plans for next week </h2>

<p>For more details and implementations, see Calvin Luo at <a href="https://arxiv.org/abs/2208.11970" target="_blank"><tt>https://arxiv.org/abs/2208.11970</tt></a></p>
<div class="alert alert-block alert-block alert-text-normal">
<b>Plans for next week</b>
<p>
<ol>
<p><li> Finalizing discussion of diffusion models</li>
<p><li> Presenting generalized adversarial networks (GANs)</li>
<p><li> Possible summary of works</li>
<p><li> Discussion of project 2</li>
</ol>
</div>
</section>



</div> <!-- class="slides" -->
</div> <!-- class="reveal" -->

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

  // Display navigation controls in the bottom right corner
  controls: true,

  // Display progress bar (below the horiz. slider)
  progress: true,

  // Display the page number of the current slide
  slideNumber: true,

  // Push each slide change to the browser history
  history: false,

  // Enable keyboard shortcuts for navigation
  keyboard: true,

  // Enable the slide overview mode
  overview: true,

  // Vertical centering of slides
  //center: true,
  center: false,

  // Enables touch navigation on devices with touch input
  touch: true,

  // Loop the presentation
  loop: false,

  // Change the presentation direction to be RTL
  rtl: false,

  // Turns fragments on and off globally
  fragments: true,

  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,

  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,

  // Stop auto-sliding after user input
  autoSlideStoppable: true,

  // Enable slide navigation via mouse wheel
  mouseWheel: false,

  // Hides the address bar on mobile devices
  hideAddressBar: true,

  // Opens links in an iframe preview overlay
  previewLinks: false,

  // Transition style
  transition: 'default', // default/cube/page/concave/zoom/linear/fade/none

  // Transition speed
  transitionSpeed: 'default', // default/fast/slow

  // Transition style for full page slide backgrounds
  backgroundTransition: 'default', // default/none/slide/concave/convex/zoom

  // Number of slides away from the current that are visible
  viewDistance: 3,

  // Parallax background image
    //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

  // Parallax background size
  //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"

  theme: Reveal.getQueryHash().theme, // available themes are in reveal.js/css/theme
    transition: Reveal.getQueryHash().transition || 'none', // default/cube/page/concave/zoom/linear/none

});

Reveal.initialize({
  dependencies: [
      // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
      { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },

      // Interpret Markdown in <section> elements
      { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

      // Syntax highlight for <code> elements
      { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

      // Zoom in and out with Alt+click
      { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

      // Speaker notes
      { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

      // Remote control your reveal.js presentation using a touch device
      //{ src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } },

      // MathJax
      //{ src: 'reveal.js/plugin/math/math.js', async: true }
  ]
});

Reveal.initialize({

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 1170,  // original: 960,
  height: 700,

  // Factor of the display size that should remain empty around the content
  margin: 0.1,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 1.0

});
</script>

<!-- begin footer logo
<div style="position: absolute; bottom: 0px; left: 0; margin-left: 0px">
<img src="somelogo.png">
</div>
   end footer logo -->




</body>
</html>
