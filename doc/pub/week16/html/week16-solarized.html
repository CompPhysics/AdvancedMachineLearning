<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week16.do.txt --pygments_html_style=perldoc --html_style=solarized3 --html_links_in_new_window --html_output=week16-solarized --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Advanced machine learning and data analysis for the physical sciences">
<title>Advanced machine learning and data analysis for the physical sciences</title>
<link href="https://cdn.rawgit.com/doconce/doconce/master/bundled/html_styles/style_solarized_box/css/solarized_light_code.css" rel="stylesheet" type="text/css" title="light"/>
<script src="https://cdn.rawgit.com/doconce/doconce/master/bundled/html_styles/style_solarized_box/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<link href="https://thomasf.github.io/solarized-css/solarized-light.min.css" rel="stylesheet">
<style type="text/css">
h1 {color: #b58900;}  /* yellow */
/* h1 {color: #cb4b16;}  orange */
/* h1 {color: #d33682;}  magenta, the original choice of thomasf */
code { padding: 0px; background-color: inherit; }
pre {
  border: 0pt solid #93a1a1;
  box-shadow: none;
}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #93a1a1;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #eee8d5;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_yellow_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_yellow_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_yellow_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_yellow_question.png); }
div { text-align: justify; text-justify: inter-word; }
.tab {
  padding-left: 1.5em;
}
div.toc p,a {
  line-height: 1.3;
  margin-top: 1.1;
  margin-bottom: 1.1;
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plans for the week of May 12-16, 2025',
               2,
               None,
               'plans-for-the-week-of-may-12-16-2025'),
              ('Readings', 2, None, 'readings'),
              ('What is a GAN?', 2, None, 'what-is-a-gan'),
              ('Labeling the networks', 2, None, 'labeling-the-networks'),
              ('Which data?', 2, None, 'which-data'),
              ('Semi-supervised learning', 2, None, 'semi-supervised-learning'),
              ('Improving functionalities',
               2,
               None,
               'improving-functionalities'),
              ('More on Generative Adversarial Networks',
               2,
               None,
               'more-on-generative-adversarial-networks'),
              ('Appplications of GANs', 2, None, 'appplications-of-gans'),
              ('Setup of the GAN', 2, None, 'setup-of-the-gan'),
              ('Generative Adversarial Networks',
               2,
               None,
               'generative-adversarial-networks'),
              ('Discriminator', 2, None, 'discriminator'),
              ('Zero-sum game', 2, None, 'zero-sum-game'),
              ('Maximizing reward', 2, None, 'maximizing-reward'),
              ('Progression in training', 2, None, 'progression-in-training'),
              ('Deafault choice', 2, None, 'deafault-choice'),
              ('Design of GANs', 2, None, 'design-of-gans'),
              ('Improving functionalities',
               2,
               None,
               'improving-functionalities'),
              ('Setup of the GAN', 2, None, 'setup-of-the-gan'),
              ('Optimization part', 2, None, 'optimization-part'),
              ('Minimax game', 2, None, 'minimax-game'),
              ('Optimal value for $D$', 2, None, 'optimal-value-for-d'),
              ('Best value of $D$', 2, None, 'best-value-of-d'),
              ('Ignore integral', 2, None, 'ignore-integral'),
              ('Best values', 2, None, 'best-values'),
              ('What does the Loss Function Represent?',
               2,
               None,
               'what-does-the-loss-function-represent'),
              ('What does the loss function quantify?',
               2,
               None,
               'what-does-the-loss-function-quantify'),
              ('Problems with GANs', 2, None, 'problems-with-gans'),
              ('Vanishing Gradient', 2, None, 'vanishing-gradient'),
              ('Improved GANs', 2, None, 'improved-gans'),
              ('Writing Our First Generative Adversarial Network',
               2,
               None,
               'writing-our-first-generative-adversarial-network'),
              ('Code elements', 2, None, 'code-elements'),
              ('Setting up the GAN', 2, None, 'setting-up-the-gan'),
              ('Printing the model', 2, None, 'printing-the-model'),
              ('Defining the training set',
               2,
               None,
               'defining-the-training-set'),
              ('Defining the training set, part 2',
               2,
               None,
               'defining-the-training-set-part-2'),
              ('Training the GAN', 2, None, 'training-the-gan'),
              ('More on training', 2, None, 'more-on-training'),
              ('Visualizing', 2, None, 'visualizing'),
              ('Calculating scores', 2, None, 'calculating-scores'),
              ('More codes', 2, None, 'more-codes')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- ------------------- main content ---------------------- -->
<center>
<h1>Advanced machine learning and data analysis for the physical sciences</h1>
</center>  <!-- document title -->

<!-- author(s): Morten Hjorth-Jensen -->
<center>
<b>Morten Hjorth-Jensen</b> 
</center>
<!-- institution -->
<center>
<b>Department of Physics and Center for Computing in Science Education, University of Oslo, Norway</b>
</center>
<br>
<center>
<h4>May 15</h4>
</center> <!-- date -->
<br>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="plans-for-the-week-of-may-12-16-2025">Plans for the week of May 12-16, 2025  </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Generative models</b>
<p>
<ol>
<li> Summary from last week about diffusion models, mainly discussion of codes (see jupyter-notebook from the week of May 5-9)</li>
<li> Generative Adversarial Networks, see <a href="https://lilianweng.github.io/posts/2017-08-20-gan/" target="_blank"><tt>https://lilianweng.github.io/posts/2017-08-20-gan/</tt></a> for nice overview</li>
<li> A summary of the course will be made available in the form of a video. The summary material can also be viewed at <a href="https://github.com/CompPhysics/AdvancedMachineLearning/tree/main/doc/pub/week17" target="_blank"><tt>https://github.com/CompPhysics/AdvancedMachineLearning/tree/main/doc/pub/week17</tt></a> in various formats (pdf, jupyter-notebook or html formats).
<!-- o <a href="https://youtu.be/lYgKGCQRUhQ" target="_blank">Video of lecture</a> -->
<!-- o <a href="https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/HandwrittenNotes/2024/NotesMay7.pdf" target="_blank">Whiteboard notes</a> --></li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="readings">Readings </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<li> Reading recommendation: Goodfellow et al, for GANs see sections 20.10-20.11</li>
<li> For codes and background, see Raschka et al, Machine Learning with PyTorch and Scikit-Learn, chapter 17, see <a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch17" target="_blank"><tt>https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch17</tt></a> for codes</li>
<li> Babcock and Bali, Generative AI with Python and TensorFlow2, chapter 6 and codes at <a href="https://github.com/raghavbali/generative_ai_with_tensorflow/blob/master/Chapter_6/conditional_gan.ipynb" target="_blank"><tt>https://github.com/raghavbali/generative_ai_with_tensorflow/blob/master/Chapter_6/conditional_gan.ipynb</tt></a></li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="what-is-a-gan">What is a GAN? </h2>

<p>A GAN is a deep neural network which consists of two networks, a
so-called generator network and a discriminating network, or just
discriminator. Through several iterations of generation and
discrimination, the idea is that these networks will train each other,
while also trying to outsmart each other.
</p>

<p>In its simplest version, the two networks could be two standard neural networks with a given number of hidden of hidden layers and parameters to train.
The generator we have trained can then be used to produce new images.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="labeling-the-networks">Labeling the networks </h2>

<p>For a GAN we have: </p>
<ol>
<li> a discriminator \( D \) estimates the probability of a given sample coming from the real dataset. It attempts at discriminating the trained data by the generator and is optimized to tell the fake samples from the real ones (our data set). We say a  discriminator tries to distinguish between real data and those generated by the abovementioned generator.</li>
<li> a generator \( G \) outputs synthetic samples given a noise variable input \( z \) (\( z \) brings in potential output diversity). It is trained to capture the real data distribution in order to generate samples that can be as real as possible, or in other words, can trick the discriminator to offer a high probability.</li>
</ol>
<p>At the end of the training, the generator can be used to generate for
example new images. In this sense we have trained a model which can
produce new samples. We say that we have implicitely defined a
probability.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="which-data">Which data? </h2>

<p><b>GANs are generally a form of unsupervised machine learning</b>, although
they also incorporate aspects of supervised learning. Internally the
discriminator sets up a supervised learning problem. Its goal is to
learn to distinguish between the two classes of generated data and
original data. The generator then considers this classification
problem and tries to find adversarial examples, that is  samples which
will be misclassified by the discriminator.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="semi-supervised-learning">Semi-supervised learning </h2>

<p>One can also design GAN architectures which work in a
semi-supervised learning setting. A semi-supervised learning environment includes both labeled and unlabeled data.
See <a href="https://proceedings.neurips.cc/paper_files/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf" target="_blank"><tt>https://proceedings.neurips.cc/paper_files/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf</tt></a> for a further discussion.
</p>

<p>Thus, GANs can be used both on labeled and on unlabeled data and are used in three most commonly used contexts, that is</p>
<ol>
<li> with labeled data (supervised training)</li>
<li> with unlabeled data (unsupervised learning)</li>
<li> a with a mix labed and unlabeled  data</li>
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="improving-functionalities">Improving functionalities </h2>

<p>These two models compete against each other during the training
process: the generator \( G \) is trying hard to trick the discriminator,
while the critic model \( D \) is trying hard not to be cheated. This
interesting zero-sum game between two models motivates both to improve
their functionalities.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="more-on-generative-adversarial-networks">More on Generative Adversarial Networks </h2>

<p>Generative adversarial networks  have shown great results in
many generative tasks to replicate the real-world rich content such as
images, human language, and music. It is inspired by game theory: two
models, a generator and a discriminator, are competing with each other while
making each other stronger at the same time. However, it is rather
challenging to train a GANs model, 
training instability or failure to converge.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="appplications-of-gans">Appplications of GANs </h2>

<p>There are exteremely many applications of GANs</p>
<ol>
<li> Image generation</li>
<li> Text-to-image analysis</li>
<li> Face-aging</li>
<li> Image-to-image translation</li>
<li> Video synthesis</li>
<li> High-resolution image generation</li>
<li> Completing missing parts of images and much more</li>
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="setup-of-the-gan">Setup of the GAN </h2>

<p>We define a probability \( p_{\boldsymbol{h}} \) which is used by the
generator. Usually it is given by a uniform distribution over the
input \( \boldsymbol{h} \). Thereafter we define the distribution of the
generator which we want to train, \( p_{g} \) This is the generator's
distribution over the data \( \boldsymbol{x} \). Finally, we have the distribution
\( p_{r} \) over the real sample \( \boldsymbol{x} \)
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="generative-adversarial-networks">Generative Adversarial Networks </h2>

<p>The simplest formulation of
the model is based on a game theoretic approach, <em>zero sum game</em>, where we pit
two neural networks against one another. We define two rival networks, one
generator \( G \), and one discriminator \( D \). The generator directly produces
samples
</p>
$$
    x = G(z; \theta^{(g)}).
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="discriminator">Discriminator </h2>

<p>The discriminator attempts to distinguish between samples drawn from the
training data and samples drawn from the generator. In other words, it tries to
tell the difference between the fake data produced by \( G \) and the actual data
samples we want to do prediction on. The discriminator outputs a probability
value given by
</p>

$$
    D(x; \theta^{(d)}).
$$

<p>indicating the probability that \( x \) is a real training example rather than a
fake sample the generator has generated.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="zero-sum-game">Zero-sum game </h2>

<p>The simplest way to formulate the
learning process in a generative adversarial network is a zero-sum game, in
which a function
</p>

$$
    v(\theta^{(g)}, \theta^{(d)}),
$$

<p>determines the reward for the discriminator, while the generator gets the
conjugate reward
</p>

$$
    -v(\theta^{(g)}, \theta^{(d)})
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="maximizing-reward">Maximizing reward </h2>

<p>During learning both of the networks maximize their own reward function, so that
the generator gets better and better at tricking the discriminator, while the
discriminator gets better and better at telling the difference between the fake
and real data. The generator and discriminator alternate on which one trains at
one time (i.e. for one epoch). In other words, we keep the generator constant
and train the discriminator, then we keep the discriminator constant to train
the generator and repeat. It is this back and forth dynamic which lets GANs
tackle otherwise intractable generative problems. As the generator improves with
 training, the discriminator's performance gets worse because it cannot easily
 tell the difference between real and fake. If the generator ends up succeeding
 perfectly, the the discriminator will do no better than random guessing i.e.
 50\%.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="progression-in-training">Progression in training </h2>

<p>This progression in the training poses a problem for the convergence
 criteria for GANs. The discriminator feedback gets less meaningful over time,
 if we continue training after this point then the generator is effectively
 training on junk data which can undo the learning up to that point. Therefore,
 we stop training when the discriminator starts outputting \( 1/2 \) everywhere.
 At convergence we have
</p>

$$
    G^* = \underset{g}{\mathrm{argmin}}\hspace{2pt}
          \underset{d}{\mathrm{max}}v(\theta^{(g)}, \theta^{(d)}),
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="deafault-choice">Deafault choice </h2>
<p>The default choice for \( v \) is</p>
$$
    v(\theta^{(g)}, \theta^{(d)}) = \mathbb{E}_{x\sim p_\mathrm{data}}\log D(x)
                                  + \mathbb{E}_{x\sim p_\mathrm{model}}
                                  \log (1 - D(x)).
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="design-of-gans">Design of GANs </h2>
<p>The main motivation for the design of GANs is that the learning process requires
neither approximate inference (variational autoencoders for example) nor
approximation of a partition function. In the case where
</p>
$$
    \underset{d}{\mathrm{max}}v(\theta^{(g)}, \theta^{(d)})
$$

<p>is convex in \( \theta^{(g)} \) then the procedure is guaranteed to converge and is
asymptotically consistent
( <a href="https://arxiv.org/pdf/1804.09139.pdf" target="_blank">Seth Lloyd on QuGANs</a>  ). This is in
general not the case and it is possible to get situations where the training
process never converges because the generator and discriminator chase one
another around in the parameter space indefinitely.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="improving-functionalities">Improving functionalities </h2>

<p>These two models compete against each other during the training
process: the generator \( G \) is trying hard to trick the discriminator,
while the critic model \( D \) is trying hard not to be cheated. This
interesting zero-sum game between two models motivates both to improve
their functionalities.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="setup-of-the-gan">Setup of the GAN </h2>

<p>We define a probability \( p_{\boldsymbol{h}} \) which is used by the
generator. Usually it is given by a uniform distribution over the
input input \( \boldsymbol{h} \). Thereafter we define the distribution of the
generator which we want to train, \( p_{g} \) This is the generator's
distribution over the data \( \boldsymbol{x} \). Finally, we have the distribution
\( p_{r} \) over the real sample \( \boldsymbol{x} \)
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="optimization-part">Optimization part </h2>

<p>On one hand, we want to make sure the discriminator \( D \)'s decisions
over real data are accurate by maximizing \( \mathbb{E}_{\boldsymbol{x} \sim
p_{r}(\boldsymbol{x})} [\log D(\boldsymbol{x})] \). Meanwhile, given a fake sample \( G(\boldsymbol{h}), \boldsymbol{h} \sim
p_{\boldsymbol{h}}(\boldsymbol{h}) \), the discriminator is expected to output a probability,
\( D(G(\boldsymbol{h})) \), close to zero by maximizing \( \mathbb{E}_{\boldsymbol{h} \sim p_{\boldsymbol{h}}(\boldsymbol{h})}
[\log (1 - D(G(\boldsymbol{h})))] \).
</p>

<p>On the other hand, the generator is trained to increase the chances of
\( D \) producing a high probability for a fake example, thus to minimize
\( \mathbb{E}_{\boldsymbol{h} \sim p_{\boldsymbol{h}}(\boldsymbol{h})} [\log (1 - D(G(\boldsymbol{h})))] \).
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="minimax-game">Minimax game </h2>

<p>When combining both aspects together, \( D \) and \( G \) are playing a <b>minimax game</b> in which we should optimize the following loss function:</p>

$$
\begin{aligned}
\min_G \max_D L(D, G) 
& = \mathbb{E}_{\boldsymbol{x} \sim p_{r}(\boldsymbol{x})} [\log D(\boldsymbol{x})] + \mathbb{E}_{\boldsymbol{h} \sim p_{\boldsymbol{h}}(\boldsymbol{h})} [\log(1 - D(G(\boldsymbol{h})))] \\
& = \mathbb{E}_{\boldsymbol{x} \sim p_{r}(\boldsymbol{x})} [\log D(\boldsymbol{x})] + \mathbb{E}_{\boldsymbol{x} \sim p_g(\boldsymbol{x})} [\log(1 - D(\boldsymbol{x})]
\end{aligned}
$$

<p>where \( \mathbb{E}_{\boldsymbol{x} \sim p_{r}(\boldsymbol{x})} [\log D(\boldsymbol{x})] \) has no impact on \( G \) during gradient descent updates.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="optimal-value-for-d">Optimal value for \( D \) </h2>

<p>Now we have a well-defined loss function. Let's first examine what is the best value for \( D \).</p>

$$
L(G, D) = \int_{\boldsymbol{x}} \bigg( p_{r}(\boldsymbol{x}) \log(D(\boldsymbol{x})) + p_g (\boldsymbol{x}) \log(1 - D(\boldsymbol{x})) \bigg) dx
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="best-value-of-d">Best value of \( D \) </h2>
<p>Since we are interested in what is the best value of \( D(\boldsymbol{x}) \) to maximize \( L(G, D) \), let us label </p>

$$
\tilde{\boldsymbol{x}} = D(\boldsymbol{x}), 
A=p_{r}(\boldsymbol{x}), 
B=p_g(\boldsymbol{x})
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="ignore-integral">Ignore integral </h2>

<p>And then what is inside the integral (we can safely ignore the integral because \( \boldsymbol{x} \) is sampled over all the possible values) is:</p>

$$
\begin{align*}
f(\tilde{\boldsymbol{x}}) 
& = A \log{\tilde{\boldsymbol{x}}} + B \log{(1-\tilde{\boldsymbol{x}})} \\
\frac{d f(\tilde{\boldsymbol{x}})}{d \tilde{\boldsymbol{x}}} & = A \frac{1}{\tilde{\boldsymbol{x}}} - B\frac{1}{1 - \tilde{\boldsymbol{x}}} \\
& = \frac{A - (A + B)\tilde{\boldsymbol{x}}} {\tilde{\boldsymbol{x}} (1 - \tilde{\boldsymbol{x}})}. \\
\end{align*}
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="best-values">Best values </h2>

<p>Thus, if we set \( \frac{d f(\tilde{\boldsymbol{x}})}{d \tilde{\boldsymbol{x}}} = 0 \), we get
the best value of the discriminator: \( D^*(\boldsymbol{x}) = \tilde{\boldsymbol{x}}^* =
\frac{A}{A + B} = \frac{p_{r}(\boldsymbol{x})}{p_{r}(\boldsymbol{x}) + p_g(\boldsymbol{x})}
\in [0, 1] \).  Once the generator is trained to its optimal, \( p_g \) gets
very close to \( p_{r} \). When \( p_g = p_{r} \), \( D^*(\boldsymbol{x}) \) becomes
\( 1/2 \). We will observe this when running the code below here.
</p>

<p>When both \( G \) and \( D \) are at their optimal values, we have \( p_g = p_{r} \) and \( D^*(\boldsymbol{x}) = 1/2 \) and the loss function becomes:</p>

$$
\begin{align*}
L(G, D^*) 
&= \int_{\boldsymbol{x}} \bigg( p_{r}(\boldsymbol{x}) \log(D^*(\boldsymbol{x})) + p_g (\boldsymbol{x}) \log(1 - D^*(\boldsymbol{x})) \bigg) d\boldsymbol{x} \\
&= \log \frac{1}{2} \int_{\boldsymbol{h}} p_{r}(\boldsymbol{x}) d\boldsymbol{x} + \log \frac{1}{2} \int_{\boldsymbol{x}} p_g(\boldsymbol{x}) d\boldsymbol{x} \\
&= -2\log2
\end{align*}
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="what-does-the-loss-function-represent">What does the Loss Function Represent? </h2>

<p>The JS divergence between \( p_{r} \) and \( p_g \) can be computed as:</p>

$$
\begin{align*}
D_{JS}(p_{r} \| p_g) 
=& \frac{1}{2} D_{KL}(p_{r} || \frac{p_{r} + p_g}{2}) + \frac{1}{2} D_{KL}(p_{g} || \frac{p_{r} + p_g}{2}) \\
=& \frac{1}{2} \bigg( \log2 + \int_x p_{r}(\boldsymbol{x}) \log \frac{p_{r}(\boldsymbol{x})}{p_{r} + p_g(\boldsymbol{x})} d\boldsymbol{x} \bigg) + \\& \frac{1}{2} \bigg( \log2 + \int_x p_g(\boldsymbol{x}) \log \frac{p_g(\boldsymbol{x})}{p_{r} + p_g(\boldsymbol{x})} d\boldsymbol{x} \bigg) \\
=& \frac{1}{2} \bigg( \log4 + L(G, D^*) \bigg)
\end{align*}
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="what-does-the-loss-function-quantify">What does the loss function quantify? </h2>

<p>We have </p>
$$
L(G, D^*) = 2D_{JS}(p_{r} \| p_g) - 2\log2.
$$

<p>Essentially the loss function of a GAN quantifies the similarity between
the generative data distribution \( p_g \) and the real sample
distribution \( p_{r} \) by JS divergence when the discriminator is
optimal. The best \( G^* \) that replicates the real data distribution
leads to the minimum \( L(G^*, D^*) = -2\log2 \) which is aligned with
equations above.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="problems-with-gans">Problems with GANs </h2>

<p>Although GANs have achieved  great success in the generation of realistic images, the training is not easy; The process is known to be slow and unstable.</p>

<div class="alert alert-block alert-block alert-text-normal">
<b>Hard to reach equilibrium.</b>
<p>
<p>Two models are trained simultaneously to an equilibrium to a
two-player non-cooperative game. However, each model updates its cost
independently with no respect to another player in the game. Updating
the gradient of both models concurrently cannot guarantee a
convergence.
</p>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="vanishing-gradient">Vanishing Gradient  </h2>

<p>When the discriminator is perfect, we are guaranteed with
\( D(\boldsymbol{x}) = 1, \forall \boldsymbol{x} \in p_r \) and \( D(\boldsymbol{x}) = 0, \forall \boldsymbol{x} \in p_g \).
</p>

<p>Then, the
loss function \( L \) falls to zero and we end up with no gradient to
update the loss during learning iterations. One can encouter situations where 
the discriminator gets better and the gradient vanishes fast.
</p>

<p>As a result, training GANs may face the following problems </p>
<ol>
<li> If the discriminator behaves badly, the generator does not have accurate feedback and the loss function cannot represent the real data</li>
<li> If the discriminator does a great job, the gradient of the loss function drops down to close to zero and the learning can become slow</li>
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="improved-gans">Improved GANs </h2>

<p>One of the solutions to improved GANs training, is the introduction of
what is called the Wasserstein diatance, which is a way to compute the
difference/distance between two probability distribitions. For those
interested in reading more, we recommend for example chapter 17 of
Rashcka's et al textbook,
Machine Learning with PyTorch and Scikit-Learn, chapter 17, see <a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch17" target="_blank"><tt>https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch17</tt></a>
</p>

<p>For a definition of the Wasserstein distance, see for example <a href="https://arxiv.org/pdf/2103.01678" target="_blank"><tt>https://arxiv.org/pdf/2103.01678</tt></a></p>

<!-- Direct quote: <b>In this best-performing formulation, the generator aims to -->
<!-- increase the log probability that the discriminator makes a mistake, rather than -->
<!-- aiming to decrease the log probability that the discriminator makes the correct -->
<!-- prediction.</b> Another interesting read can be found at <a href="https://arxiv.org/abs/1701.00160" target="_blank"><tt>https://arxiv.org/abs/1701.00160</tt></a>. -->

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="writing-our-first-generative-adversarial-network">Writing Our First Generative Adversarial Network </h2>

<p>This part is best seen using the jupyter-notebook. We follow here
closely the code developed by Raschka et al from chapter 17 of their
textbook, see <a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch17" target="_blank"><tt>https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch17</tt></a> for codes.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="code-elements">Code elements </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">torch</span>
<span style="color: #658b00">print</span>(torch.__version__)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;GPU Available:&quot;</span>, torch.cuda.is_available())

<span style="color: #8B008B; font-weight: bold">if</span> torch.cuda.is_available():
    device = torch.device(<span style="color: #CD5555">&quot;cuda:0&quot;</span>)
<span style="color: #8B008B; font-weight: bold">else</span>:
    device = <span style="color: #CD5555">&quot;cpu&quot;</span>

<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">torch.nn</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">nn</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="setting-up-the-gan">Setting up the GAN </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22">## define a function for the generator:</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">make_generator_network</span>(
        input_size=<span style="color: #B452CD">20</span>,
        num_hidden_layers=<span style="color: #B452CD">1</span>,
        num_hidden_units=<span style="color: #B452CD">100</span>,
        num_output_units=<span style="color: #B452CD">784</span>):
    model = nn.Sequential()
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(num_hidden_layers):
        model.add_module(<span style="color: #CD5555">f&#39;fc_g{</span>i<span style="color: #CD5555">}&#39;</span>, 
                         nn.Linear(input_size, 
                                   num_hidden_units)) 
        model.add_module(<span style="color: #CD5555">f&#39;relu_g{</span>i<span style="color: #CD5555">}&#39;</span>, 
                         nn.LeakyReLU())     
        input_size = num_hidden_units
    model.add_module(<span style="color: #CD5555">f&#39;fc_g{</span>num_hidden_layers<span style="color: #CD5555">}&#39;</span>, 
                    nn.Linear(input_size, num_output_units))   
    model.add_module(<span style="color: #CD5555">&#39;tanh_g&#39;</span>, nn.Tanh())      
    <span style="color: #8B008B; font-weight: bold">return</span> model

<span style="color: #228B22">## define a function for the discriminator:</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">make_discriminator_network</span>(
        input_size,
        num_hidden_layers=<span style="color: #B452CD">1</span>,
        num_hidden_units=<span style="color: #B452CD">100</span>,
        num_output_units=<span style="color: #B452CD">1</span>):
    model = nn.Sequential()
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(num_hidden_layers):
        model.add_module(<span style="color: #CD5555">f&#39;fc_d{</span>i<span style="color: #CD5555">}&#39;</span>, 
                 nn.Linear(input_size, 
                           num_hidden_units, bias=<span style="color: #8B008B; font-weight: bold">False</span>)) 
        model.add_module(<span style="color: #CD5555">f&#39;relu_d{</span>i<span style="color: #CD5555">}&#39;</span>, 
                         nn.LeakyReLU())  
        model.add_module(<span style="color: #CD5555">&#39;dropout&#39;</span>, nn.Dropout(p=<span style="color: #B452CD">0.5</span>))
        input_size = num_hidden_units
    model.add_module(<span style="color: #CD5555">f&#39;fc_d{</span>num_hidden_layers<span style="color: #CD5555">}&#39;</span>, 
                     nn.Linear(input_size, num_output_units))   
    model.add_module(<span style="color: #CD5555">&#39;sigmoid&#39;</span>, nn.Sigmoid())
    <span style="color: #8B008B; font-weight: bold">return</span> model
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="printing-the-model">Printing the model </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;">image_size = (<span style="color: #B452CD">28</span>, <span style="color: #B452CD">28</span>)
z_size = <span style="color: #B452CD">20</span>

gen_hidden_layers = <span style="color: #B452CD">1</span>
gen_hidden_size = <span style="color: #B452CD">100</span>
disc_hidden_layers = <span style="color: #B452CD">1</span>
disc_hidden_size = <span style="color: #B452CD">100</span>

torch.manual_seed(<span style="color: #B452CD">1</span>)

gen_model = make_generator_network(
    input_size=z_size,
    num_hidden_layers=gen_hidden_layers, 
    num_hidden_units=gen_hidden_size,
    num_output_units=np.prod(image_size))
 
<span style="color: #658b00">print</span>(gen_model)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;">disc_model = make_discriminator_network(
    input_size=np.prod(image_size),
    num_hidden_layers=disc_hidden_layers,
    num_hidden_units=disc_hidden_size)

<span style="color: #658b00">print</span>(disc_model)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="defining-the-training-set">Defining the training set </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">torchvision</span> 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">torchvision</span> <span style="color: #8B008B; font-weight: bold">import</span> transforms 


image_path = <span style="color: #CD5555">&#39;./&#39;</span>
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=(<span style="color: #B452CD">0.5</span>), std=(<span style="color: #B452CD">0.5</span>)),
])
mnist_dataset = torchvision.datasets.MNIST(root=image_path, 
                                           train=<span style="color: #8B008B; font-weight: bold">True</span>, 
                                           transform=transform, 
                                           download=<span style="color: #8B008B; font-weight: bold">True</span>)

example, label = <span style="color: #658b00">next</span>(<span style="color: #658b00">iter</span>(mnist_dataset))
<span style="color: #658b00">print</span>(<span style="color: #CD5555">f&#39;Min: {</span>example.min()<span style="color: #CD5555">} Max: {</span>example.max()<span style="color: #CD5555">}&#39;</span>)
<span style="color: #658b00">print</span>(example.shape)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="defining-the-training-set-part-2">Defining the training set, part 2 </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">create_noise</span>(batch_size, z_size, mode_z):
    <span style="color: #8B008B; font-weight: bold">if</span> mode_z == <span style="color: #CD5555">&#39;uniform&#39;</span>:
        input_z = torch.rand(batch_size, z_size)*<span style="color: #B452CD">2</span> - <span style="color: #B452CD">1</span> 
    <span style="color: #8B008B; font-weight: bold">elif</span> mode_z == <span style="color: #CD5555">&#39;normal&#39;</span>:
        input_z = torch.randn(batch_size, z_size)
    <span style="color: #8B008B; font-weight: bold">return</span> input_z


<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">torch.utils.data</span> <span style="color: #8B008B; font-weight: bold">import</span> DataLoader


batch_size = <span style="color: #B452CD">32</span>
dataloader = DataLoader(mnist_dataset, batch_size, shuffle=<span style="color: #8B008B; font-weight: bold">False</span>)
input_real, label = <span style="color: #658b00">next</span>(<span style="color: #658b00">iter</span>(dataloader))
input_real = input_real.view(batch_size, -<span style="color: #B452CD">1</span>)

torch.manual_seed(<span style="color: #B452CD">1</span>)
mode_z = <span style="color: #CD5555">&#39;uniform&#39;</span>  <span style="color: #228B22"># &#39;uniform&#39; vs. &#39;normal&#39;</span>
input_z = create_noise(batch_size, z_size, mode_z)

<span style="color: #658b00">print</span>(<span style="color: #CD5555">&#39;input-z -- shape:&#39;</span>, input_z.shape)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&#39;input-real -- shape:&#39;</span>, input_real.shape)

g_output = gen_model(input_z)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&#39;Output of G -- shape:&#39;</span>, g_output.shape)

d_proba_real = disc_model(input_real)
d_proba_fake = disc_model(g_output)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&#39;Disc. (real) -- shape:&#39;</span>, d_proba_real.shape)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&#39;Disc. (fake) -- shape:&#39;</span>, d_proba_fake.shape)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="training-the-gan">Training the GAN </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;">loss_fn = nn.BCELoss()

<span style="color: #228B22">## Loss for the Generator</span>
g_labels_real = torch.ones_like(d_proba_fake)
g_loss = loss_fn(d_proba_fake, g_labels_real)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">f&#39;Generator Loss: {</span>g_loss<span style="color: #CD5555">:.4f}&#39;</span>)

<span style="color: #228B22">## Loss for the Discriminator</span>
d_labels_real = torch.ones_like(d_proba_real)
d_labels_fake = torch.zeros_like(d_proba_fake)

d_loss_real = loss_fn(d_proba_real, d_labels_real)
d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">f&#39;Discriminator Losses: Real {</span>d_loss_real<span style="color: #CD5555">:.4f} Fake {</span>d_loss_fake<span style="color: #CD5555">:.4f}&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="more-on-training">More on training  </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;">batch_size = <span style="color: #B452CD">64</span>

torch.manual_seed(<span style="color: #B452CD">1</span>)
np.random.seed(<span style="color: #B452CD">1</span>)

<span style="color: #228B22">## Set up the dataset</span>
mnist_dl = DataLoader(mnist_dataset, batch_size=batch_size, 
                      shuffle=<span style="color: #8B008B; font-weight: bold">True</span>, drop_last=<span style="color: #8B008B; font-weight: bold">True</span>)
 
<span style="color: #228B22">## Set up the models</span>
gen_model = make_generator_network(
    input_size=z_size,
    num_hidden_layers=gen_hidden_layers, 
    num_hidden_units=gen_hidden_size,
    num_output_units=np.prod(image_size)).to(device)
 
disc_model = make_discriminator_network(
    input_size=np.prod(image_size),
    num_hidden_layers=disc_hidden_layers,
    num_hidden_units=disc_hidden_size).to(device)
 
<span style="color: #228B22">## Loss function and optimizers:</span>
loss_fn = nn.BCELoss()
g_optimizer = torch.optim.Adam(gen_model.parameters())
d_optimizer = torch.optim.Adam(disc_model.parameters())

<span style="color: #228B22">## Train the discriminator</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">d_train</span>(x):
    disc_model.zero_grad()

    <span style="color: #228B22"># Train discriminator with a real batch</span>
    batch_size = x.size(<span style="color: #B452CD">0</span>)
    x = x.view(batch_size, -<span style="color: #B452CD">1</span>).to(device)
    d_labels_real = torch.ones(batch_size, <span style="color: #B452CD">1</span>, device=device)

    d_proba_real = disc_model(x)
    d_loss_real = loss_fn(d_proba_real, d_labels_real)

    <span style="color: #228B22"># Train discriminator on a fake batch</span>
    input_z = create_noise(batch_size, z_size, mode_z).to(device)
    g_output = gen_model(input_z)
    
    d_proba_fake = disc_model(g_output)
    d_labels_fake = torch.zeros(batch_size, <span style="color: #B452CD">1</span>, device=device)
    d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)

    <span style="color: #228B22"># gradient backprop &amp; optimize ONLY D&#39;s parameters</span>
    d_loss = d_loss_real + d_loss_fake
    d_loss.backward()
    d_optimizer.step()
  
    <span style="color: #8B008B; font-weight: bold">return</span> d_loss.data.item(), d_proba_real.detach(), d_proba_fake.detach()

<span style="color: #228B22">## Train the generator</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">g_train</span>(x):
    gen_model.zero_grad()
    
    batch_size = x.size(<span style="color: #B452CD">0</span>)
    input_z = create_noise(batch_size, z_size, mode_z).to(device)
    g_labels_real = torch.ones(batch_size, <span style="color: #B452CD">1</span>, device=device)

    g_output = gen_model(input_z)
    d_proba_fake = disc_model(g_output)
    g_loss = loss_fn(d_proba_fake, g_labels_real)

    <span style="color: #228B22"># gradient backprop &amp; optimize ONLY G&#39;s parameters</span>
    g_loss.backward()
    g_optimizer.step()
        
    <span style="color: #8B008B; font-weight: bold">return</span> g_loss.data.item()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;">fixed_z = create_noise(batch_size, z_size, mode_z).to(device)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">create_samples</span>(g_model, input_z):
    g_output = g_model(input_z)
    images = torch.reshape(g_output, (batch_size, *image_size))    
    <span style="color: #8B008B; font-weight: bold">return</span> (images+<span style="color: #B452CD">1</span>)/<span style="color: #B452CD">2.0</span>

epoch_samples = []

all_d_losses = []
all_g_losses = []

all_d_real = []
all_d_fake = []

num_epochs = <span style="color: #B452CD">100</span>
torch.manual_seed(<span style="color: #B452CD">1</span>)
<span style="color: #8B008B; font-weight: bold">for</span> epoch <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">1</span>, num_epochs+<span style="color: #B452CD">1</span>):
    d_losses, g_losses = [], []
    d_vals_real, d_vals_fake = [], []
    <span style="color: #8B008B; font-weight: bold">for</span> i, (x, _) <span style="color: #8B008B">in</span> <span style="color: #658b00">enumerate</span>(mnist_dl):
        d_loss, d_proba_real, d_proba_fake = d_train(x)
        d_losses.append(d_loss)
        g_losses.append(g_train(x))
        
        d_vals_real.append(d_proba_real.mean().cpu())
        d_vals_fake.append(d_proba_fake.mean().cpu())
        
    all_d_losses.append(torch.tensor(d_losses).mean())
    all_g_losses.append(torch.tensor(g_losses).mean())
    all_d_real.append(torch.tensor(d_vals_real).mean())
    all_d_fake.append(torch.tensor(d_vals_fake).mean())
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">f&#39;Epoch {</span>epoch<span style="color: #CD5555">:03d} | Avg Losses &gt;&gt;&#39;</span>
          <span style="color: #CD5555">f&#39; G/D {</span>all_g_losses[-<span style="color: #B452CD">1</span>]<span style="color: #CD5555">:.4f}/{</span>all_d_losses[-<span style="color: #B452CD">1</span>]<span style="color: #CD5555">:.4f}&#39;</span>
          <span style="color: #CD5555">f&#39; [D-Real: {</span>all_d_real[-<span style="color: #B452CD">1</span>]<span style="color: #CD5555">:.4f} D-Fake: {</span>all_d_fake[-<span style="color: #B452CD">1</span>]<span style="color: #CD5555">:.4f}]&#39;</span>)
    epoch_samples.append(
        create_samples(gen_model, fixed_z).detach().cpu().numpy())
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="visualizing">Visualizing </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">itertools</span>


fig = plt.figure(figsize=(<span style="color: #B452CD">16</span>, <span style="color: #B452CD">6</span>))

<span style="color: #228B22">## Plotting the losses</span>
ax = fig.add_subplot(<span style="color: #B452CD">1</span>, <span style="color: #B452CD">2</span>, <span style="color: #B452CD">1</span>)
 
plt.plot(all_g_losses, label=<span style="color: #CD5555">&#39;Generator loss&#39;</span>)
half_d_losses = [all_d_loss/<span style="color: #B452CD">2</span> <span style="color: #8B008B; font-weight: bold">for</span> all_d_loss <span style="color: #8B008B">in</span> all_d_losses]
plt.plot(half_d_losses, label=<span style="color: #CD5555">&#39;Discriminator loss&#39;</span>)
plt.legend(fontsize=<span style="color: #B452CD">20</span>)
ax.set_xlabel(<span style="color: #CD5555">&#39;Iteration&#39;</span>, size=<span style="color: #B452CD">15</span>)
ax.set_ylabel(<span style="color: #CD5555">&#39;Loss&#39;</span>, size=<span style="color: #B452CD">15</span>)

<span style="color: #228B22">## Plotting the outputs of the discriminator</span>
ax = fig.add_subplot(<span style="color: #B452CD">1</span>, <span style="color: #B452CD">2</span>, <span style="color: #B452CD">2</span>)
plt.plot(all_d_real, label=<span style="color: #CD5555">r&#39;Real: $D(\mathbf{x})$&#39;</span>)
plt.plot(all_d_fake, label=<span style="color: #CD5555">r&#39;Fake: $D(G(\mathbf{z}))$&#39;</span>)
plt.legend(fontsize=<span style="color: #B452CD">20</span>)
ax.set_xlabel(<span style="color: #CD5555">&#39;Iteration&#39;</span>, size=<span style="color: #B452CD">15</span>)
ax.set_ylabel(<span style="color: #CD5555">&#39;Discriminator output&#39;</span>, size=<span style="color: #B452CD">15</span>)

<span style="color: #228B22">#plt.savefig(&#39;figures/ch17-gan-learning-curve.pdf&#39;)</span>
plt.show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;">selected_epochs = [<span style="color: #B452CD">1</span>, <span style="color: #B452CD">2</span>, <span style="color: #B452CD">4</span>, <span style="color: #B452CD">10</span>, <span style="color: #B452CD">50</span>, <span style="color: #B452CD">100</span>]
fig = plt.figure(figsize=(<span style="color: #B452CD">10</span>, <span style="color: #B452CD">14</span>))
<span style="color: #8B008B; font-weight: bold">for</span> i,e <span style="color: #8B008B">in</span> <span style="color: #658b00">enumerate</span>(selected_epochs):
    <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">5</span>):
        ax = fig.add_subplot(<span style="color: #B452CD">6</span>, <span style="color: #B452CD">5</span>, i*<span style="color: #B452CD">5</span>+j+<span style="color: #B452CD">1</span>)
        ax.set_xticks([])
        ax.set_yticks([])
        <span style="color: #8B008B; font-weight: bold">if</span> j == <span style="color: #B452CD">0</span>:
            ax.text(
                -<span style="color: #B452CD">0.06</span>, <span style="color: #B452CD">0.5</span>, <span style="color: #CD5555">f&#39;Epoch {</span>e<span style="color: #CD5555">}&#39;</span>,
                rotation=<span style="color: #B452CD">90</span>, size=<span style="color: #B452CD">18</span>, color=<span style="color: #CD5555">&#39;red&#39;</span>,
                horizontalalignment=<span style="color: #CD5555">&#39;right&#39;</span>,
                verticalalignment=<span style="color: #CD5555">&#39;center&#39;</span>, 
                transform=ax.transAxes)
        
        image = epoch_samples[e-<span style="color: #B452CD">1</span>][j]
        ax.imshow(image, cmap=<span style="color: #CD5555">&#39;gray_r&#39;</span>)
    
<span style="color: #228B22">#plt.savefig(&#39;figures/ch17-vanila-gan-samples.pdf&#39;)</span>
plt.show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="calculating-scores">Calculating scores </h2>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">math</span>


<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">distance</span>(X, Y, sqrt):
    nX = X.size(<span style="color: #B452CD">0</span>)
    nY = Y.size(<span style="color: #B452CD">0</span>)
    X = X.view(nX,-<span style="color: #B452CD">1</span>).cuda()
    X2 = (X*X).sum(<span style="color: #B452CD">1</span>).resize_(nX,<span style="color: #B452CD">1</span>)
    Y = Y.view(nY,-<span style="color: #B452CD">1</span>).cuda()
    Y2 = (Y*Y).sum(<span style="color: #B452CD">1</span>).resize_(nY,<span style="color: #B452CD">1</span>)

    M = torch.zeros(nX, nY)
    M.copy_(X2.expand(nX,nY) + Y2.expand(nY,nX).transpose(<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>) - <span style="color: #B452CD">2</span>*torch.mm(X,Y.transpose(<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>)))

    <span style="color: #8B008B; font-weight: bold">del</span> X, X2, Y, Y2
    
    <span style="color: #8B008B; font-weight: bold">if</span> sqrt:
        M = ((M+M.abs())/<span style="color: #B452CD">2</span>).sqrt()
    
    <span style="color: #8B008B; font-weight: bold">return</span> M

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">mmd</span>(Mxx, Mxy, Myy, sigma) :
    scale = Mxx.mean()
    Mxx = torch.exp(-Mxx/(scale*<span style="color: #B452CD">2</span>*sigma*sigma))
    Mxy = torch.exp(-Mxy/(scale*<span style="color: #B452CD">2</span>*sigma*sigma))
    Myy = torch.exp(-Myy/(scale*<span style="color: #B452CD">2</span>*sigma*sigma))
    a = Mxx.mean()+Myy.mean()-<span style="color: #B452CD">2</span>*Mxy.mean()
    mmd = math.sqrt(<span style="color: #658b00">max</span>(a, <span style="color: #B452CD">0</span>))

    <span style="color: #8B008B; font-weight: bold">return</span> mmd

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">compute_score</span>(fake, real , k=<span style="color: #B452CD">1</span>, sigma=<span style="color: #B452CD">1</span>, sqrt=<span style="color: #8B008B; font-weight: bold">True</span>):

    Mxx = distance(real, real, <span style="color: #8B008B; font-weight: bold">False</span>)
    Mxy = distance(real, fake, <span style="color: #8B008B; font-weight: bold">False</span>)
    Myy = distance(fake, fake, <span style="color: #8B008B; font-weight: bold">False</span>)

 
    <span style="color: #658b00">print</span>(mmd(Mxx, Mxy, Myy, sigma))

whole_dl = DataLoader(mnist_dataset, batch_size=<span style="color: #B452CD">10000</span>, 
                      shuffle=<span style="color: #8B008B; font-weight: bold">True</span>, drop_last=<span style="color: #8B008B; font-weight: bold">True</span>) 
real_image = <span style="color: #658b00">next</span>(<span style="color: #658b00">iter</span>(whole_dl))[<span style="color: #B452CD">0</span>]
compute_score(torch.from_numpy(epoch_samples[-<span style="color: #B452CD">1</span>]), real_image)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="more-codes">More codes </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<li> For codes and background, see Raschka et al, Machine Learning with PyTorch and Scikit-Learn, chapter 17, see <a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch17" target="_blank"><tt>https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch17</tt></a> for codes</li>
<li> Babcock and Bali, Generative AI with Python and TensorFlow2, chapter 6 and codes at <a href="https://github.com/raghavbali/generative_ai_with_tensorflow/blob/master/Chapter_6/conditional_gan.ipynb" target="_blank"><tt>https://github.com/raghavbali/generative_ai_with_tensorflow/blob/master/Chapter_6/conditional_gan.ipynb</tt></a></li>
<li> See also Foster's text Generative Deep Learning and chapter 4 with codes at <a href="https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/tree/main/notebooks/04_gan" target="_blank"><tt>https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/tree/main/notebooks/04_gan</tt></a></li>
</ol>
</div>


<!-- ------------------- end of main content --------------- -->
<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2025, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</body>
</html>

