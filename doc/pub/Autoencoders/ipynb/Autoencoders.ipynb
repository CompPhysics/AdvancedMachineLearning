{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c4b4dc",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html Autoencoders.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Data Analysis and Machine Learning: Autoencoders -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467a6244",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Analysis and Machine Learning: Autoencoders\n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo and Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University\n",
    "\n",
    "Date: **Jan 25, 2023**\n",
    "\n",
    "Copyright 1999-2023, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e583c39",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Tod do\n",
    "\n",
    "* make example link with pca\n",
    "\n",
    "* develop examples and add more text\n",
    "\n",
    "* excellent website <http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4391d6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Autoencoders: Overarching view\n",
    "\n",
    "Autoencoders are artificial neural networks capable of learning\n",
    "efficient representations of the input data (these representations are called codings)  without\n",
    "any supervision (i.e., the training set is unlabeled). These codings\n",
    "typically have a much lower dimensionality than the input data, making\n",
    "autoencoders useful for dimensionality reduction. \n",
    "\n",
    "More importantly, autoencoders act as powerful feature detectors, and\n",
    "they can be used for unsupervised pretraining of deep neural networks.\n",
    "\n",
    "Lastly, they are capable of randomly generating new data that looks\n",
    "very similar to the training data; this is called a generative\n",
    "model. For example, you could train an autoencoder on pictures of\n",
    "faces, and it would then be able to generate new faces.  Surprisingly,\n",
    "autoencoders work by simply learning to copy their inputs to their\n",
    "outputs. This may sound like a trivial task, but we will see that\n",
    "constraining the network in various ways can make it rather\n",
    "difficult. For example, you can limit the size of the internal\n",
    "representation, or you can add noise to the inputs and train the\n",
    "network to recover the original inputs. These constraints prevent the\n",
    "autoencoder from trivially copying the inputs directly to the outputs,\n",
    "which forces it to learn efficient ways of representing the data. In\n",
    "short, the codings are byproducts of the autoencoderâ€™s attempt to\n",
    "learn the identity function under some constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a1c7e3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple examples of Autoencoders"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
