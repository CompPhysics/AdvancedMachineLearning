<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week1.do.txt --pygments_html_style=default --html_style=bloodish --html_links_in_new_window --html_output=week1 --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Mathematics of discriminative and generative deep learning, from deep neural networks to diffusion models">
<title>Mathematics of discriminative and generative deep learning, from deep neural networks to diffusion models</title>
<style type="text/css">
/* bloodish style */
body {
  font-family: Helvetica, Verdana, Arial, Sans-serif;
  color: #404040;
  background: #ffffff;
}
h1 { font-size: 1.8em; color: #8A0808; }
h2 { font-size: 1.6em; color: #8A0808; }
h3 { font-size: 1.4em; color: #8A0808; }
h4 { font-size: 1.2em; color: #8A0808; }
a { color: #8A0808; text-decoration:none; }
tt { font-family: "Courier New", Courier; }
p { text-indent: 0px; }
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-style: normal; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa; }div.highlight {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    line-height: 1.21429em;
}
div.cell {
    width: 100%;
    padding: 5px 5px 5px 0;
    margin: 0;
    outline: none;
}
div.input {
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.inner_cell {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
    box-flex: 1;
    flex: 1;
}
div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 4px;
    background: #f7f7f7;
    line-height: 1.21429em;
}
div.input_area > div.highlight {
    margin: .4em;
    border: none;
    padding: 0;
    background-color: transparent;
}
div.output_wrapper {
    position: relative;
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
.output {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
div.output_area {
    padding: 0;
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.output_subarea {
    padding: .4em .4em 0 .4em;
    box-flex: 1;
    flex: 1;
}
div.output_text {
    text-align: left;
    color: #000;
    line-height: 1.21429em;
}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #bababa;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #f8f8f8;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_question.png); }
div { text-align: justify; text-justify: inter-word; }
.tab {
  padding-left: 1.5em;
}
div.toc p,a {
  line-height: 1.3;
  margin-top: 1.1;
  margin-bottom: 1.1;
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Types of machine learning',
               2,
               None,
               'types-of-machine-learning'),
              ('Main categories', 2, None, 'main-categories'),
              ('Machine learning. A simple perspective on the interface '
               'between ML and Physics',
               2,
               None,
               'machine-learning-a-simple-perspective-on-the-interface-between-ml-and-physics'),
              ('ML in Nuclear  Physics (or any field in physics)',
               2,
               None,
               'ml-in-nuclear-physics-or-any-field-in-physics'),
              ('The plethora  of machine learning algorithms/methods',
               2,
               None,
               'the-plethora-of-machine-learning-algorithms-methods'),
              ('What Is Generative Modeling?',
               2,
               None,
               'what-is-generative-modeling'),
              ('Example of generative modeling, "taken from Generative Deep '
               'Learning by David '
               'Foster":"https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html"',
               2,
               None,
               'example-of-generative-modeling-taken-from-generative-deep-learning-by-david-foster-https-www-oreilly-com-library-view-generative-deep-learning-9781098134174-ch01-html'),
              ('Generative Modeling', 2, None, 'generative-modeling'),
              ('Generative Versus Discriminative Modeling',
               2,
               None,
               'generative-versus-discriminative-modeling'),
              ('Example of discriminative modeling, "taken from Generative '
               'Deeep Learning by David '
               'Foster":"https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html"',
               2,
               None,
               'example-of-discriminative-modeling-taken-from-generative-deeep-learning-by-david-foster-https-www-oreilly-com-library-view-generative-deep-learning-9781098134174-ch01-html'),
              ('Discriminative Modeling', 2, None, 'discriminative-modeling'),
              ('Taxonomy of generative deep learning, "taken from Generative '
               'Deep Learning by David '
               'Foster":"https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html"',
               2,
               None,
               'taxonomy-of-generative-deep-learning-taken-from-generative-deep-learning-by-david-foster-https-www-oreilly-com-library-view-generative-deep-learning-9781098134174-ch01-html'),
              ('Good books with hands-on material and codes',
               2,
               None,
               'good-books-with-hands-on-material-and-codes'),
              ('More references', 2, None, 'more-references'),
              ('What are the basic Machine Learning ingredients?',
               2,
               None,
               'what-are-the-basic-machine-learning-ingredients'),
              ('Low-level machine learning, the family of ordinary least '
               'squares methods',
               2,
               None,
               'low-level-machine-learning-the-family-of-ordinary-least-squares-methods'),
              ('Setting up the equations', 2, None, 'setting-up-the-equations'),
              ('The objective/cost/loss function',
               2,
               None,
               'the-objective-cost-loss-function'),
              ('Training solution', 2, None, 'training-solution'),
              ('Ridge and LASSO Regression',
               2,
               None,
               'ridge-and-lasso-regression'),
              ('From OLS to Ridge and Lasso',
               2,
               None,
               'from-ols-to-ridge-and-lasso'),
              ('Lasso regression', 2, None, 'lasso-regression'),
              ('Selected references', 2, None, 'selected-references'),
              ('Setting up the basic equations for neural networks',
               2,
               None,
               'setting-up-the-basic-equations-for-neural-networks'),
              ('Overarching view of a neural network',
               2,
               None,
               'overarching-view-of-a-neural-network'),
              ('Illustration of a single perceptron model and a multilayer '
               'FFNN',
               2,
               None,
               'illustration-of-a-single-perceptron-model-and-a-multilayer-ffnn'),
              ('The optimization problem', 2, None, 'the-optimization-problem'),
              ('Weights and biases', 2, None, 'weights-and-biases'),
              ('Other ingredients of a neural network',
               2,
               None,
               'other-ingredients-of-a-neural-network'),
              ('Other parameters', 2, None, 'other-parameters'),
              ('Why Feed Forward Neural Networks (FFNN)?',
               2,
               None,
               'why-feed-forward-neural-networks-ffnn'),
              ('Universal approximation theorem',
               2,
               None,
               'universal-approximation-theorem'),
              ('The approximation theorem in words',
               2,
               None,
               'the-approximation-theorem-in-words'),
              ('More on the general approximation theorem',
               2,
               None,
               'more-on-the-general-approximation-theorem'),
              ('Class of functions we can approximate',
               2,
               None,
               'class-of-functions-we-can-approximate'),
              ('Simple example, fitting nuclear masses',
               2,
               None,
               'simple-example-fitting-nuclear-masses')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- ------------------- main content ---------------------- -->
<center>
<h1>Mathematics of discriminative and generative deep learning, from deep neural networks to diffusion models</h1>
</center>  <!-- document title -->

<!-- author(s): Morten Hjorth-Jensen -->
<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>
<!-- institution(s) -->
<center>
[1] <b>Department of Physics and Center for Computing in Science Education, University of Oslo, Norway</b>
</center>
<center>
[2] <b>Department of Physics and Astronomy and Facility for Rare Isotope Beams, Michigan State University, East Lansing, Michigan, USA</b>
</center>
<br>
<center>
<h4>STREAMLINE meeting, May 9-10, MSU, 2024</h4>
</center> <!-- date -->
<br>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="types-of-machine-learning">Types of machine learning </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>The approaches to machine learning are many, but are often split into two main categories. 
In <em>supervised learning</em> we know the answer to a problem,
and let the computer deduce the logic behind it. On the other hand, <em>unsupervised learning</em>
is a method for finding patterns and relationship in data sets without any prior knowledge of the system.
</p>

<p>An emerging  third category is  <em>reinforcement learning</em>. This is a paradigm 
of learning inspired by behavioural psychology, where learning is achieved by trial-and-error, 
solely from rewards and punishment.
</p>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="main-categories">Main categories </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>Another way to categorize machine learning tasks is to consider the desired output of a system.
Some of the most common tasks are:
</p>

<ul>
  <li> Classification: Outputs are divided into two or more classes. The goal is to   produce a model that assigns inputs into one of these classes. An example is to identify  digits based on pictures of hand-written ones. Classification is typically supervised learning.</li>
  <li> Regression: Finding a functional relationship between an input data set and a reference data set.   The goal is to construct a function that maps input data to continuous output values.</li>
  <li> Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.  It is thus a form of unsupervised learning.</li>
</ul>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="machine-learning-a-simple-perspective-on-the-interface-between-ml-and-physics">Machine learning. A simple perspective on the interface between ML and Physics </h2>

<br/><br/>
<center>
<p><img src="figures/mlimage.png" width="800" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="ml-in-nuclear-physics-or-any-field-in-physics">ML in Nuclear  Physics (or any field in physics) </h2>

<br/><br/>
<center>
<p><img src="figures/ML-NP.png" width="900" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="the-plethora-of-machine-learning-algorithms-methods">The plethora  of machine learning algorithms/methods </h2>

<ol>
<li> Deep learning: Neural Networks (NN), Convolutional NN, Recurrent NN, Boltzmann machines, autoencoders and variational autoencoders  and generative adversarial networks, stable diffusion and many more generative models</li>
<li> Bayesian statistics and Bayesian Machine Learning, Bayesian experimental design, Bayesian Regression models, Bayesian neural networks, Gaussian processes and much more</li>
<li> Dimensionality reduction (Principal component analysis), Clustering Methods and more</li>
<li> Ensemble Methods, Random forests, bagging and voting methods, gradient boosting approaches</li> 
<li> Linear and logistic regression, Kernel methods, support vector machines and more</li>
<li> Reinforcement Learning; Transfer Learning and more</li> 
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="what-is-generative-modeling">What Is Generative Modeling? </h2>

<p>Generative modeling can be broadly defined as follows:</p>

<p>Generative modeling is a branch of machine learning that involves
training a model to produce new data that is similar to a given
dataset.
</p>

<p>What does this mean in practice? Suppose we have a dataset containing
photos of horses. We can train a generative model on this dataset to
capture the rules that govern the complex relationships between pixels
in images of horses. Then we can sample from this model to create
novel, realistic images of horses that did not exist in the original
dataset. 
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="example-of-generative-modeling-taken-from-generative-deep-learning-by-david-foster-https-www-oreilly-com-library-view-generative-deep-learning-9781098134174-ch01-html">Example of generative modeling, <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html" target="_blank">taken from Generative Deep Learning by David Foster</a>  </h2>

<br/><br/>
<center>
<p><img src="figures/generativelearning.png" width="900" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="generative-modeling">Generative Modeling </h2>

<p>In order to build a generative model, we require a dataset consisting
of many examples of the entity we are trying to generate. This is
known as the training data, and one such data point is called an
observation.
</p>

<p>Each observation consists of many features. For an image generation
problem, the features are usually the individual pixel values; for a
text generation problem, the features could be individual words or
groups of letters. It is our goal to build a model that can generate
new sets of features that look as if they have been created using the
same rules as the original data. Conceptually, for image generation
this is an incredibly difficult task, considering the vast number of
ways that individual pixel values can be assigned and the relatively
tiny number of such arrangements that constitute an image of the
entity we are trying to generate.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="generative-versus-discriminative-modeling">Generative Versus Discriminative Modeling </h2>

<p>In order to truly understand what generative modeling aims to achieve
and why this is important, it is useful to compare it to its
counterpart, discriminative modeling. If you have studied machine
learning, most problems you will have faced will have most likely been
discriminative in nature. 
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="example-of-discriminative-modeling-taken-from-generative-deeep-learning-by-david-foster-https-www-oreilly-com-library-view-generative-deep-learning-9781098134174-ch01-html">Example of discriminative modeling, <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html" target="_blank">taken from Generative Deeep Learning by David Foster</a>  </h2>

<br/><br/>
<center>
<p><img src="figures/standarddeeplearning.png" width="900" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="discriminative-modeling">Discriminative Modeling </h2>

<p>When performing discriminative modeling, each observation in the
training data has a label. For a binary classification problem such as
our data could be labeled as ones and zeros. Our model then learns how to
discriminate between these two groups and outputs the probability that
a new observation has label 1 or 0
</p>

<p>In contrast, generative modeling doesn&#8217;t require the dataset to be
labeled because it concerns itself with generating entirely new
data (for example an image), rather than trying to predict a label for say  a given image.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="taxonomy-of-generative-deep-learning-taken-from-generative-deep-learning-by-david-foster-https-www-oreilly-com-library-view-generative-deep-learning-9781098134174-ch01-html">Taxonomy of generative deep learning, <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html" target="_blank">taken from Generative Deep Learning by David Foster</a>  </h2>

<br/><br/>
<center>
<p><img src="figures/generativemodels.png" width="900" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="good-books-with-hands-on-material-and-codes">Good books with hands-on material and codes </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ul>
<li> <a href="https://sebastianraschka.com/blog/2022/ml-pytorch-book.html" target="_blank">Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch</a></li>
<li> <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html" target="_blank">David Foster, Generative Deep Learning with TensorFlow</a></li>
<li> <a href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2" target="_blank">Babcock and Gavras, Generative AI with Python and TensorFlow 2</a></li>
</ul>
</div>


<p>All three books have GitHub sites from where  one can download all codes. A good and more general text (2016)
is Goodfellow, Bengio and Courville, <a href="https://www.deeplearningbook.org/" target="_blank">Deep Learning</a>
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="more-references">More references </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Reading on diffusion models</b>
<p>
<ol>
<li> A central paper is the one by Sohl-Dickstein et al, Deep Unsupervised Learning using Nonequilibrium Thermodynamics, <a href="https://arxiv.org/abs/1503.03585" target="_blank"><tt>https://arxiv.org/abs/1503.03585</tt></a></li>
<li> See also Diederik P. Kingma, Tim Salimans, Ben Poole, Jonathan Ho, Variational Diffusion Models, <a href="https://arxiv.org/abs/2107.00630" target="_blank"><tt>https://arxiv.org/abs/2107.00630</tt></a></li>
</ol>
</div>
   

<div class="alert alert-block alert-block alert-text-normal">
<b>and VAEs</b>
<p>
<ol>
<li> Calvin Luo <a href="https://calvinyluo.com/2022/08/26/diffusion-tutorial.html" target="_blank"><tt>https://calvinyluo.com/2022/08/26/diffusion-tutorial.html</tt></a></li>
<li> An Introduction to Variational Autoencoders, by Kingma and Welling, see <a href="https://arxiv.org/abs/1906.02691" target="_blank"><tt>https://arxiv.org/abs/1906.02691</tt></a></li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="what-are-the-basic-machine-learning-ingredients">What are the basic Machine Learning ingredients? </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>Almost every problem in ML and data science starts with the same ingredients:</p>
<ul>
<li> The dataset \( \boldsymbol{x} \) (could be some observable quantity of the system we are studying)</li>
<li> A model which is a function of a set of parameters \( \boldsymbol{\alpha} \) that relates to the dataset, say a likelihood  function \( p(\boldsymbol{x}\vert \boldsymbol{\alpha}) \) or just a simple model \( f(\boldsymbol{\alpha}) \)</li>
<li> A so-called <b>loss/cost/risk</b> function \( \mathcal{C} (\boldsymbol{x}, f(\boldsymbol{\alpha})) \) which allows us to decide how well our model represents the dataset.</li> 
</ul>
<p>We seek to minimize the function \( \mathcal{C} (\boldsymbol{x}, f(\boldsymbol{\alpha})) \) by finding the parameter values which minimize \( \mathcal{C} \). This leads to  various minimization algorithms. It may surprise many, but at the heart of all machine learning algortihms there is an optimization problem. </p>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="low-level-machine-learning-the-family-of-ordinary-least-squares-methods">Low-level machine learning, the family of ordinary least squares methods  </h2>

<p>Our data which we want to apply a machine learning method on, consist
of a set of inputs \( \boldsymbol{x}^T=[x_0,x_1,x_2,\dots,x_{n-1}] \) and the
outputs we want to model \( \boldsymbol{y}^T=[y_0,y_1,y_2,\dots,y_{n-1}] \).
We assume  that the output data can be represented (for a regression case) by a continuous function \( f \)
through
</p>
$$
\boldsymbol{y}=f(\boldsymbol{x})+\boldsymbol{\epsilon}.
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="setting-up-the-equations">Setting up the equations </h2>

<p>In linear regression we approximate the unknown function with another
continuous function \( \tilde{\boldsymbol{y}}(\boldsymbol{x}) \) which depends linearly on
some unknown parameters
\( \boldsymbol{\theta}^T=[\theta_0,\theta_1,\theta_2,\dots,\theta_{p-1}] \).
</p>

<p>The input data can be organized in terms of a so-called design matrix 
with an approximating function \( \boldsymbol{\tilde{y}} \) 
</p>
$$
\boldsymbol{\tilde{y}}= \boldsymbol{X}\boldsymbol{\theta},
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="the-objective-cost-loss-function">The objective/cost/loss function </h2>

<p>The  simplest approach is the mean squared error</p>
$$
C(\boldsymbol{\Theta})=\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)^T\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)\right\},
$$

<p>or using the matrix \( \boldsymbol{X} \) and in a more compact matrix-vector notation as</p>
$$
C(\boldsymbol{\Theta})=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right\}.
$$

<p>This function represents one of many possible ways to define the so-called cost function.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="training-solution">Training solution  </h2>

<p>Optimizing with respect to the unknown parameters \( \theta_j \) we get </p>
$$
\boldsymbol{X}^T\boldsymbol{y} = \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\theta},  
$$

<p>and if the matrix \( \boldsymbol{X}^T\boldsymbol{X} \) is invertible we have the optimal values</p>
$$
\hat{\boldsymbol{\theta}} =\left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}.
$$

<p>We say we 'learn' the unknown parameters \( \boldsymbol{\theta} \) from the last equation.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="ridge-and-lasso-regression">Ridge and LASSO Regression </h2>

<p>Our optimization problem is</p>
$$
{\displaystyle \min_{\boldsymbol{\theta}\in {\mathbb{R}}^{p}}}\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right\}.
$$

<p>or we can state it as</p>
$$
{\displaystyle \min_{\boldsymbol{\theta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\vert\vert_2^2,
$$

<p>where we have used the definition of  a norm-2 vector, that is</p>
$$
\vert\vert \boldsymbol{x}\vert\vert_2 = \sqrt{\sum_i x_i^2}. 
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="from-ols-to-ridge-and-lasso">From OLS to Ridge and Lasso </h2>

<p>By minimizing the above equation with respect to the parameters
\( \boldsymbol{\theta} \) we could then obtain an analytical expression for the
parameters \( \boldsymbol{\theta} \).  We can add a regularization parameter \( \lambda \) by
defining a new cost function to be optimized, that is
</p>

$$
{\displaystyle \min_{\boldsymbol{\theta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\theta}\vert\vert_2^2
$$

<p>which leads to the Ridge regression minimization problem where we
require that \( \vert\vert \boldsymbol{\theta}\vert\vert_2^2\le t \), where \( t \) is
a finite number larger than zero. We do not include such a constraints in the discussions here.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="lasso-regression">Lasso regression  </h2>

<p>Defining</p>

$$
C(\boldsymbol{X},\boldsymbol{\theta})=\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\theta}\vert\vert_1,
$$

<p>we have a new optimization equation</p>
$$
{\displaystyle \min_{\boldsymbol{\theta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\theta}\vert\vert_1
$$

<p>which leads to Lasso regression. Lasso stands for least absolute shrinkage and selection operator. 
Here we have defined the norm-1 as 
</p>
$$
\vert\vert \boldsymbol{x}\vert\vert_1 = \sum_i \vert x_i\vert. 
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="selected-references">Selected references </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ul>
<li> <a href="https://arxiv.org/abs/1803.08823" target="_blank">Mehta et al.</a> and <a href="https://www.sciencedirect.com/science/article/pii/S0370157319300766?via%3Dihub" target="_blank">Physics Reports (2019)</a>.</li>
<li> <a href="https://link.aps.org/doi/10.1103/RevModPhys.91.045002" target="_blank">Machine Learning and the Physical Sciences by Carleo et al</a></li>
<li> <a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.94.031003" target="_blank">Artificial Intelligence and Machine Learning in Nuclear Physics, Amber Boehnlein et al., Reviews Modern of Physics 94, 031003 (2022)</a></li> 
<li> <a href="https://journals.aps.org/prresearch/pdf/10.1103/PhysRevResearch.5.033062" target="_blank">Dilute neutron star matter from neural-network quantum states by Fore et al, Physical Review Research 5, 033062 (2023)</a></li>
<li> Neural-network quantum states for ultra-cold Fermi gases, Jane Kim et al, Nature Physics Communcication, in press, see <a href="https://doi.org/10.48550/arXiv.2305.08831" target="_blank"><tt>https://doi.org/10.48550/arXiv.2305.08831</tt></a></li>
<li> <a href="https://doi.org/10.48550/arXiv.2305.07240" target="_blank">Message-Passing Neural Quantum States for the Homogeneous Electron Gas, Gabriel Pescia, Jane Kim et al. arXiv.2305.07240,</a></li>
<li> <a href="https://pdg.lbl.gov/2021/reviews/rpp2021-rev-machine-learning.pdf" target="_blank">Particle Data Group summary on ML methods</a></li>
</ul>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="setting-up-the-basic-equations-for-neural-networks">Setting up the basic equations for neural networks </h2>

<p>Neural networks, in its so-called feed-forward form, where each
iterations contains a feed-forward stage and a back-propgagation
stage, consist of series of affine matrix-matrix and matrix-vector
multiplications. The unknown parameters (the so-called biases and
weights which deternine the architecture of a neural network), are
uptaded iteratively using the so-called back-propagation algorithm.
This algorithm corresponds to the so-called reverse mode of the
automatic differentation algorithm. These algorithms will be discussed
in more detail below.
</p>

<p>We start however first with the  definitions of the various variables which make up a neural network.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="overarching-view-of-a-neural-network">Overarching view of a neural network </h2>

<p>The architecture of a neural network defines our model. This model
aims at describing some function \( f(\boldsymbol{x} \) which aims at describing
some final result (outputs or tagrget values) given a specific inpput
\( \boldsymbol{x} \). Note that here \( \boldsymbol{y} \) and \( \boldsymbol{x} \) are not limited to be
vectors.
</p>

<p>The architecture consists of</p>
<ol>
<li> An input and an output layer where the input layer is defined by the inputs \( \boldsymbol{x} \). The output layer produces the model ouput \( \boldsymbol{\tilde{y}} \) which is compared with the target value \( \boldsymbol{y} \)</li>
<li> A given number of hidden layers and neurons/nodes/units for each layer (this may vary)</li>
<li> A given activation function \( \sigma(\boldsymbol{z}) \) with arguments \( \boldsymbol{z} \) to be defined below. The activation functions may differ from layer to layer.</li>
<li> The last layer, normally called <b>output</b> layer has normally an activation function tailored to the specific problem</li>
<li> Finally we define a so-called cost or loss function which is used to gauge the quality of our model.</li> 
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="illustration-of-a-single-perceptron-model-and-a-multilayer-ffnn">Illustration of a single perceptron model and a multilayer FFNN </h2>

<center>  <!-- FIGURE -->
<hr class="figure">
<center>
<p class="caption">Figure 1:  In a) we show a single perceptron model while in b) we dispay a network with two  hidden layers, an input layer and an output layer. </p>
</center>
<p><img src="figures/nns.png" width="600" align="bottom"></p>
</center>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="the-optimization-problem">The optimization problem </h2>

<p>The cost function is a function of the unknown parameters
\( \boldsymbol{\Theta} \) where the latter is a container for all possible
parameters needed to define a neural network
</p>

<p>If we are dealing with a regression task a typical cost/loss function
is the mean squared error
</p>
$$
C(\boldsymbol{\Theta})=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right\}.
$$

<p>This function represents one of many possible ways to define
the so-called cost function.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="weights-and-biases">Weights and biases </h2>

<p>For neural networks the parameters
\( \boldsymbol{\Theta} \) are given by the so-called weights and biases (to be
defined below).
</p>

<p>The weights are given by matrix elements \( w_{ij}^{(l)} \) where the
superscript indicates the layer number. The biases are typically given
by vector elements representing each single node of a given layer,
that is \( b_j^{(l)} \).
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="other-ingredients-of-a-neural-network">Other ingredients of a neural network </h2>

<p>Having defined the architecture of a neural network, the optimization
of the cost function with respect to the parameters \( \boldsymbol{\Theta} \),
involves the calculations of gradients and their optimization. The
gradients represent the derivatives of a multidimensional object and
are often approximated by various gradient methods, including
</p>
<ol>
<li> various quasi-Newton methods,</li>
<li> plain gradient descent (GD) with a constant learning rate \( \eta \),</li>
<li> GD with momentum and other approximations to the learning rates such as</li>
<ul>
  <li> Adapative gradient (ADAgrad)</li>
  <li> Root mean-square propagation (RMSprop)</li>
  <li> Adaptive gradient with momentum (ADAM) and many other</li>
</ul>
<li> Stochastic gradient descent and various families of learning rate approximations</li>
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="other-parameters">Other parameters </h2>

<p>In addition to the above, there are often additional hyperparamaters
which are included in the setup of a neural network. These will be
discussed below.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="why-feed-forward-neural-networks-ffnn">Why Feed Forward Neural Networks (FFNN)?  </h2>

<p>According to the <em>Universal approximation theorem</em>, a feed-forward
neural network with just a single hidden layer containing a finite
number of neurons can approximate a continuous multidimensional
function to arbitrary accuracy, assuming the activation function for
the hidden layer is a <b>non-constant, bounded and
monotonically-increasing continuous function</b>.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="universal-approximation-theorem">Universal approximation theorem </h2>

<p>The universal approximation theorem plays a central role in deep
learning.  <a href="https://link.springer.com/article/10.1007/BF02551274" target="_blank">Cybenko (1989)</a> showed
the following:
</p>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>Let \( \sigma \) be any continuous sigmoidal function such that</p>
$$
\sigma(z) = \left\{\begin{array}{cc} 1 & z\rightarrow \infty\\ 0 & z \rightarrow -\infty \end{array}\right.
$$

<p>Given a continuous and deterministic function \( F(\boldsymbol{x}) \) on the unit
cube in \( d \)-dimensions \( F\in [0,1]^d \), \( x\in [0,1]^d \) and a parameter
\( \epsilon >0 \), there is a one-layer (hidden) neural network
\( f(\boldsymbol{x};\boldsymbol{\Theta}) \) with \( \boldsymbol{\Theta}=(\boldsymbol{W},\boldsymbol{b}) \) and \( \boldsymbol{W}\in
\mathbb{R}^{m\times n} \) and \( \boldsymbol{b}\in \mathbb{R}^{n} \), for which
</p>
$$
\vert F(\boldsymbol{x})-f(\boldsymbol{x};\boldsymbol{\Theta})\vert < \epsilon \hspace{0.1cm} \forall \boldsymbol{x}\in[0,1]^d.
$$
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="the-approximation-theorem-in-words">The approximation theorem in words </h2>

<p><b>Any continuous function \( y=F(\boldsymbol{x}) \) supported on the unit cube in
\( d \)-dimensions can be approximated by a one-layer sigmoidal network to
arbitrary accuracy.</b>
</p>

<p><a href="https://www.sciencedirect.com/science/article/abs/pii/089360809190009T" target="_blank">Hornik (1991)</a> extended the theorem by letting any non-constant, bounded activation function to be included using that the expectation value</p>
$$
\mathbb{E}[\vert F(\boldsymbol{x})\vert^2] =\int_{\boldsymbol{x}\in D} \vert F(\boldsymbol{x})\vert^2p(\boldsymbol{x})d\boldsymbol{x} < \infty.
$$

<p>Then we have</p>
$$
\mathbb{E}[\vert F(\boldsymbol{x})-f(\boldsymbol{x};\boldsymbol{\Theta})\vert^2] =\int_{\boldsymbol{x}\in D} \vert F(\boldsymbol{x})-f(\boldsymbol{x};\boldsymbol{\Theta})\vert^2p(\boldsymbol{x})d\boldsymbol{x} < \epsilon.
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="more-on-the-general-approximation-theorem">More on the general approximation theorem </h2>

<p>None of the proofs give any insight into the relation between the
number of of hidden layers and nodes and the approximation error
\( \epsilon \), nor the magnitudes of \( \boldsymbol{W} \) and \( \boldsymbol{b} \).
</p>

<p>Neural networks (NNs) have what we may call a kind of universality no matter what function we want to compute.</p>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>It does not mean that an NN can be used to exactly compute any function. Rather, we get an approximation that is as good as we want. </p>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="class-of-functions-we-can-approximate">Class of functions we can approximate </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>The class of functions that can be approximated are the continuous ones.
If the function \( F(\boldsymbol{x}) \) is discontinuous, it won't in general be possible to approximate it. However, an NN may still give an approximation even if we fail in some points.
</p>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="simple-example-fitting-nuclear-masses">Simple example, fitting nuclear masses </h2>

<p>See example at <a href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week34/ipynb/week34.ipynb" target="_blank"><tt>https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week34/ipynb/week34.ipynb</tt></a>, and scroll down to nuclear masses.</p>

<p>And the recent article <a href="https://www.sciencedirect.com/science/article/pii/S0375947423001100" target="_blank"><tt>https://www.sciencedirect.com/science/article/pii/S0375947423001100</tt></a></p>

<!-- ------------------- end of main content --------------- -->
<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2024, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</body>
</html>

