{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2643bf6e",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html streamline.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Mathematics of discriminative and generative deep learning, from deep neural networks to diffusion models -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c746a",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Mathematics of discriminative and generative deep learning, from deep neural networks to diffusion models\n",
    "**Morten Hjorth-Jensen**, Department of Physics and Center for Computing in Science Education, University of Oslo, Norway and Department of Physics and Astronomy and Facility for Rare Isotope Beams, Michigan State University, East Lansing, Michigan, USA\n",
    "\n",
    "Date: **STREAMLINE meeting, May 9-10, MSU, 2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e794a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Types of machine learning\n",
    "\n",
    "The approaches to machine learning are many, but are often split into two main categories. \n",
    "In *supervised learning* we know the answer to a problem,\n",
    "and let the computer deduce the logic behind it. On the other hand, *unsupervised learning*\n",
    "is a method for finding patterns and relationship in data sets without any prior knowledge of the system.\n",
    "\n",
    "An emerging  third category is  *reinforcement learning*. This is a paradigm \n",
    "of learning inspired by behavioural psychology, where learning is achieved by trial-and-error, \n",
    "solely from rewards and punishment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc97f6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Main categories\n",
    "Another way to categorize machine learning tasks is to consider the desired output of a system.\n",
    "Some of the most common tasks are:\n",
    "\n",
    "  * Classification: Outputs are divided into two or more classes. The goal is to   produce a model that assigns inputs into one of these classes. An example is to identify  digits based on pictures of hand-written ones. Classification is typically supervised learning.\n",
    "\n",
    "  * Regression: Finding a functional relationship between an input data set and a reference data set.   The goal is to construct a function that maps input data to continuous output values.\n",
    "\n",
    "  * Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.  It is thus a form of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b9da8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Machine learning. A simple perspective on the interface between ML and Physics\n",
    "\n",
    "<!-- dom:FIGURE: [figures/mlimage.png, width=800 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/mlimage.png\" width=\"800\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e703547",
   "metadata": {
    "editable": true
   },
   "source": [
    "## ML in Nuclear  Physics (or any field in physics)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/ML-NP.png, width=900 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/ML-NP.png\" width=\"900\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae2a02",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The plethora  of machine learning algorithms/methods\n",
    "\n",
    "1. Deep learning: Neural Networks (NN), Convolutional NN, Recurrent NN, Boltzmann machines, autoencoders and variational autoencoders  and generative adversarial networks, stable diffusion and many more generative models\n",
    "\n",
    "2. Bayesian statistics and Bayesian Machine Learning, Bayesian experimental design, Bayesian Regression models, Bayesian neural networks, Gaussian processes and much more\n",
    "\n",
    "3. Dimensionality reduction (Principal component analysis), Clustering Methods and more\n",
    "\n",
    "4. Ensemble Methods, Random forests, bagging and voting methods, gradient boosting approaches \n",
    "\n",
    "5. Linear and logistic regression, Kernel methods, support vector machines and more\n",
    "\n",
    "6. Reinforcement Learning; Transfer Learning and more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6f0a4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## What Is Generative Modeling?\n",
    "\n",
    "Generative modeling can be broadly defined as follows:\n",
    "\n",
    "Generative modeling is a branch of machine learning that involves\n",
    "training a model to produce new data that is similar to a given\n",
    "dataset.\n",
    "\n",
    "What does this mean in practice? Suppose we have a dataset containing\n",
    "photos of horses. We can train a generative model on this dataset to\n",
    "capture the rules that govern the complex relationships between pixels\n",
    "in images of horses. Then we can sample from this model to create\n",
    "novel, realistic images of horses that did not exist in the original\n",
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040eb3ab",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Example of generative modeling, [taken from Generative Deep Learning by David Foster](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/generativelearning.png, width=900 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/generativelearning.png\" width=\"900\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a147e3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Generative Modeling\n",
    "\n",
    "In order to build a generative model, we require a dataset consisting\n",
    "of many examples of the entity we are trying to generate. This is\n",
    "known as the training data, and one such data point is called an\n",
    "observation.\n",
    "\n",
    "Each observation consists of many features. For an image generation\n",
    "problem, the features are usually the individual pixel values; for a\n",
    "text generation problem, the features could be individual words or\n",
    "groups of letters. It is our goal to build a model that can generate\n",
    "new sets of features that look as if they have been created using the\n",
    "same rules as the original data. Conceptually, for image generation\n",
    "this is an incredibly difficult task, considering the vast number of\n",
    "ways that individual pixel values can be assigned and the relatively\n",
    "tiny number of such arrangements that constitute an image of the\n",
    "entity we are trying to generate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac112d5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Generative Versus Discriminative Modeling\n",
    "\n",
    "In order to truly understand what generative modeling aims to achieve\n",
    "and why this is important, it is useful to compare it to its\n",
    "counterpart, discriminative modeling. If you have studied machine\n",
    "learning, most problems you will have faced will have most likely been\n",
    "discriminative in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6130e16",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Example of discriminative modeling, [taken from Generative Deeep Learning by David Foster](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/standarddeeplearning.png, width=900 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/standarddeeplearning.png\" width=\"900\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ae2b5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Discriminative Modeling\n",
    "\n",
    "When performing discriminative modeling, each observation in the\n",
    "training data has a label. For a binary classification problem such as\n",
    "our data could be labeled as ones and zeros. Our model then learns how to\n",
    "discriminate between these two groups and outputs the probability that\n",
    "a new observation has label 1 or 0\n",
    "\n",
    "In contrast, generative modeling doesnâ€™t require the dataset to be\n",
    "labeled because it concerns itself with generating entirely new\n",
    "data (for example an image), rather than trying to predict a label for say  a given image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953317ed",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Taxonomy of generative deep learning, [taken from Generative Deep Learning by David Foster](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/generativemodels.png, width=900 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/generativemodels.png\" width=\"900\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52045ad",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Good books with hands-on material and codes\n",
    "* [Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch](https://sebastianraschka.com/blog/2022/ml-pytorch-book.html)\n",
    "\n",
    "* [David Foster, Generative Deep Learning with TensorFlow](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html)\n",
    "\n",
    "* [Babcock and Gavras, Generative AI with Python and TensorFlow 2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2)\n",
    "\n",
    "All three books have GitHub sites from where  one can download all codes. A good and more general text (2016)\n",
    "is Goodfellow, Bengio and Courville, [Deep Learning](https://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7fde04",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More references\n",
    "\n",
    "**Reading on diffusion models.**\n",
    "\n",
    "1. A central paper is the one by Sohl-Dickstein et al, Deep Unsupervised Learning using Nonequilibrium Thermodynamics, <https://arxiv.org/abs/1503.03585>\n",
    "\n",
    "2. See also Diederik P. Kingma, Tim Salimans, Ben Poole, Jonathan Ho, Variational Diffusion Models, <https://arxiv.org/abs/2107.00630>\n",
    "\n",
    "   \n",
    "\n",
    "**and VAEs.**\n",
    "\n",
    "1. Calvin Luo <https://calvinyluo.com/2022/08/26/diffusion-tutorial.html>\n",
    "\n",
    "2. An Introduction to Variational Autoencoders, by Kingma and Welling, see <https://arxiv.org/abs/1906.02691>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417892be",
   "metadata": {
    "editable": true
   },
   "source": [
    "## What are the basic Machine Learning ingredients?\n",
    "Almost every problem in ML and data science starts with the same ingredients:\n",
    "* The dataset $\\boldsymbol{x}$ (could be some observable quantity of the system we are studying)\n",
    "\n",
    "* A model which is a function of a set of parameters $\\boldsymbol{\\alpha}$ that relates to the dataset, say a likelihood  function $p(\\boldsymbol{x}\\vert \\boldsymbol{\\alpha})$ or just a simple model $f(\\boldsymbol{\\alpha})$\n",
    "\n",
    "* A so-called **loss/cost/risk** function $\\mathcal{C} (\\boldsymbol{x}, f(\\boldsymbol{\\alpha}))$ which allows us to decide how well our model represents the dataset. \n",
    "\n",
    "We seek to minimize the function $\\mathcal{C} (\\boldsymbol{x}, f(\\boldsymbol{\\alpha}))$ by finding the parameter values which minimize $\\mathcal{C}$. This leads to  various minimization algorithms. It may surprise many, but at the heart of all machine learning algortihms there is an optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040418d2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Low-level machine learning, the family of ordinary least squares methods\n",
    "\n",
    "Our data which we want to apply a machine learning method on, consist\n",
    "of a set of inputs $\\boldsymbol{x}^T=[x_0,x_1,x_2,\\dots,x_{n-1}]$ and the\n",
    "outputs we want to model $\\boldsymbol{y}^T=[y_0,y_1,y_2,\\dots,y_{n-1}]$.\n",
    "We assume  that the output data can be represented (for a regression case) by a continuous function $f$\n",
    "through"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d899f541",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{y}=f(\\boldsymbol{x})+\\boldsymbol{\\epsilon}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b841cf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Setting up the equations\n",
    "\n",
    "In linear regression we approximate the unknown function with another\n",
    "continuous function $\\tilde{\\boldsymbol{y}}(\\boldsymbol{x})$ which depends linearly on\n",
    "some unknown parameters\n",
    "$\\boldsymbol{\\theta}^T=[\\theta_0,\\theta_1,\\theta_2,\\dots,\\theta_{p-1}]$.\n",
    "\n",
    "The input data can be organized in terms of a so-called design matrix \n",
    "with an approximating function $\\boldsymbol{\\tilde{y}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6b804",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\tilde{y}}= \\boldsymbol{X}\\boldsymbol{\\theta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62a187",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The objective/cost/loss function\n",
    "\n",
    "The  simplest approach is the mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e49cce1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\Theta})=\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(y_i-\\tilde{y}_i\\right)^2=\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}}\\right)\\right\\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4b108",
   "metadata": {
    "editable": true
   },
   "source": [
    "or using the matrix $\\boldsymbol{X}$ and in a more compact matrix-vector notation as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bccf34",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\Theta})=\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee35f8b",
   "metadata": {
    "editable": true
   },
   "source": [
    "This function represents one of many possible ways to define the so-called cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdaff11",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Training solution\n",
    "\n",
    "Optimizing with respect to the unknown parameters $\\theta_j$ we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77988762",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{y} = \\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1479ed4",
   "metadata": {
    "editable": true
   },
   "source": [
    "and if the matrix $\\boldsymbol{X}^T\\boldsymbol{X}$ is invertible we have the optimal values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdfec4a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}} =\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d318564",
   "metadata": {
    "editable": true
   },
   "source": [
    "We say we 'learn' the unknown parameters $\\boldsymbol{\\theta}$ from the last equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6a0f08",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Ridge and LASSO Regression\n",
    "\n",
    "Our optimization problem is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04febf65",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in {\\mathbb{R}}^{p}}}\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8686967f",
   "metadata": {
    "editable": true
   },
   "source": [
    "or we can state it as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf3ef8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(y_i-\\tilde{y}_i\\right)^2=\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f969c45",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have used the definition of  a norm-2 vector, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb4089",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert\\vert \\boldsymbol{x}\\vert\\vert_2 = \\sqrt{\\sum_i x_i^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626f241",
   "metadata": {
    "editable": true
   },
   "source": [
    "## From OLS to Ridge and Lasso\n",
    "\n",
    "By minimizing the above equation with respect to the parameters\n",
    "$\\boldsymbol{\\theta}$ we could then obtain an analytical expression for the\n",
    "parameters $\\boldsymbol{\\theta}$.  We can add a regularization parameter $\\lambda$ by\n",
    "defining a new cost function to be optimized, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e0007",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65344b",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to the Ridge regression minimization problem where we\n",
    "require that $\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_2^2\\le t$, where $t$ is\n",
    "a finite number larger than zero. We do not include such a constraints in the discussions here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51bd4b4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Lasso regression\n",
    "\n",
    "Defining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf314e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\theta})=\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc879a",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have a new optimization equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ad781",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c1f4c1",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to Lasso regression. Lasso stands for least absolute shrinkage and selection operator. \n",
    "Here we have defined the norm-1 as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2218dd92",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert\\vert \\boldsymbol{x}\\vert\\vert_1 = \\sum_i \\vert x_i\\vert.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf16afd2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Selected references\n",
    "* [Mehta et al.](https://arxiv.org/abs/1803.08823) and [Physics Reports (2019)](https://www.sciencedirect.com/science/article/pii/S0370157319300766?via%3Dihub).\n",
    "\n",
    "* [Machine Learning and the Physical Sciences by Carleo et al](https://link.aps.org/doi/10.1103/RevModPhys.91.045002)\n",
    "\n",
    "* [Artificial Intelligence and Machine Learning in Nuclear Physics, Amber Boehnlein et al., Reviews Modern of Physics 94, 031003 (2022)](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.94.031003) \n",
    "\n",
    "* [Dilute neutron star matter from neural-network quantum states by Fore et al, Physical Review Research 5, 033062 (2023)](https://journals.aps.org/prresearch/pdf/10.1103/PhysRevResearch.5.033062)\n",
    "\n",
    "* Neural-network quantum states for ultra-cold Fermi gases, Jane Kim et al, Nature Physics Communcication, in press, see <https://doi.org/10.48550/arXiv.2305.08831>\n",
    "\n",
    "* [Message-Passing Neural Quantum States for the Homogeneous Electron Gas, Gabriel Pescia, Jane Kim et al. arXiv.2305.07240,](https://doi.org/10.48550/arXiv.2305.07240)\n",
    "\n",
    "* [Particle Data Group summary on ML methods](https://pdg.lbl.gov/2021/reviews/rpp2021-rev-machine-learning.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6af6fd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Setting up the basic equations for neural networks\n",
    "\n",
    "Neural networks, in its so-called feed-forward form, where each\n",
    "iterations contains a feed-forward stage and a back-propgagation\n",
    "stage, consist of series of affine matrix-matrix and matrix-vector\n",
    "multiplications. The unknown parameters (the so-called biases and\n",
    "weights which deternine the architecture of a neural network), are\n",
    "uptaded iteratively using the so-called back-propagation algorithm.\n",
    "This algorithm corresponds to the so-called reverse mode of the\n",
    "automatic differentation algorithm. These algorithms will be discussed\n",
    "in more detail below.\n",
    "\n",
    "We start however first with the  definitions of the various variables which make up a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a0be0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Overarching view of a neural network\n",
    "\n",
    "The architecture of a neural network defines our model. This model\n",
    "aims at describing some function $f(\\boldsymbol{x}$ which aims at describing\n",
    "some final result (outputs or tagrget values) given a specific inpput\n",
    "$\\boldsymbol{x}$. Note that here $\\boldsymbol{y}$ and $\\boldsymbol{x}$ are not limited to be\n",
    "vectors.\n",
    "\n",
    "The architecture consists of\n",
    "1. An input and an output layer where the input layer is defined by the inputs $\\boldsymbol{x}$. The output layer produces the model ouput $\\boldsymbol{\\tilde{y}}$ which is compared with the target value $\\boldsymbol{y}$\n",
    "\n",
    "2. A given number of hidden layers and neurons/nodes/units for each layer (this may vary)\n",
    "\n",
    "3. A given activation function $\\sigma(\\boldsymbol{z})$ with arguments $\\boldsymbol{z}$ to be defined below. The activation functions may differ from layer to layer.\n",
    "\n",
    "4. The last layer, normally called **output** layer has normally an activation function tailored to the specific problem\n",
    "\n",
    "5. Finally we define a so-called cost or loss function which is used to gauge the quality of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfc1eb",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Illustration of a single perceptron model and a multilayer FFNN\n",
    "\n",
    "<!-- dom:FIGURE: [figures/nns.png, width=600 frac=0.7]  In a) we show a single perceptron model while in b) we dispay a network with two  hidden layers, an input layer and an output layer. -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/nns.png\" width=\"600\"><p style=\"font-size: 0.9em\"><i>Figure 1: In a) we show a single perceptron model while in b) we dispay a network with two  hidden layers, an input layer and an output layer.</i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1844f82f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The optimization problem\n",
    "\n",
    "The cost function is a function of the unknown parameters\n",
    "$\\boldsymbol{\\Theta}$ where the latter is a container for all possible\n",
    "parameters needed to define a neural network\n",
    "\n",
    "If we are dealing with a regression task a typical cost/loss function\n",
    "is the mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0ae9c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\Theta})=\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2af33",
   "metadata": {
    "editable": true
   },
   "source": [
    "This function represents one of many possible ways to define\n",
    "the so-called cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4a192",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Weights and biases\n",
    "\n",
    "For neural networks the parameters\n",
    "$\\boldsymbol{\\Theta}$ are given by the so-called weights and biases (to be\n",
    "defined below).\n",
    "\n",
    "The weights are given by matrix elements $w_{ij}^{(l)}$ where the\n",
    "superscript indicates the layer number. The biases are typically given\n",
    "by vector elements representing each single node of a given layer,\n",
    "that is $b_j^{(l)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7788e39",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Other ingredients of a neural network\n",
    "\n",
    "Having defined the architecture of a neural network, the optimization\n",
    "of the cost function with respect to the parameters $\\boldsymbol{\\Theta}$,\n",
    "involves the calculations of gradients and their optimization. The\n",
    "gradients represent the derivatives of a multidimensional object and\n",
    "are often approximated by various gradient methods, including\n",
    "1. various quasi-Newton methods,\n",
    "\n",
    "2. plain gradient descent (GD) with a constant learning rate $\\eta$,\n",
    "\n",
    "3. GD with momentum and other approximations to the learning rates such as\n",
    "\n",
    "  * Adapative gradient (ADAgrad)\n",
    "\n",
    "  * Root mean-square propagation (RMSprop)\n",
    "\n",
    "  * Adaptive gradient with momentum (ADAM) and many other\n",
    "\n",
    "4. Stochastic gradient descent and various families of learning rate approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73271485",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Other parameters\n",
    "\n",
    "In addition to the above, there are often additional hyperparamaters\n",
    "which are included in the setup of a neural network. These will be\n",
    "discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0529fee9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Why Feed Forward Neural Networks (FFNN)?\n",
    "\n",
    "According to the *Universal approximation theorem*, a feed-forward\n",
    "neural network with just a single hidden layer containing a finite\n",
    "number of neurons can approximate a continuous multidimensional\n",
    "function to arbitrary accuracy, assuming the activation function for\n",
    "the hidden layer is a **non-constant, bounded and\n",
    "monotonically-increasing continuous function**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff6585",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Universal approximation theorem\n",
    "\n",
    "The universal approximation theorem plays a central role in deep\n",
    "learning.  [Cybenko (1989)](https://link.springer.com/article/10.1007/BF02551274) showed\n",
    "the following:\n",
    "\n",
    "Let $\\sigma$ be any continuous sigmoidal function such that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5ccdd",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\sigma(z) = \\left\\{\\begin{array}{cc} 1 & z\\rightarrow \\infty\\\\ 0 & z \\rightarrow -\\infty \\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c53d0",
   "metadata": {
    "editable": true
   },
   "source": [
    "Given a continuous and deterministic function $F(\\boldsymbol{x})$ on the unit\n",
    "cube in $d$-dimensions $F\\in [0,1]^d$, $x\\in [0,1]^d$ and a parameter\n",
    "$\\epsilon >0$, there is a one-layer (hidden) neural network\n",
    "$f(\\boldsymbol{x};\\boldsymbol{\\Theta})$ with $\\boldsymbol{\\Theta}=(\\boldsymbol{W},\\boldsymbol{b})$ and $\\boldsymbol{W}\\in\n",
    "\\mathbb{R}^{m\\times n}$ and $\\boldsymbol{b}\\in \\mathbb{R}^{n}$, for which"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa92f38",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert F(\\boldsymbol{x})-f(\\boldsymbol{x};\\boldsymbol{\\Theta})\\vert < \\epsilon \\hspace{0.1cm} \\forall \\boldsymbol{x}\\in[0,1]^d.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b5b9c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The approximation theorem in words\n",
    "\n",
    "**Any continuous function $y=F(\\boldsymbol{x})$ supported on the unit cube in\n",
    "$d$-dimensions can be approximated by a one-layer sigmoidal network to\n",
    "arbitrary accuracy.**\n",
    "\n",
    "[Hornik (1991)](https://www.sciencedirect.com/science/article/abs/pii/089360809190009T) extended the theorem by letting any non-constant, bounded activation function to be included using that the expectation value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f105e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbb{E}[\\vert F(\\boldsymbol{x})\\vert^2] =\\int_{\\boldsymbol{x}\\in D} \\vert F(\\boldsymbol{x})\\vert^2p(\\boldsymbol{x})d\\boldsymbol{x} < \\infty.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543e72b",
   "metadata": {
    "editable": true
   },
   "source": [
    "Then we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb4d6b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbb{E}[\\vert F(\\boldsymbol{x})-f(\\boldsymbol{x};\\boldsymbol{\\Theta})\\vert^2] =\\int_{\\boldsymbol{x}\\in D} \\vert F(\\boldsymbol{x})-f(\\boldsymbol{x};\\boldsymbol{\\Theta})\\vert^2p(\\boldsymbol{x})d\\boldsymbol{x} < \\epsilon.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08246c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More on the general approximation theorem\n",
    "\n",
    "None of the proofs give any insight into the relation between the\n",
    "number of of hidden layers and nodes and the approximation error\n",
    "$\\epsilon$, nor the magnitudes of $\\boldsymbol{W}$ and $\\boldsymbol{b}$.\n",
    "\n",
    "Neural networks (NNs) have what we may call a kind of universality no matter what function we want to compute.\n",
    "\n",
    "It does not mean that an NN can be used to exactly compute any function. Rather, we get an approximation that is as good as we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec26e5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Class of functions we can approximate\n",
    "\n",
    "The class of functions that can be approximated are the continuous ones.\n",
    "If the function $F(\\boldsymbol{x})$ is discontinuous, it won't in general be possible to approximate it. However, an NN may still give an approximation even if we fail in some points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff01fe19",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple example, fitting nuclear masses\n",
    "\n",
    "See example at <https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week34/ipynb/week34.ipynb>, and scroll down to nuclear masses.\n",
    "\n",
    "And the recent article <https://www.sciencedirect.com/science/article/pii/S0375947423001100>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f7589",
   "metadata": {
    "editable": true
   },
   "source": [
    "## First network example, simple percepetron with one input\n",
    "\n",
    "As yet another example we define now a simple perceptron model with\n",
    "all quantities given by scalars. We consider only one input variable\n",
    "$x$ and one target value $y$.  We define an activation function\n",
    "$\\sigma_1$ which takes as input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38441fb6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "z_1 = w_1x+b_1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf8d98",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $w_1$ is the weight and $b_1$ is the bias. These are the\n",
    "parameters we want to optimize.  The output is $a_1=\\sigma(z_1)$ (see\n",
    "graph from whiteboard notes). This output is then fed into the\n",
    "**cost/loss** function, which we here for the sake of simplicity just\n",
    "define as the squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f8b91",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(x;w_1,b_1)=\\frac{1}{2}(a_1-y)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b49752",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Optimizing the parameters\n",
    "\n",
    "In setting up the feed forward and back propagation parts of the\n",
    "algorithm, we need now the derivative of the various variables we want\n",
    "to train.\n",
    "\n",
    "We need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81205ab3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial w_1} \\hspace{0.1cm}\\mathrm{and}\\hspace{0.1cm}\\frac{\\partial C}{\\partial b_1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038bd72",
   "metadata": {
    "editable": true
   },
   "source": [
    "Using the chain rule we find"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d0b738",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial w_1}=\\frac{\\partial C}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1}\\frac{\\partial z_1}{\\partial w_1}=(a_1-y)\\sigma_1'x,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b34ee6d",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181e53e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial b_1}=\\frac{\\partial C}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1}\\frac{\\partial z_1}{\\partial b_1}=(a_1-y)\\sigma_1',\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df75a4",
   "metadata": {
    "editable": true
   },
   "source": [
    "which we later will just define as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96ed39",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1}=\\delta_1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b4714",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Implementing the simple perceptron model\n",
    "\n",
    "In the example code here we implement the above equations (with explict\n",
    "expressions for the derivatives) with just one input variable $x$ and\n",
    "one output variable.  The target value $y=2x+1$ is a simple linear\n",
    "function in $x$. Since this is a regression problem, we define the cost function to be proportional to the least squares error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9fc4cd",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(y,w_1,b_1)=\\frac{1}{2}(a_1-y)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f5c5e",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $a_1$ the output from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fbbc649",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def feed_forward(x):\n",
    "    # weighted sum of inputs to the output layer\n",
    "    z_1 = x*output_weights + output_bias\n",
    "    # Output from output node (one node only)\n",
    "    # Here the output is equal to the input\n",
    "    a_1 = z_1\n",
    "    return a_1\n",
    "\n",
    "def backpropagation(x, y):\n",
    "    a_1 = feed_forward(x)\n",
    "    # derivative of cost function\n",
    "    derivative_cost = a_1 - y\n",
    "    # the variable delta in the equations, note that output a_1 = z_1, its derivatives wrt z_o is thus 1\n",
    "    delta_1 = derivative_cost\n",
    "    # gradients for the output layer\n",
    "    output_weights_gradient = delta_1*x\n",
    "    output_bias_gradient = delta_1\n",
    "    # The cost function is 0.5*(a_1-y)^2. This gives a measure of the error for each iteration\n",
    "    return output_weights_gradient, output_bias_gradient\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "# Input variable\n",
    "x = 4.0\n",
    "# Target values\n",
    "y = 2*x+1.0\n",
    "\n",
    "# Defining the neural network\n",
    "n_inputs = 1\n",
    "n_outputs = 1\n",
    "# Initialize the network\n",
    "# weights and bias in the output layer\n",
    "output_weights = np.random.randn()\n",
    "output_bias = np.random.randn()\n",
    "\n",
    "# implementing a simple gradient descent approach with fixed learning rate\n",
    "eta = 0.01\n",
    "for i in range(40):\n",
    "    # calculate gradients from back propagation\n",
    "    derivative_w1, derivative_b1 = backpropagation(x, y)\n",
    "    # update weights and biases\n",
    "    output_weights -= eta * derivative_w1\n",
    "    output_bias -= eta * derivative_b1\n",
    "# our final prediction after training\n",
    "ytilde = output_weights*x+output_bias\n",
    "print(0.5*((ytilde-y)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d16fb87",
   "metadata": {
    "editable": true
   },
   "source": [
    "Running this code gives us an acceptable results after some 40-50 iterations. Note that the results depend on the value of the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4fa156",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 1: Extensions to the above code\n",
    "\n",
    "Feel free to add more input nodes and weights to the above\n",
    "code. Furthermore, try to increase the amount of input and\n",
    "target/output data. Try also to perform calculations for more values\n",
    "of the learning rates. Feel free to add either hyperparameters with an\n",
    "$l_1$ norm or an $l_2$ norm and discuss your results.\n",
    "\n",
    "You could also try to change the function $f(x)=y$ from a linear polynomial in $x$ to a higher-order polynomial.\n",
    "Comment your results.\n",
    "\n",
    "**Hint**: Increasing the number of input variables and input nodes requires a rewrite of the input data in terms of a matrix. You need to figure out the correct dimensionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b5f7d9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Adding a hidden layer\n",
    "\n",
    "We change our simple model to (see graph below)\n",
    "a network with just one hidden layer but with scalar variables only.\n",
    "\n",
    "Our output variable changes to $a_2$ and $a_1$ is now the output from the hidden node and $a_0=x$.\n",
    "We have then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e43cf",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "z_1 = w_1a_0+b_1 \\hspace{0.1cm} \\wedge a_1 = \\sigma_1(z_1),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913d329",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "z_2 = w_2a_1+b_2 \\hspace{0.1cm} \\wedge a_2 = \\sigma_2(z_2),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e8c23",
   "metadata": {
    "editable": true
   },
   "source": [
    "and the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcedebf",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(x;\\boldsymbol{\\Theta})=\\frac{1}{2}(a_2-y)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c64d0",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\boldsymbol{\\Theta}=[w_1,w_2,b_1,b_2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611e936",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The derivatives\n",
    "\n",
    "The derivatives are now, using the chain rule again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083cf552",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial w_2}=\\frac{\\partial C}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_2}\\frac{\\partial z_2}{\\partial w_2}=(a_2-y)\\sigma_2'a_1=\\delta_2a_1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94c81d7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial b_2}=\\frac{\\partial C}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_2}\\frac{\\partial z_2}{\\partial b_2}=(a_2-y)\\sigma_2'=\\delta_2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b80aa",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial w_1}=\\frac{\\partial C}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_2}\\frac{\\partial z_2}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1}\\frac{\\partial z_1}{\\partial w_1}=(a_2-y)\\sigma_2'a_1\\sigma_1'a_0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d02661",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial b_1}=\\frac{\\partial C}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_2}\\frac{\\partial z_2}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1}\\frac{\\partial z_1}{\\partial b_1}=(a_2-y)\\sigma_2'\\sigma_1'=\\delta_1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb3a98",
   "metadata": {
    "editable": true
   },
   "source": [
    "Can you generalize this to more than one hidden layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ffe65",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Important observations\n",
    "\n",
    "From the above equations we see that the derivatives of the activation\n",
    "functions play a central role. If they vanish, the training may\n",
    "stop. This is called the vanishing gradient problem, see discussions below. If they become\n",
    "large, the parameters $w_i$ and $b_i$ may simply go to infinity. This\n",
    "is referenced as  the exploding gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f4c3a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The training\n",
    "\n",
    "The training of the parameters is done through various gradient descent approximations with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c5b4a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "w_{i}\\leftarrow w_{i}- \\eta \\delta_i a_{i-1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d50dc",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e01d8d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "b_i \\leftarrow b_i-\\eta \\delta_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c81cd",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\eta$ is the learning rate.\n",
    "\n",
    "One iteration consists of one feed forward step and one back-propagation step. Each back-propagation step does one update of the parameters $\\boldsymbol{\\Theta}$.\n",
    "\n",
    "For the first hidden layer $a_{i-1}=a_0=x$ for this simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7dd5b2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Code example\n",
    "\n",
    "The code here implements the above model with one hidden layer and\n",
    "scalar variables for the same function we studied in the previous\n",
    "example.  The code is however set up so that we can add multiple\n",
    "inputs $x$ and target values $y$. Note also that we have the\n",
    "possibility of defining a feature matrix $\\boldsymbol{X}$ with more than just\n",
    "one column for the input values. This will turn useful in our next example. We have also defined matrices and vectors for all of our operations although it is not necessary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded3017e",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# We use the Sigmoid function as activation function\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def forwardpropagation(x):\n",
    "    # weighted sum of inputs to the hidden layer\n",
    "    z_1 = np.matmul(x, w_1) + b_1\n",
    "    # activation in the hidden layer\n",
    "    a_1 = sigmoid(z_1)\n",
    "    # weighted sum of inputs to the output layer\n",
    "    z_2 = np.matmul(a_1, w_2) + b_2\n",
    "    a_2 = z_2\n",
    "    return a_1, a_2\n",
    "\n",
    "def backpropagation(x, y):\n",
    "    a_1, a_2 = forwardpropagation(x)\n",
    "    # parameter delta for the output layer, note that a_2=z_2 and its derivative wrt z_2 is just 1\n",
    "    delta_2 = a_2 - y\n",
    "    print(0.5*((a_2-y)**2))\n",
    "    # delta for  the hidden layer\n",
    "    delta_1 = np.matmul(delta_2, w_2.T) * a_1 * (1 - a_1)\n",
    "    # gradients for the output layer\n",
    "    output_weights_gradient = np.matmul(a_1.T, delta_2)\n",
    "    output_bias_gradient = np.sum(delta_2, axis=0)\n",
    "    # gradient for the hidden layer\n",
    "    hidden_weights_gradient = np.matmul(x.T, delta_1)\n",
    "    hidden_bias_gradient = np.sum(delta_1, axis=0)\n",
    "    return output_weights_gradient, output_bias_gradient, hidden_weights_gradient, hidden_bias_gradient\n",
    "\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "# Input variable\n",
    "x = np.array([4.0],dtype=np.float64)\n",
    "# Target values\n",
    "y = 2*x+1.0 \n",
    "\n",
    "# Defining the neural network, only scalars here\n",
    "n_inputs = x.shape\n",
    "n_features = 1\n",
    "n_hidden_neurons = 1\n",
    "n_outputs = 1\n",
    "\n",
    "# Initialize the network\n",
    "# weights and bias in the hidden layer\n",
    "w_1 = np.random.randn(n_features, n_hidden_neurons)\n",
    "b_1 = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "# weights and bias in the output layer\n",
    "w_2 = np.random.randn(n_hidden_neurons, n_outputs)\n",
    "b_2 = np.zeros(n_outputs) + 0.01\n",
    "\n",
    "eta = 0.1\n",
    "for i in range(50):\n",
    "    # calculate gradients\n",
    "    derivW2, derivB2, derivW1, derivB1 = backpropagation(x, y)\n",
    "    # update weights and biases\n",
    "    w_2 -= eta * derivW2\n",
    "    b_2 -= eta * derivB2\n",
    "    w_1 -= eta * derivW1\n",
    "    b_1 -= eta * derivB1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a16adb",
   "metadata": {
    "editable": true
   },
   "source": [
    "We see that after some few iterations (the results do depend on the learning rate however), we get an error which is rather small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716d150",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2: Including more data\n",
    "\n",
    "Try to increase the amount of input and\n",
    "target/output data. Try also to perform calculations for more values\n",
    "of the learning rates. Feel free to add either hyperparameters with an\n",
    "$l_1$ norm or an $l_2$ norm and discuss your results.\n",
    "Discuss your results as functions of the amount of training data and various learning rates.\n",
    "\n",
    "**Challenge:** Try to change the activation functions and replace the hard-coded analytical expressions with automatic derivation via either **autograd** or **JAX**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d200f7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple neural network and the  back propagation equations\n",
    "\n",
    "Let us now try to increase our level of ambition and attempt at setting \n",
    "up the equations for a neural network with two input nodes, one hidden\n",
    "layer with two hidden nodes and one output layer with one output node/neuron only (see graph)..\n",
    "\n",
    "We need to define the following parameters and variables with the input layer (layer $(0)$) \n",
    "where we label the  nodes $x_0$ and $x_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd43e21",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "x_0 = a_0^{(0)} \\wedge x_1 = a_1^{(0)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff034c",
   "metadata": {
    "editable": true
   },
   "source": [
    "The  hidden layer (layer $(1)$) has  nodes which yield the outputs $a_0^{(1)}$ and $a_1^{(1)}$) with  weight $\\boldsymbol{w}$ and bias $\\boldsymbol{b}$ parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e21c2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "w_{ij}^{(1)}=\\left\\{w_{00}^{(1)},w_{01}^{(1)},w_{10}^{(1)},w_{11}^{(1)}\\right\\} \\wedge b^{(1)}=\\left\\{b_0^{(1)},b_1^{(1)}\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdcd212",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The ouput layer\n",
    "\n",
    "Finally, we have the ouput layer given by layer label $(2)$ with output $a^{(2)}$ and weights and biases to be determined given by the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98daadfa",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "w_{i}^{(2)}=\\left\\{w_{0}^{(2)},w_{1}^{(2)}\\right\\} \\wedge b^{(2)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d5669",
   "metadata": {
    "editable": true
   },
   "source": [
    "Our output is $\\tilde{y}=a^{(2)}$ and we define a generic cost function $C(a^{(2)},y;\\boldsymbol{\\Theta})$ where $y$ is the target value (a scalar here).\n",
    "The parameters we need to optimize are given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b9e17",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\Theta}=\\left\\{w_{00}^{(1)},w_{01}^{(1)},w_{10}^{(1)},w_{11}^{(1)},w_{0}^{(2)},w_{1}^{(2)},b_0^{(1)},b_1^{(1)},b^{(2)}\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02367fd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Compact expressions\n",
    "\n",
    "We can define the inputs to the activation functions for the various layers in terms of various matrix-vector multiplications and vector additions.\n",
    "The inputs to the first hidden layer are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8149d016",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{bmatrix}z_0^{(1)} \\\\ z_1^{(1)} \\end{bmatrix}=\\begin{bmatrix}w_{00}^{(1)} & w_{01}^{(1)}\\\\ w_{10}^{(1)} &w_{11}^{(1)} \\end{bmatrix}\\begin{bmatrix}a_0^{(0)} \\\\ a_1^{(0)} \\end{bmatrix}+\\begin{bmatrix}b_0^{(1)} \\\\ b_1^{(1)} \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a809ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "with outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d192685",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{bmatrix}a_0^{(1)} \\\\ a_1^{(1)} \\end{bmatrix}=\\begin{bmatrix}\\sigma^{(1)}(z_0^{(1)}) \\\\ \\sigma^{(1)}(z_1^{(1)}) \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8df6a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Output layer\n",
    "\n",
    "For the final output layer we have the inputs to the final activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38504854",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "z^{(2)} = w_{0}^{(2)}a_0^{(1)} +w_{1}^{(2)}a_1^{(1)}+b^{(2)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccfb9b6",
   "metadata": {
    "editable": true
   },
   "source": [
    "resulting in the  output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81330ff4",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "a^{(2)}=\\sigma^{(2)}(z^{(2)}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f1e9d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Explicit derivatives\n",
    "\n",
    "In total we have nine parameters which we need to train.  Using the\n",
    "chain rule (or just the back-propagation algorithm) we can find all\n",
    "derivatives. Since we will use automatic differentiation in reverse\n",
    "mode, we start with the derivatives of the cost function with respect\n",
    "to the parameters of the output layer, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ab36a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial w_{i}^{(2)}}=\\frac{\\partial C}{\\partial a^{(2)}}\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\frac{\\partial z^{(2)}}{\\partial w_{i}^{(2)}}=\\delta^{(2)}a_i^{(1)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87707e4",
   "metadata": {
    "editable": true
   },
   "source": [
    "with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1b4980",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\delta^{(2)}=\\frac{\\partial C}{\\partial a^{(2)}}\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c470ac7",
   "metadata": {
    "editable": true
   },
   "source": [
    "and finally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c736fc2a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial b^{(2)}}=\\frac{\\partial C}{\\partial a^{(2)}}\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\frac{\\partial z^{(2)}}{\\partial b^{(2)}}=\\delta^{(2)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db37d62",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Derivatives of the hidden layer\n",
    "\n",
    "Using the chain rule we have the following expressions for say one of the weight parameters (it is easy to generalize to the other weight parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f535b0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial w_{00}^{(1)}}=\\frac{\\partial C}{\\partial a^{(2)}}\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\n",
    "\\frac{\\partial z^{(2)}}{\\partial z_0^{(1)}}\\frac{\\partial z_0^{(1)}}{\\partial w_{00}^{(1)}}=    \\delta^{(2)}\\frac{\\partial z^{(2)}}{\\partial z_0^{(1)}}\\frac{\\partial z_0^{(1)}}{\\partial w_{00}^{(1)}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36910ad1",
   "metadata": {
    "editable": true
   },
   "source": [
    "which, noting that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a0347",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "z^{(2)} =w_0^{(2)}a_0^{(1)}+w_1^{(2)}a_1^{(1)}+b^{(2)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0af45",
   "metadata": {
    "editable": true
   },
   "source": [
    "allows us to rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bc69b8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial z^{(2)}}{\\partial z_0^{(1)}}\\frac{\\partial z_0^{(1)}}{\\partial w_{00}^{(1)}}=w_0^{(2)}\\frac{\\partial a_0^{(1)}}{\\partial z_0^{(1)}}a_0^{(1)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb2481",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Final expression\n",
    "Defining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bce440",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\delta_0^{(1)}=w_0^{(2)}\\frac{\\partial a_0^{(1)}}{\\partial z_0^{(1)}}\\delta^{(2)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be87f9",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed07044",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial w_{00}^{(1)}}=\\delta_0^{(1)}a_0^{(1)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9cde5",
   "metadata": {
    "editable": true
   },
   "source": [
    "Similarly, we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be752f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial w_{01}^{(1)}}=\\delta_0^{(1)}a_1^{(1)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645728eb",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Completing the list\n",
    "\n",
    "Similarly, we find"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba9c1b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial w_{10}^{(1)}}=\\delta_1^{(1)}a_0^{(1)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8153321",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c2df00",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial w_{11}^{(1)}}=\\delta_1^{(1)}a_1^{(1)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a249239c",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ef6342",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\delta_1^{(1)}=w_1^{(2)}\\frac{\\partial a_1^{(1)}}{\\partial z_1^{(1)}}\\delta^{(2)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e62bd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Final expressions for the biases of the hidden layer\n",
    "\n",
    "For the sake of completeness, we list the derivatives of the biases, which are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c0e7b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial b_{0}^{(1)}}=\\delta_0^{(1)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c931fde",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87bd310",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial b_{1}^{(1)}}=\\delta_1^{(1)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b801b",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we will see below, these expressions can be generalized in a more compact form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ea371",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gradient expressions\n",
    "\n",
    "For this specific model, with just one output node and two hidden\n",
    "nodes, the gradient descent equations take the following form for output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf919416",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "w_{i}^{(2)}\\leftarrow w_{i}^{(2)}- \\eta \\delta^{(2)} a_{i}^{(1)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33d5ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2488e111",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "b^{(2)} \\leftarrow b^{(2)}-\\eta \\delta^{(2)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fa28a",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910468d8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "w_{ij}^{(1)}\\leftarrow w_{ij}^{(1)}- \\eta \\delta_{i}^{(1)} a_{j}^{(0)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b2945",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acc7a57",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "b_{i}^{(1)} \\leftarrow b_{i}^{(1)}-\\eta \\delta_{i}^{(1)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b582f81",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbaa0e4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 3: Extended program\n",
    "\n",
    "We extend our simple code to a function which depends on two variable $x_0$ and $x_1$, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff9633",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "y=f(x_0,x_1)=x_0^2+3x_0x_1+x_1^2+5.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ec768",
   "metadata": {
    "editable": true
   },
   "source": [
    "We feed our network with $n=100$ entries $x_0$ and $x_1$. We have thus two features represented by these variable and an input matrix/design matrix $\\boldsymbol{X}\\in \\mathbf{R}^{n\\times 2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169114a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\\begin{bmatrix} x_{00} & x_{01} \\\\ x_{00} & x_{01} \\\\ x_{10} & x_{11} \\\\ x_{20} & x_{21} \\\\ \\dots & \\dots \\\\ \\dots & \\dots \\\\ x_{n-20} & x_{n-21} \\\\ x_{n-10} & x_{n-11} \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c70aab",
   "metadata": {
    "editable": true
   },
   "source": [
    "Write a code, based on the previous code examples, which takes as input these data and fit the above function.\n",
    "You can extend your code to include automatic differentiation.\n",
    "\n",
    "With these examples, we are now ready to embark upon the writing of more a general code for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f9ec97",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Getting serious, the  back propagation equations for a neural network\n",
    "\n",
    "Now it is time to move away from one node in each layer only. Our inputs are also represented either by several inputs.\n",
    "\n",
    "We have thus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256eb07f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial{\\cal C}((\\boldsymbol{\\Theta}^L)}{\\partial w_{jk}^L}  =  \\left(a_j^L - y_j\\right)a_j^L(1-a_j^L)a_k^{L-1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354215fd",
   "metadata": {
    "editable": true
   },
   "source": [
    "Defining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d5b4f7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\delta_j^L = a_j^L(1-a_j^L)\\left(a_j^L - y_j\\right) = \\sigma'(z_j^L)\\frac{\\partial {\\cal C}}{\\partial (a_j^L)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e627a0",
   "metadata": {
    "editable": true
   },
   "source": [
    "and using the Hadamard product of two vectors we can write this as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f703ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\delta}^L = \\sigma'(\\hat{z}^L)\\circ\\frac{\\partial {\\cal C}}{\\partial (\\boldsymbol{a}^L)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5cedfc",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Analyzing the last results\n",
    "\n",
    "This is an important expression. The second term on the right handside\n",
    "measures how fast the cost function is changing as a function of the $j$th\n",
    "output activation.  If, for example, the cost function doesn't depend\n",
    "much on a particular output node $j$, then $\\delta_j^L$ will be small,\n",
    "which is what we would expect. The first term on the right, measures\n",
    "how fast the activation function $f$ is changing at a given activation\n",
    "value $z_j^L$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed73cce",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More considerations\n",
    "\n",
    "Notice that everything in the above equations is easily computed.  In\n",
    "particular, we compute $z_j^L$ while computing the behaviour of the\n",
    "network, and it is only a small additional overhead to compute\n",
    "$\\sigma'(z^L_j)$.  The exact form of the derivative with respect to the\n",
    "output depends on the form of the cost function.\n",
    "However, provided the cost function is known there should be little\n",
    "trouble in calculating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5829d2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial {\\cal C}}{\\partial (a_j^L)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4e8c0",
   "metadata": {
    "editable": true
   },
   "source": [
    "With the definition of $\\delta_j^L$ we have a more compact definition of the derivative of the cost function in terms of the weights, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a54e2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial{\\cal C}}{\\partial w_{jk}^L}  =  \\delta_j^La_k^{L-1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22caecab",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Derivatives in terms of $z_j^L$\n",
    "\n",
    "It is also easy to see that our previous equation can be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc74ba2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\delta_j^L =\\frac{\\partial {\\cal C}}{\\partial z_j^L}= \\frac{\\partial {\\cal C}}{\\partial a_j^L}\\frac{\\partial a_j^L}{\\partial z_j^L},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6da858",
   "metadata": {
    "editable": true
   },
   "source": [
    "which can also be interpreted as the partial derivative of the cost function with respect to the biases $b_j^L$, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6072551",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\delta_j^L = \\frac{\\partial {\\cal C}}{\\partial b_j^L}\\frac{\\partial b_j^L}{\\partial z_j^L}=\\frac{\\partial {\\cal C}}{\\partial b_j^L},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c3b42",
   "metadata": {
    "editable": true
   },
   "source": [
    "That is, the error $\\delta_j^L$ is exactly equal to the rate of change of the cost function as a function of the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf19264",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Bringing it together\n",
    "\n",
    "We have now three equations that are essential for the computations of the derivatives of the cost function at the output layer. These equations are needed to start the algorithm and they are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa97ae6",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{\\cal C}(\\hat{W^L})}{\\partial w_{jk}^L}  =  \\delta_j^La_k^{L-1},\n",
    "\\label{_auto1} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2a4c51",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317efd9",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\delta_j^L = \\sigma'(z_j^L)\\frac{\\partial {\\cal C}}{\\partial (a_j^L)},\n",
    "\\label{_auto2} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc5f34",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd18886",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\delta_j^L = \\frac{\\partial {\\cal C}}{\\partial b_j^L},\n",
    "\\label{_auto3} \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc2be2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Final back propagating equation\n",
    "\n",
    "We have that (replacing $L$ with a general layer $l$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91baf0da",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\delta_j^l =\\frac{\\partial {\\cal C}}{\\partial z_j^l}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861aaaef",
   "metadata": {
    "editable": true
   },
   "source": [
    "We want to express this in terms of the equations for layer $l+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935f3f5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using the chain rule and summing over all $k$ entries\n",
    "\n",
    "We obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ff860",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\delta_j^l =\\sum_k \\frac{\\partial {\\cal C}}{\\partial z_k^{l+1}}\\frac{\\partial z_k^{l+1}}{\\partial z_j^{l}}=\\sum_k \\delta_k^{l+1}\\frac{\\partial z_k^{l+1}}{\\partial z_j^{l}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f6bf2",
   "metadata": {
    "editable": true
   },
   "source": [
    "and recalling that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9bb2ae",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "z_j^{l+1} = \\sum_{i=1}^{M_{l}}w_{ij}^{l+1}a_i^{l}+b_j^{l+1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a4754",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $M_l$ being the number of nodes in layer $l$, we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1b018",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\delta_j^l =\\sum_k \\delta_k^{l+1}w_{kj}^{l+1}\\sigma'(z_j^l),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe29ee9",
   "metadata": {
    "editable": true
   },
   "source": [
    "This is our final equation.\n",
    "\n",
    "We are now ready to set up the algorithm for back propagation and learning the weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c0434",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Setting up the back propagation algorithm\n",
    "\n",
    "The four equations  provide us with a way of computing the gradient of the cost function. Let us write this out in the form of an algorithm.\n",
    "\n",
    "**First**, we set up the input data $\\hat{x}$ and the activations\n",
    "$\\hat{z}_1$ of the input layer and compute the activation function and\n",
    "the pertinent outputs $\\hat{a}^1$.\n",
    "\n",
    "**Secondly**, we perform then the feed forward till we reach the output\n",
    "layer and compute all $\\hat{z}_l$ of the input layer and compute the\n",
    "activation function and the pertinent outputs $\\hat{a}^l$ for\n",
    "$l=1,2,3,\\dots,L$.\n",
    "\n",
    "**Notation**: The first hidden layer has $l=1$ as label and the final output layer has $l=L$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a100c4d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Setting up the back propagation algorithm, part 2\n",
    "\n",
    "Thereafter we compute the ouput error $\\hat{\\delta}^L$ by computing all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be64b38d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\delta_j^L = \\sigma'(z_j^L)\\frac{\\partial {\\cal C}}{\\partial (a_j^L)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90bd3a9",
   "metadata": {
    "editable": true
   },
   "source": [
    "Then we compute the back propagate error for each $l=L-1,L-2,\\dots,1$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7da6b5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\delta_j^l = \\sum_k \\delta_k^{l+1}w_{kj}^{l+1}\\sigma'(z_j^l).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfd195c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Setting up the Back propagation algorithm, part 3\n",
    "\n",
    "Finally, we update the weights and the biases using gradient descent\n",
    "for each $l=L-1,L-2,\\dots,1$ and update the weights and biases\n",
    "according to the rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2efc325",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "w_{jk}^l\\leftarrow  = w_{jk}^l- \\eta \\delta_j^la_k^{l-1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7827f88",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "b_j^l \\leftarrow b_j^l-\\eta \\frac{\\partial {\\cal C}}{\\partial b_j^l}=b_j^l-\\eta \\delta_j^l,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412bf3d",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\eta$ being the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401c939",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Updating the gradients\n",
    "\n",
    "With the back propagate error for each $l=L-1,L-2,\\dots,1$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13f8a6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\delta_j^l = \\sum_k \\delta_k^{l+1}w_{kj}^{l+1}sigma'(z_j^l),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbb8d26",
   "metadata": {
    "editable": true
   },
   "source": [
    "we update the weights and the biases using gradient descent for each $l=L-1,L-2,\\dots,1$ and update the weights and biases according to the rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d22ef8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "w_{jk}^l\\leftarrow  = w_{jk}^l- \\eta \\delta_j^la_k^{l-1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d260f33",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "b_j^l \\leftarrow b_j^l-\\eta \\frac{\\partial {\\cal C}}{\\partial b_j^l}=b_j^l-\\eta \\delta_j^l,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa2150",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Essential elements of generative models\n",
    "\n",
    "The aim of generative methods is to train a probability distribution $p$. The methods we will focus on are:\n",
    "1. Energy based models, with the family of Boltzmann distributions as a typical example\n",
    "\n",
    "2. Variational autoencoders, based on our discussions on autoencoders\n",
    "\n",
    "3. Generative adversarial networks (GANs) and\n",
    "\n",
    "4. Diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248469d2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Energy models\n",
    "\n",
    "Last week we defined a domain $\\boldsymbol{X}$ of stochastic variables $\\boldsymbol{X}= \\{x_0,x_1, \\dots , x_{n-1}\\}$ with a pertinent probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d376453",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{X})=\\prod_{x_i\\in \\boldsymbol{X}}p(x_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f8626",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have assumed that the random varaibles $x_i$ are all independent and identically distributed (iid).\n",
    "\n",
    "We will now assume that we can defined this function in terms of optimization parameters $\\boldsymbol{\\Theta}$, which could be the biases and weights of deep network, and a set of hidden variables we also assume to be random variables which also are iid. The domain of these variables is\n",
    "$\\boldsymbol{H}= \\{h_0,h_1, \\dots , h_{m-1}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d261e8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Probability model\n",
    "\n",
    "We define a probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a8458",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(x_i,h_j;\\boldsymbol{\\Theta}) = \\frac{f(x_i,h_j;\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7dcae8",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $f(x_i,h_j;\\boldsymbol{\\Theta})$ is a function which we assume is larger or\n",
    "equal than zero and obeys all properties required for a probability\n",
    "distribution and $Z(\\boldsymbol{\\Theta})$ is a normalization constant. Inspired by\n",
    "statistical mechanics, we call it often for the partition function.\n",
    "It is defined as (assuming that we have discrete probability distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c30c77",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "Z(\\boldsymbol{\\Theta})=\\sum_{x_i\\in \\boldsymbol{X}}\\sum_{h_j\\in \\boldsymbol{H}} f(x_i,h_j;\\boldsymbol{\\Theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79720b17",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Marginal and conditional probabilities\n",
    "\n",
    "We can in turn define the marginal probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46039c4",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(x_i;\\boldsymbol{\\Theta}) = \\frac{\\sum_{h_j\\in \\boldsymbol{H}}f(x_i,h_j;\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8a4002",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6d5b2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(h_i;\\boldsymbol{\\Theta}) = \\frac{\\sum_{x_i\\in \\boldsymbol{X}}f(x_i,h_j;\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149f596",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Change of notation\n",
    "\n",
    "**Note the change to a vector notation**. A variable like $\\boldsymbol{x}$\n",
    "represents now a specific **configuration**. We can generate an infinity\n",
    "of such configurations. The final partition function is then the sum\n",
    "over all such possible configurations, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b125d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "Z(\\boldsymbol{\\Theta})=\\sum_{x_i\\in \\boldsymbol{X}}\\sum_{h_j\\in \\boldsymbol{H}} f(x_i,h_j;\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981a9da",
   "metadata": {
    "editable": true
   },
   "source": [
    "changes to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d919e3c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "Z(\\boldsymbol{\\Theta})=\\sum_{\\boldsymbol{x}}\\sum_{\\boldsymbol{h}} f(\\boldsymbol{x},\\boldsymbol{h};\\boldsymbol{\\Theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83225c92",
   "metadata": {
    "editable": true
   },
   "source": [
    "If we have a binary set of variable $x_i$ and $h_j$ and $M$ values of $x_i$ and $N$ values of $h_j$ we have in total $2^M$ and $2^N$ possible $\\boldsymbol{x}$ and $\\boldsymbol{h}$ configurations, respectively.\n",
    "\n",
    "We see that even for the modest binary case, we can easily approach a\n",
    "number of configuration which is not possible to deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add79ed",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Optimization problem\n",
    "\n",
    "At the end, we are not interested in the probabilities of the hidden variables. The probability we thus want to optimize is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21356ba0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{X};\\boldsymbol{\\Theta})=\\prod_{x_i\\in \\boldsymbol{X}}p(x_i;\\boldsymbol{\\Theta})=\\prod_{x_i\\in \\boldsymbol{X}}\\left(\\frac{\\sum_{h_j\\in \\boldsymbol{H}}f(x_i,h_j;\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})}\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73993b9",
   "metadata": {
    "editable": true
   },
   "source": [
    "which we rewrite as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c38311f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{X};\\boldsymbol{\\Theta})=\\frac{1}{Z(\\boldsymbol{\\Theta})}\\prod_{x_i\\in \\boldsymbol{X}}\\left(\\sum_{h_j\\in \\boldsymbol{H}}f(x_i,h_j;\\boldsymbol{\\Theta})\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4ff56",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Further simplifications\n",
    "\n",
    "We simplify further by rewriting it as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c1141",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{X};\\boldsymbol{\\Theta})=\\frac{1}{Z(\\boldsymbol{\\Theta})}\\prod_{x_i\\in \\boldsymbol{X}}f(x_i;\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df7977",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we used $p(x_i;\\boldsymbol{\\Theta}) = \\sum_{h_j\\in \\boldsymbol{H}}f(x_i,h_j;\\boldsymbol{\\Theta})$.\n",
    "The optimization problem is then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c0aefc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\mathrm{arg} \\hspace{0.1cm}\\max_{\\boldsymbol{\\boldsymbol{\\Theta}}\\in {\\mathbb{R}}^{p}}} \\hspace{0.1cm}p(\\boldsymbol{X};\\boldsymbol{\\Theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da88afa",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Optimizing the logarithm instead\n",
    "\n",
    "Computing the derivatives with respect to the parameters $\\boldsymbol{\\Theta}$ is\n",
    "easier (and equivalent) with taking the logarithm of the\n",
    "probability. We will thus optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8632f50",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\mathrm{arg} \\hspace{0.1cm}\\max_{\\boldsymbol{\\boldsymbol{\\Theta}}\\in {\\mathbb{R}}^{p}}} \\hspace{0.1cm}\\log{p(\\boldsymbol{X};\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77b19d",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffefdad",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{p(\\boldsymbol{X};\\boldsymbol{\\Theta})}=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0fe13",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Expression for the gradients\n",
    "This leads to the following equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479f42c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{p(\\boldsymbol{X};\\boldsymbol{\\Theta})}=\\nabla_{\\boldsymbol{\\Theta}}\\left(\\sum_{x_i\\in \\boldsymbol{X}}\\log{f(x_i;\\boldsymbol{\\Theta})}\\right)-\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab32fa3",
   "metadata": {
    "editable": true
   },
   "source": [
    "The first term is called the positive phase and we assume that we have a model for the function $f$ from which we can sample values. Below we will develop an explicit model for this.\n",
    "The second term is called the negative phase and is the one which leads to more difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de964f72",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The derivative of the partition function\n",
    "\n",
    "The partition function, defined above as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee971a9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "Z(\\boldsymbol{\\Theta})=\\sum_{x_i\\in \\boldsymbol{X}}\\sum_{h_j\\in \\boldsymbol{H}} f(x_i,h_j;\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55871351",
   "metadata": {
    "editable": true
   },
   "source": [
    "is in general the most problematic term. In principle both $x$ and $h$ can span large degrees of freedom, if not even infinitely many ones, and computing the partition function itself is often not desirable or even feasible. The above derivative of the partition function can however be written in terms of an expectation value which is in turn evaluated  using Monte Carlo sampling and the theory of Markov chains, popularly shortened to MCMC (or just MC$^2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f1ad7d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Explicit expression for the derivative\n",
    "We can rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab424f86",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\frac{\\nabla_{\\boldsymbol{\\Theta}}Z(\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2c050",
   "metadata": {
    "editable": true
   },
   "source": [
    "which reads in more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca8ad76",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\frac{\\nabla_{\\boldsymbol{\\Theta}} \\sum_{x_i\\in \\boldsymbol{X}}f(x_i;\\boldsymbol{\\Theta})   }{Z(\\boldsymbol{\\Theta})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d84ebd3",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can rewrite the function $f$ (we have assumed that is larger or\n",
    "equal than zero) as $f=\\exp{\\log{f}}$. We can then reqrite the last\n",
    "equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a40f0c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\frac{ \\sum_{x_i\\in \\boldsymbol{X}} \\nabla_{\\boldsymbol{\\Theta}}\\exp{\\log{f(x_i;\\boldsymbol{\\Theta})}}   }{Z(\\boldsymbol{\\Theta})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126aef08",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Final expression\n",
    "\n",
    "Taking the derivative gives us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeba90d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\frac{ \\sum_{x_i\\in \\boldsymbol{X}}f(x_i;\\boldsymbol{\\Theta}) \\nabla_{\\boldsymbol{\\Theta}}\\log{f(x_i;\\boldsymbol{\\Theta})}   }{Z(\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec539297",
   "metadata": {
    "editable": true
   },
   "source": [
    "which is the expectation value of $\\log{f}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8f317",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\sum_{x_i\\in \\boldsymbol{X}}p(x_i;\\boldsymbol{\\Theta}) \\nabla_{\\boldsymbol{\\Theta}}\\log{f(x_i;\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e36af41",
   "metadata": {
    "editable": true
   },
   "source": [
    "that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb39f97",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\mathbb{E}(\\log{f(x_i;\\boldsymbol{\\Theta})}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f40e8",
   "metadata": {
    "editable": true
   },
   "source": [
    "This quantity is evaluated using Monte Carlo sampling, with Gibbs\n",
    "sampling as the standard sampling rule.  Before we discuss the\n",
    "explicit algorithms, we need to remind ourselves about Markov chains\n",
    "and sampling rules like the Metropolis-Hastings algorithm and Gibbs\n",
    "sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f2a301",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Introducing the energy model\n",
    "\n",
    "As we will see below, a typical Boltzmann machines employs a probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f8e50",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x},\\boldsymbol{h};\\boldsymbol{\\Theta}) = \\frac{f(\\boldsymbol{x},\\boldsymbol{h};\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26c200",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $f(\\boldsymbol{x},\\boldsymbol{h};\\boldsymbol{\\Theta})$ is given by a so-called energy model. If we assume that the random variables $x_i$ and $h_j$ take binary values only, for example $x_i,h_j=\\{0,1\\}$, we have a so-called binary-binary model where"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7674479",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "f(\\boldsymbol{x},\\boldsymbol{h};\\boldsymbol{\\Theta})=-E(\\boldsymbol{x}, \\boldsymbol{h};\\boldsymbol{\\Theta}) = \\sum_{x_i\\in \\boldsymbol{X}} x_i a_i+\\sum_{h_j\\in \\boldsymbol{H}} b_j h_j + \\sum_{x_i\\in \\boldsymbol{X},h_j\\in\\boldsymbol{H}} x_i w_{ij} h_j,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304fd4d2",
   "metadata": {
    "editable": true
   },
   "source": [
    "where the set of parameters are given by the biases and weights $\\boldsymbol{\\Theta}=\\{\\boldsymbol{a},\\boldsymbol{b},\\boldsymbol{W}\\}$.\n",
    "**Note the vector notation** instead of $x_i$ and $h_j$ for $f$. The vectors $\\boldsymbol{x}$ and $\\boldsymbol{h}$ represent a specific instance of stochastic variables $x_i$ and $h_j$. These arrangements of $\\boldsymbol{x}$ and $\\boldsymbol{h}$ lead to a specific energy configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0902720",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More compact notation\n",
    "\n",
    "With the above definition we can write the probability as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a11b065",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x},\\boldsymbol{h};\\boldsymbol{\\Theta}) = \\frac{\\exp{(\\boldsymbol{a}^T\\boldsymbol{x}+\\boldsymbol{b}^T\\boldsymbol{h}+\\boldsymbol{x}^T\\boldsymbol{W}\\boldsymbol{h})}}{Z(\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b0233",
   "metadata": {
    "editable": true
   },
   "source": [
    "where the biases $\\boldsymbol{a}$ and $\\boldsymbol{h}$ and the weights defined by the matrix $\\boldsymbol{W}$ are the parameters we need to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4189b74",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Anticipating results to be derived\n",
    "\n",
    "Since the binary-binary energy model is linear in the parameters $a_i$, $b_j$ and\n",
    "$w_{ij}$, it is easy to see that the derivatives with respect to the\n",
    "various optimization parameters yield expressions used in the\n",
    "evaluation of gradients like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf5caa3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial E(\\boldsymbol{x}, \\boldsymbol{h};\\boldsymbol{\\Theta})}{\\partial w_{ij}}=-x_ih_j,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98c1848",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31318ed",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial E(\\boldsymbol{x}, \\boldsymbol{h};\\boldsymbol{\\Theta})}{\\partial a_i}=-x_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28959165",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb237c5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial E(\\boldsymbol{x}, \\boldsymbol{h};\\boldsymbol{\\Theta})}{\\partial b_j}=-h_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153a7b47",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Boltzmann Machines, marginal and conditional probabilities\n",
    "\n",
    "A generative model can learn to represent and sample from a\n",
    "probability distribution. The core idea is to learn a parametric model\n",
    "of the probability distribution from which the training data was\n",
    "drawn. As an example\n",
    "1. A model for images could learn to draw new examples of cats and dogs, given a training dataset of images of cats and dogs.\n",
    "\n",
    "2. Generate a sample of an ordered or disordered Ising model phase, having been given samples of such phases.\n",
    "\n",
    "3. Model the trial function for Monte Carlo calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d4aceb",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Generative and discriminative models\n",
    "\n",
    "Generative and discriminative models use both gradient-descent based\n",
    "learning procedures for minimizing cost functions\n",
    "\n",
    "However, in energy based models we don't use backpropagation and automatic\n",
    "differentiation for computing gradients, instead we turn to Markov\n",
    "Chain Monte Carlo methods.\n",
    "\n",
    "A typical deep neural network has several hidden layers. A restricted\n",
    "Boltzmann machine has normally one hidden layer, however several RBMs can\n",
    "be stacked to make up deep Belief Networks, of which they constitute\n",
    "the building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281a703b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Basics of the Boltzmann machine\n",
    "A BM is what we would call an undirected probabilistic graphical model\n",
    "with stochastic continuous or discrete units.\n",
    "\n",
    "It is interpreted as a stochastic recurrent neural network where the\n",
    "state of each unit(neurons/nodes) depends on the units it is connected\n",
    "to. The weights in the network represent thus the strength of the\n",
    "interaction between various units/nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba085b42",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More about the basics\n",
    "A standard BM network is divided into a set of observable and visible units $\\boldsymbol{x}$ and a set of unknown hidden units/nodes $\\boldsymbol{h}$.\n",
    "\n",
    "Additionally there can be bias nodes for the hidden and visible layers. These biases are normally set to $1$.\n",
    "\n",
    "BMs are stackable, meaning they cwe can train a BM which serves as input to another BM. We can construct deep networks for learning complex PDFs. The layers can be trained one after another, a feature which makes them popular in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61e321",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Difficult to train\n",
    "\n",
    "However, they are often hard to train. This leads to the introduction\n",
    "of so-called restricted BMs, or RBMS.  Here we take away all lateral\n",
    "connections between nodes in the visible layer as well as connections\n",
    "between nodes in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c23d79",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The network layers\n",
    "\n",
    "1. A function $\\boldsymbol{x}$ that represents the visible layer, a vector of $M$ elements (nodes). This layer represents both what the RBM might be given as training input, and what we want it to be able to reconstruct. This might for example be the pixels of an image, the spin values of the Ising model, or coefficients representing speech.\n",
    "\n",
    "2. The function $\\boldsymbol{h}$ represents the hidden, or latent, layer. A vector of $N$ elements (nodes). Also called \"feature detectors\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daae74f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Goal of hidden layer\n",
    "\n",
    "The goal of the hidden layer is to increase the model's expressive\n",
    "power. We encode complex interactions between visible variables by\n",
    "introducing additional, hidden variables that interact with visible\n",
    "degrees of freedom in a simple manner, yet still reproduce the complex\n",
    "correlations between visible degrees in the data once marginalized\n",
    "over (integrated out)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c52067",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The parameters\n",
    "\n",
    "The network parameters, to be optimized/learned:\n",
    "1. $\\boldsymbol{a}$ represents the visible bias, a vector of same length $M$ as $\\boldsymbol{x}$.\n",
    "\n",
    "2. $\\boldsymbol{b}$ represents the hidden bias, a vector of same length $N$  as $\\boldsymbol{h}$.\n",
    "\n",
    "3. $\\boldsymbol{W}$ represents the interaction weights, a matrix of size $M\\times N$.\n",
    "\n",
    "Note that we have specified the lengths of $bm{x}$ and $\\boldsymbol{h}$. These\n",
    "lengths define the number of visible and hidden units, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfceacd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Joint distribution\n",
    "\n",
    "The restricted Boltzmann machine is described by a Boltzmann distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c60a9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\tP_{rbm}(\\boldsymbol{x},\\boldsymbol{h},\\boldsymbol{\\Theta}) = \\frac{1}{Z(\\boldsymbol{\\Theta})} \\exp{-(E(\\boldsymbol{x},\\boldsymbol{h},\\boldsymbol{\\Theta}))},\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d8e6e",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $Z$ is the normalization constant or partition function discussed earlier and defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf76b4a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\tZ(\\boldsymbol{\\Theta}) = \\int \\int \\exp{-E(\\boldsymbol{x},\\boldsymbol{h},\\boldsymbol{\\Theta})} d\\boldsymbol{x} d\\boldsymbol{h}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f7f7f",
   "metadata": {
    "editable": true
   },
   "source": [
    "It is common to set  the temperature $T$ to one. It is omitted in the equations above. The energy is thus a dimensionless function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565efc4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Network Elements, the energy function\n",
    "\n",
    "The function $E(\\boldsymbol{x},\\boldsymbol{h},\\boldsymbol{\\Theta})$ gives the **energy** of a\n",
    "configuration (pair of vectors) $(\\boldsymbol{x}, \\boldsymbol{h})$. The lower\n",
    "the energy of a configuration, the higher the probability of it. This\n",
    "function also depends on the parameters $\\boldsymbol{a}$, $\\boldsymbol{b}$ and\n",
    "$W$. Thus, when we adjust them during the learning procedure, we are\n",
    "adjusting the energy function to best fit our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c08420",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Defining different types of RBMs\n",
    "\n",
    "There are different variants of RBMs, and the differences lie in the types of visible and hidden units we choose as well as in the implementation of the energy function $E(\\boldsymbol{x},\\boldsymbol{h},\\boldsymbol{\\Theta})$. The connection between the nodes in the two layers is given by the weights $w_{ij}$. \n",
    "\n",
    "**Binary-Binary RBM:**\n",
    "\n",
    "RBMs were first developed using binary units in both the visible and hidden layer. The corresponding energy function is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42c97dc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\tE(\\boldsymbol{x}, \\boldsymbol{h},\\boldsymbol{\\Theta}) = - \\sum_i^M x_i a_i- \\sum_j^N b_j h_j - \\sum_{i,j}^{M,N} x_i w_{ij} h_j,\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4ae1d",
   "metadata": {
    "editable": true
   },
   "source": [
    "where the binary values taken on by the nodes are most commonly 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da4f86",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gaussian-binary RBM\n",
    "\n",
    "Another varient is the RBM where the visible units are Gaussian while the hidden units remain binary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b72219",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\tE(\\boldsymbol{x}, \\boldsymbol{h},\\boldsymbol{\\Theta}) = \\sum_i^M \\frac{(x_i - a_i)^2}{2\\sigma_i^2} - \\sum_j^N b_j h_j - \\sum_{i,j}^{M,N} \\frac{x_i w_{ij} h_j}{\\sigma_i^2}. \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fad087",
   "metadata": {
    "editable": true
   },
   "source": [
    "This type of RBMs are useful when we model continuous data (i.e., we wish $\\boldsymbol{x}$ to be continuous). The paramater $\\sigma_i^2$ is meant to represent a variance and is foten just set to one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ed184",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Code for RBMs using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15cbc7bd",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid , save_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "datasets.MNIST('./data',\n",
    "    train=True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor()])\n",
    "     ),\n",
    "     batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "datasets.MNIST('./data',\n",
    "    train=False,\n",
    "    transform=transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "    ),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "\n",
    "class RBM(nn.Module):\n",
    "   def __init__(self,\n",
    "               n_vis=784,\n",
    "               n_hin=500,\n",
    "               k=5):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(n_hin,n_vis)*1e-2)\n",
    "        self.v_bias = nn.Parameter(torch.zeros(n_vis))\n",
    "        self.h_bias = nn.Parameter(torch.zeros(n_hin))\n",
    "        self.k = k\n",
    "    \n",
    "   def sample_from_p(self,p):\n",
    "       return F.relu(torch.sign(p - Variable(torch.rand(p.size()))))\n",
    "    \n",
    "   def v_to_h(self,v):\n",
    "        p_h = F.sigmoid(F.linear(v,self.W,self.h_bias))\n",
    "        sample_h = self.sample_from_p(p_h)\n",
    "        return p_h,sample_h\n",
    "    \n",
    "   def h_to_v(self,h):\n",
    "        p_v = F.sigmoid(F.linear(h,self.W.t(),self.v_bias))\n",
    "        sample_v = self.sample_from_p(p_v)\n",
    "        return p_v,sample_v\n",
    "        \n",
    "   def forward(self,v):\n",
    "        pre_h1,h1 = self.v_to_h(v)\n",
    "        \n",
    "        h_ = h1\n",
    "        for _ in range(self.k):\n",
    "            pre_v_,v_ = self.h_to_v(h_)\n",
    "            pre_h_,h_ = self.v_to_h(v_)\n",
    "        \n",
    "        return v,v_\n",
    "    \n",
    "   def free_energy(self,v):\n",
    "        vbias_term = v.mv(self.v_bias)\n",
    "        wx_b = F.linear(v,self.W,self.h_bias)\n",
    "        hidden_term = wx_b.exp().add(1).log().sum(1)\n",
    "        return (-hidden_term - vbias_term).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rbm = RBM(k=1)\n",
    "train_op = optim.SGD(rbm.parameters(),0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss_ = []\n",
    "    for _, (data,target) in enumerate(train_loader):\n",
    "        data = Variable(data.view(-1,784))\n",
    "        sample_data = data.bernoulli()\n",
    "        \n",
    "        v,v1 = rbm(sample_data)\n",
    "        loss = rbm.free_energy(v) - rbm.free_energy(v1)\n",
    "        loss_.append(loss.data)\n",
    "        train_op.zero_grad()\n",
    "        loss.backward()\n",
    "        train_op.step()\n",
    "\n",
    "    print(\"Training loss for {} epoch: {}\".format(epoch, np.mean(loss_)))\n",
    "\n",
    "\n",
    "def show_adn_save(file_name,img):\n",
    "    npimg = np.transpose(img.numpy(),(1,2,0))\n",
    "    f = \"./%s.png\" % file_name\n",
    "    plt.imshow(npimg)\n",
    "    plt.imsave(f,npimg)\n",
    "\n",
    "show_adn_save(\"real\",make_grid(v.view(32,1,28,28).data))\n",
    "show_adn_save(\"generate\",make_grid(v1.view(32,1,28,28).data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a19bd4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Energy-based models and Langevin sampling\n",
    "\n",
    "See discussions in Foster, chapter 7 on energy-based models at <https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/tree/main/notebooks/07_ebm/01_ebm>\n",
    "\n",
    "That notebook is based on a recent article by Du and Mordatch, **Implicit generation and modeling with energy-based models**, see <https://arxiv.org/pdf/1903.08689.pdf.>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af828a1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Tensor-flow examples\n",
    "\n",
    "1. To create Boltzmann machine using Keras, see Babcock and Bali chapter 4, see <https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_4/models/rbm.py>\n",
    "\n",
    "2. See also Foster, chapter 7 on energy-based models at <https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/tree/main/notebooks/07_ebm/01_ebm>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b31914",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Kullback-Leibler divergence\n",
    "\n",
    "Before we continue, we need to remind ourselves about the\n",
    "Kullback-Leibler divergence introduced earlier. This will also allow\n",
    "us to introduce another measure used in connection with the training\n",
    "of Generative Adversarial Networks, the so-called Jensen-Shannon divergence..\n",
    "These metrics are useful for quantifying the similarity between two probability distributions.\n",
    "\n",
    "The Kullbackâ€“Leibler (KL) divergence, labeled $D_{KL}$,   measures how one probability distribution $p$ diverges from a second expected probability distribution $q$,\n",
    "that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cbfbc9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "D_{KL}(p \\| q) = \\int_x p(x) \\log \\frac{p(x)}{q(x)} dx.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec788ab",
   "metadata": {
    "editable": true
   },
   "source": [
    "The KL-divegernce $D_{KL}$ achieves the minimum zero when $p(x) == q(x)$ everywhere.\n",
    "\n",
    "Note that the KL divergence is asymmetric. In cases where $p(x)$ is\n",
    "close to zero, but $q(x)$ is significantly non-zero, the $q$'s effect\n",
    "is disregarded. It could cause buggy results when we just want to\n",
    "measure the similarity between two equally important distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4eef1c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Generative model,  basic overview (Borrowed from Rashcka et al)\n",
    "\n",
    "<!-- FIGURE: [figures/figure1.png, width=900 frac=1.0] -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb2759",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reminder on VAEs\n",
    "\n",
    "Mathematically, we can imagine the latent variables and the data we\n",
    "observe as modeled by a joint distribution $p(\\boldsymbol{x}, \\boldsymbol{h};\\boldsymbol{\\Theta})$.  Recall one\n",
    "approach of generative modeling, termed likelihood-based, is to\n",
    "learn a model to maximize the likelihood $p(\\boldsymbol{x};\\boldsymbol{\\Theta})$ of all observed\n",
    "$\\boldsymbol{x}$.  There are two ways we can manipulate this joint distribution\n",
    "to recover the likelihood of purely our observed data $p(\\boldsymbol{x};\\boldsymbol{\\Theta})$; we can\n",
    "explicitly marginalize\n",
    "out the latent variable $\\boldsymbol{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b50e49",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x}) = \\int p(\\boldsymbol{x}, \\boldsymbol{h})d\\boldsymbol{h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95d71b",
   "metadata": {
    "editable": true
   },
   "source": [
    "or, we could also appeal to the chain rule of probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9bd1a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x}) = \\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{p(\\boldsymbol{h}|\\boldsymbol{x})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcaed94",
   "metadata": {
    "editable": true
   },
   "source": [
    "We suppress here the dependence\ton the optimization parameters $\\boldsymbol{\\Theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44409478",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Evidence Lower Bound\n",
    "Directly computing and maximizing the likelihood $p(\\boldsymbol{x})$ is\n",
    "difficult because it either involves integrating out all latent\n",
    "variables $\\boldsymbol{h}$, which is intractable for\n",
    "complex models, or it involves having access to a ground truth latent\n",
    "encoder $p(\\boldsymbol{h}|\\boldsymbol{x})$.\n",
    "\n",
    "Using the last  two equations, we can derive a term called the Evidence Lower Bound (ELBO), which as its name suggests, is a lower\n",
    "  bound of the evidence.  The evidence is quantified in this case as\n",
    "the log likelihood of the observed data.  Then, maximizing the ELBO\n",
    "becomes a proxy objective with which to optimize a latent variable\n",
    "model; in the best case, when the ELBO is powerfully parameterized and\n",
    "perfectly optimized, it becomes exactly equivalent to the evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a41a7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## ELBO equations\n",
    "Formally, the equation of the ELBO is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e626ac3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dbfb1d",
   "metadata": {
    "editable": true
   },
   "source": [
    "To make the relationship with the evidence explicit, we can mathematically write:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e0c11e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\log p(\\boldsymbol{x}) \\geq \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f61f44",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Introducing the encoder function\n",
    "\n",
    "Here, $q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})$ is a flexible approximate\n",
    "variational distribution with parameters $\\boldsymbol{\\phi}$ that we seek to\n",
    "optimize.  Intuitively, it can be thought of as a parameterizable\n",
    "model that is learned to estimate the true distribution over latent\n",
    "variables for given observations $\\boldsymbol{x}$; in other words, it seeks to\n",
    "approximate true posterior $p(\\boldsymbol{h}|\\boldsymbol{x})$.  As we saw last week when we\n",
    "explored Variational Autoencoders, as we increase the lower bound\n",
    "by tuning the parameters $\\boldsymbol{\\phi}$ to maximize the ELBO, we gain\n",
    "access to components that can be used to model the true data\n",
    "distribution and sample from it, thus learning a generative model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cefbaa",
   "metadata": {
    "editable": true
   },
   "source": [
    "## ELBO\n",
    "\n",
    "To better understand the relationship between the evidence and the ELBO, let us perform another derivation, this time using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe562f4",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\log p(\\boldsymbol{x}) & = \\log p(\\boldsymbol{x}) \\int q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})d\\boldsymbol{h} && \\text{(Multiply by $1 = \\int q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})d\\boldsymbol{h}$)}\\\\\n",
    "          & = \\int q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})(\\log p(\\boldsymbol{x}))d\\boldsymbol{h} && \\text{(Bring evidence into integral)}\\\\\n",
    "          & = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log p(\\boldsymbol{x})\\right] && \\text{(Definition of Expectation)}\\\\\n",
    "          & = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{p(\\boldsymbol{h}|\\boldsymbol{x})}\\right]&& \\\\\n",
    "          & = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}{p(\\boldsymbol{h}|\\boldsymbol{x})q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right]&& \\text{(Multiply by $1 = \\frac{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}$)}\\\\\n",
    "          & = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right] + \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}{p(\\boldsymbol{h}|\\boldsymbol{x})}\\right] && \\text{(Split the Expectation)}\\\\\n",
    "          & = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right] +\n",
    "\t  D_{KL}(q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})\\vert\\vert p(\\boldsymbol{h}|\\boldsymbol{x}))  && \\text{(Definition of KL Divergence)}\\\\\n",
    "          & \\geq \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right]  && \\text{(KL Divergence always $\\geq 0$)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b166318",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Analysis\n",
    "\n",
    "From this derivation, we clearly observe from the last equation\n",
    "that the evidence is equal to the ELBO plus the KL Divergence between\n",
    "the approximate posterior $q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})$ and the true\n",
    "posterior $p(\\boldsymbol{h}|\\boldsymbol{x})$.  Understanding this term is the\n",
    "key to understanding not only the relationship between the ELBO and\n",
    "the evidence, but also the reason why optimizing the ELBO is an\n",
    "appropriate objective at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e78c5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The VAE\n",
    "\n",
    "In the default formulation of the VAE by Kingma and Welling (2015), we directly maximize the ELBO.  This\n",
    "approach is \\textit{variational}, because we optimize for the best\n",
    "$q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})$ amongst a family of potential posterior\n",
    "distributions parameterized by $\\boldsymbol{\\phi}$.  It is called an\n",
    "\\textit{autoencoder} because it is reminiscent of a traditional\n",
    "autoencoder model, where input data is trained to predict itself after\n",
    "undergoing an intermediate bottlenecking representation step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d16145f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Dissecting the equations\n",
    "To make\n",
    "this connection explicit, let us dissect the ELBO term further:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34dd80",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "{\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{x}, \\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right]}\n",
    "&= {\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{h})p(\\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right]}         && {\\text{(Chain Rule of Probability)}}\\\\\n",
    "&= {\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{h})\\right] + \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log\\frac{p(\\boldsymbol{h})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\right]}         && {\\text{(Split the Expectation)}}\\\\\n",
    "&= \\underbrace{{\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{h})\\right]}}_\\text{reconstruction term} - \\underbrace{{D_{KL}(q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\vert\\vert{p(\\boldsymbol{h}))}}_\\text{prior matching term} && {\\text{(Definition of KL Divergence)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fb3df8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Bottlenecking distribution\n",
    "\n",
    "In this case, we learn an intermediate bottlenecking distribution\n",
    "$q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})$ that can be treated as\n",
    "an \\textit{encoder}; it transforms inputs into a distribution over\n",
    "possible latents.  Simultaneously, we learn a deterministic function\n",
    "$p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{h})$ to convert a given latent vector\n",
    "$\\boldsymbol{h}$ into an observation $\\boldsymbol{x}$, which can be interpreted as\n",
    "a \\textit{decoder}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4485136d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Decoder and encoder\n",
    "The two terms in the last equation each have intuitive descriptions: the first\n",
    "term measures the reconstruction likelihood of the decoder from our\n",
    "variational distribution; this ensures that the learned distribution\n",
    "is modeling effective latents that the original data can be\n",
    "regenerated from.  The second term measures how similar the learned\n",
    "variational distribution is to a prior belief held over latent\n",
    "variables.  Minimizing this term encourages the encoder to actually\n",
    "learn a distribution rather than collapse into a Dirac delta function.\n",
    "Maximizing the ELBO is thus equivalent to maximizing its first term\n",
    "and minimizing its second term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d40370",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Defining feature of VAEs\n",
    "\n",
    "A defining feature of the VAE is how the ELBO is optimized jointly over parameters $\\boldsymbol{\\phi}$ and $\\boldsymbol{\\theta}$.  The encoder of the VAE is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior is often selected to be a standard multivariate Gaussian:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc8408",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x}) &= N(\\boldsymbol{h}; \\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\boldsymbol{x}), \\boldsymbol{\\sigma}_{\\boldsymbol{\\phi}}^2(\\boldsymbol{x})\\textbf{I})\\\\\n",
    "    p(\\boldsymbol{h}) &= N(\\boldsymbol{h}; \\boldsymbol{0}, \\textbf{I})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f098bef",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Analytical evaluation\n",
    "\n",
    "Then, the KL divergence term of the ELBO can be computed analytically, and the reconstruction term can be approximated using a Monte Carlo estimate.  Our objective can then be rewritten as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e897db1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "  \\mathrm{argmax}_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}} \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})}\\left[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{h})\\right] - D_{KL}(q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})\\vert\\vert p(\\boldsymbol{h})) \\approx \\mathrm{argmax}_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}} \\sum_{l=1}^{L}\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{h}^{(l)}) - D_{KL}(q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})\\vert\\vert p(\\boldsymbol{h}))\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53254e5",
   "metadata": {
    "editable": true
   },
   "source": [
    "where latents $\\{\\boldsymbol{h}^{(l)}\\}_{l=1}^L$ are sampled from $q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})$, for every observation $\\boldsymbol{x}$ in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ab543",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reparameterization trick\n",
    "\n",
    "However, a problem arises in this default setup: each $\\boldsymbol{h}^{(l)}$\n",
    "that our loss is computed on is generated by a stochastic sampling\n",
    "procedure, which is generally non-differentiable.  Fortunately, this\n",
    "can be addressed via the \\textit{reparameterization trick} when\n",
    "$q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}|\\boldsymbol{x})$ is designed to model certain\n",
    "distributions, including the multivariate Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587c0ba",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Actual implementation\n",
    "\n",
    "The reparameterization trick rewrites a random variable as a\n",
    "deterministic function of a noise variable; this allows for the\n",
    "optimization of the non-stochastic terms through gradient descent.\n",
    "For example, samples from a normal distribution\n",
    "$x \\sim N(x;\\mu, \\sigma^2)$ with arbitrary mean $\\mu$ and\n",
    "variance $\\sigma^2$ can be rewritten as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486bb83a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    x &= \\mu + \\sigma\\epsilon \\quad \\text{with } \\epsilon \\sim N(\\epsilon; 0, \\boldsymbol{I})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbee49",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Interpretation\n",
    "An arbitrary Gaussian distributions can be interpreted as\n",
    "standard Gaussians (of which $\\epsilon$ is a sample) that have their\n",
    "mean shifted from zero to the target mean $\\mu$ by addition, and their\n",
    "variance stretched by the target variance $\\sigma^2$.  Therefore, by\n",
    "the reparameterization trick, sampling from an arbitrary Gaussian\n",
    "distribution can be performed by sampling from a standard Gaussian,\n",
    "scaling the result by the target standard deviation, and shifting it\n",
    "by the target mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c34e25",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Deterministic function\n",
    "\n",
    "In a VAE, each $\\boldsymbol{h}$ is thus computed as a deterministic function of input $\\boldsymbol{x}$ and auxiliary noise variable $\\boldsymbol{\\epsilon}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef90d38",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{h} &= \\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\boldsymbol{x}) + \\boldsymbol{\\sigma}_{\\boldsymbol{\\phi}}(\\boldsymbol{x})\\odot\\boldsymbol{\\epsilon} \\quad \\text{with } \\boldsymbol{\\epsilon} \\sim N(\\boldsymbol{\\epsilon};\\boldsymbol{0}, \\textbf{I})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199fc2f5",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\odot$ represents an element-wise product.  Under this\n",
    "reparameterized version of $\\boldsymbol{h}$, gradients can then be computed\n",
    "with respect to $\\boldsymbol{\\phi}$ as desired, to optimize\n",
    "$\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}$ and $\\boldsymbol{\\sigma}_{\\boldsymbol{\\phi}}$.  The VAE\n",
    "therefore utilizes the reparameterization trick and Monte Carlo\n",
    "estimates to optimize the ELBO jointly over $\\boldsymbol{\\phi}$ and\n",
    "$\\boldsymbol{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71cff88",
   "metadata": {
    "editable": true
   },
   "source": [
    "## After training\n",
    "\n",
    "After training a VAE, generating new data can be performed by sampling\n",
    "directly from the latent space $p(\\boldsymbol{h})$ and then running it through\n",
    "the decoder.  Variational Autoencoders are particularly interesting\n",
    "when the dimensionality of $\\boldsymbol{h}$ is less than that of input\n",
    "$\\boldsymbol{x}$, as we might then be learning compact, useful\n",
    "representations.  Furthermore, when a semantically meaningful latent\n",
    "space is learned, latent vectors can be edited before being passed to\n",
    "the decoder to more precisely control the data generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a0f43",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Diffusion models, basics\n",
    "\n",
    "Diffusion models are inspired by non-equilibrium thermodynamics. They\n",
    "define a Markov chain of diffusion steps to slowly add random noise to\n",
    "data and then learn to reverse the diffusion process to construct\n",
    "desired data samples from the noise. Unlike VAE or flow models,\n",
    "diffusion models are learned with a fixed procedure and the latent\n",
    "variable has high dimensionality (same as the original data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e73da8b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Problems with probabilistic models\n",
    "\n",
    "Historically, probabilistic models suffer from a tradeoff between two\n",
    "conflicting objectives: \\textit{tractability} and\n",
    "\\textit{flexibility}. Models that are \\textit{tractable} can be\n",
    "analytically evaluated and easily fit to data (e.g. a Gaussian or\n",
    "Laplace). However, these models are unable to aptly describe structure\n",
    "in rich datasets. On the other hand, models that are \\textit{flexible}\n",
    "can be molded to fit structure in arbitrary data. For example, we can\n",
    "define models in terms of any (non-negative) function $\\phi(\\boldsymbol{x})$\n",
    "yielding the flexible distribution $p\\left(\\boldsymbol{x}\\right) =\n",
    "\\frac{\\phi\\left(\\boldsymbol{x} \\right)}{Z}$, where $Z$ is a normalization\n",
    "constant. However, computing this normalization constant is generally\n",
    "intractable. Evaluating, training, or drawing samples from such\n",
    "flexible models typically requires a very expensive Monte Carlo\n",
    "process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44e22d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Diffusion models\n",
    "Diffusion models have several interesting features\n",
    "* extreme flexibility in model structure,\n",
    "\n",
    "* exact sampling,\n",
    "\n",
    "* easy multiplication with other distributions, e.g. in order to compute a posterior, and\n",
    "\n",
    "* the model log likelihood, and the probability of individual states, to be cheaply evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ed2ab",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Original idea\n",
    "\n",
    "In the original formulation, one uses a Markov chain to gradually\n",
    "convert one distribution into another, an idea used in non-equilibrium\n",
    "statistical physics and sequential Monte Carlo. Diffusion models build\n",
    "a generative Markov chain which converts a simple known distribution\n",
    "(e.g. a Gaussian) into a target (data) distribution using a diffusion\n",
    "process. Rather than use this Markov chain to approximately evaluate a\n",
    "model which has been otherwise defined, one can  explicitly define the\n",
    "probabilistic model as the endpoint of the Markov chain. Since each\n",
    "step in the diffusion chain has an analytically evaluable probability,\n",
    "the full chain can also be analytically evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2564bd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Diffusion learning\n",
    "\n",
    "Learning in this framework involves estimating small perturbations to\n",
    "a diffusion process. Estimating small, analytically tractable,\n",
    "perturbations is more tractable than explicitly describing the full\n",
    "distribution with a single, non-analytically-normalizable, potential\n",
    "function.  Furthermore, since a diffusion process exists for any\n",
    "smooth target distribution, this method can capture data distributions\n",
    "of arbitrary form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b93d04d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematics of diffusion models\n",
    "\n",
    "Let us go back our discussions of the variational autoencoders from\n",
    "last week, see\n",
    "<https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week15/ipynb/week15.ipynb>. As\n",
    "a first attempt at understanding diffusion models, we can think of\n",
    "these as stacked VAEs, or better, recursive VAEs.\n",
    "\n",
    "Let us try to see why. As an intermediate step, we consider so-called\n",
    "hierarchical VAEs, which can be seen as a generalization of VAEs that\n",
    "include multiple hierarchies of latent spaces.\n",
    "\n",
    "**Note**: Many of the derivations and figures here are inspired and borrowed from the excellent exposition of diffusion models by Calvin Luo at <https://arxiv.org/abs/2208.11970>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafb5f3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Chains of VAEs\n",
    "\n",
    "Markovian\n",
    "VAEs represent a  generative process where we use  Markov chain to build a hierarchy of VAEs.\n",
    "\n",
    "Each transition down the hierarchy is Markovian, where we decode each\n",
    "latent set of variables $\\boldsymbol{h}_t$ in terms of the previous latent variable $\\boldsymbol{h}_{t-1}$.\n",
    "Intuitively, and visually, this can be seen as simply stacking VAEs on\n",
    "top of each other (see figure next slide).\n",
    "\n",
    "One can think of such a model as a recursive VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3264827",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematical representation\n",
    "\n",
    "Mathematically, we represent the joint distribution and the posterior\n",
    "of a Markovian VAE as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dabd01",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    p(\\boldsymbol{x}, \\boldsymbol{h}_{1:T}) &= p(\\boldsymbol{h}_T)p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{h}_1)\\prod_{t=2}^{T}p_{\\boldsymbol{\\theta}}(\\boldsymbol{h}_{t-1}|\\boldsymbol{h}_{t})\\\\\n",
    "    q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_{1:T}|\\boldsymbol{x}) &= q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_1|\\boldsymbol{x})\\prod_{t=2}^{T}q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_{t}|\\boldsymbol{h}_{t-1})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62a11b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back to the marginal probability\n",
    "\n",
    "We can then define the marginal probability we want to optimize as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4caff04",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\log p(\\boldsymbol{x}) &= \\log \\int p(\\boldsymbol{x}, \\boldsymbol{h}_{1:T}) d\\boldsymbol{h}_{1:T}  \\\\\n",
    "&= \\log \\int \\frac{p(\\boldsymbol{x}, \\boldsymbol{h}_{1:T})q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_{1:T}|\\boldsymbol{x})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_{1:T}|\\boldsymbol{x})} d\\boldsymbol{h}_{1:T}         && \\text{(Multiply by 1 = $\\frac{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_{1:T}|\\boldsymbol{x})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_{1:T}|\\boldsymbol{x})}$)}\\\\\n",
    "&= \\log \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_{1:T}|\\boldsymbol{x})}\\left[\\frac{p(\\boldsymbol{x}, \\boldsymbol{h}_{1:T})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_{1:T}|\\boldsymbol{x})}\\right]         && \\text{(Definition of Expectation)}\\\\\n",
    "&\\geq \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_{1:T}|\\boldsymbol{x})}\\left[\\log \\frac{p(\\boldsymbol{x}, \\boldsymbol{h}_{1:T})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_{1:T}|\\boldsymbol{x})}\\right]         && \\text{(Discussed last week)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ea4d0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Diffusion models for hierarchical VAE, from <https://arxiv.org/abs/2208.11970>\n",
    "\n",
    "A Markovian hierarchical Variational Autoencoder with $T$ hierarchical\n",
    "latents.  The generative process is modeled as a Markov chain, where\n",
    "each latent $\\boldsymbol{h}_t$ is generated only from the previous latent\n",
    "$\\boldsymbol{h}_{t+1}$. Here $\\boldsymbol{z}$ is our latent variable $\\boldsymbol{h}$.\n",
    "\n",
    "<!-- dom:FIGURE: [figures/figure1.png, width=800 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/figure1.png\" width=\"800\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22316dc4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Equation for the Markovian hierarchical VAE\n",
    "\n",
    "We obtain then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98853ede",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_{1:T}|\\boldsymbol{x})}\\left[\\log \\frac{p(\\boldsymbol{x}, \\boldsymbol{h}_{1:T})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_{1:T}|\\boldsymbol{x})}\\right]\n",
    "&= \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_{1:T}|\\boldsymbol{x})}\\left[\\log \\frac{p(\\boldsymbol{h}_T)p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{h}_1)\\prod_{t=2}^{T}p_{\\boldsymbol{\\theta}}(\\boldsymbol{h}_{t-1}|\\boldsymbol{h}_{t})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_1|\\boldsymbol{x})\\prod_{t=2}^{T}q_{\\boldsymbol{\\phi}}(\\boldsymbol{h}_{t}|\\boldsymbol{h}_{t-1})}\\right]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284f0bb",
   "metadata": {
    "editable": true
   },
   "source": [
    "We will modify this equation when we discuss what are normally called Variational Diffusion Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c19da54",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Variational Diffusion Models\n",
    "\n",
    "The easiest way to think of a Variational Diffusion Model (VDM) is as a Markovian Hierarchical Variational Autoencoder with three key restrictions:\n",
    "\n",
    "1. The latent dimension is exactly equal to the data dimension\n",
    "\n",
    "2. The structure of the latent encoder at each timestep is not learned; it is pre-defined as a linear Gaussian model.  In other words, it is a Gaussian distribution centered around the output of the previous timestep\n",
    "\n",
    "3. The Gaussian parameters of the latent encoders vary over time in such a way that the distribution of the latent at final timestep $T$ is a standard Gaussian\n",
    "\n",
    "The VDM posterior is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973bfe20",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0) = \\prod_{t = 1}^{T}q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t-1})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc39ed8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Second assumption\n",
    "\n",
    "The distribution of each latent variable in the encoder is a Gaussian centered around its previous hierarchical latent.\n",
    "Here then, the structure of the encoder at each timestep $t$ is not learned; it\n",
    "is fixed as a linear Gaussian model, where the mean and standard\n",
    "deviation can be set beforehand as hyperparameters, or learned as\n",
    "parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4ae584",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Parameterizing Gaussian encoder\n",
    "\n",
    "We parameterize the Gaussian encoder with mean $\\boldsymbol{\\mu}_t(\\boldsymbol{x}_t) =\n",
    "\\sqrt{\\alpha_t} \\boldsymbol{x}_{t-1}$, and variance $\\boldsymbol{\\Sigma}_t(\\boldsymbol{x}_t) =\n",
    "(1 - \\alpha_t) \\textbf{I}$, where the form of the coefficients are\n",
    "chosen such that the variance of the latent variables stay at a\n",
    "similar scale; in other words, the encoding process is\n",
    "variance-preserving.\n",
    "\n",
    "Note that alternate Gaussian parameterizations\n",
    "are allowed, and lead to similar derivations.  The main takeaway is\n",
    "that $\\alpha_t$ is a (potentially learnable) coefficient that can vary\n",
    "with the hierarchical depth $t$, for flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7a9cf3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Encoder transitions\n",
    "\n",
    "Mathematically, the encoder transitions are defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ff257",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:27\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t-1}) = \\mathcal{N}(\\boldsymbol{x}_{t} ; \\sqrt{\\alpha_t} \\boldsymbol{x}_{t-1}, (1 - \\alpha_t) \\textbf{I}) \\label{eq:27} \\tag{4}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1abc43",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Third assumption\n",
    "\n",
    "From the third assumption, we know that $\\alpha_t$ evolves over time\n",
    "according to a fixed or learnable schedule structured such that the\n",
    "distribution of the final latent $p(\\boldsymbol{x}_T)$ is a standard Gaussian.\n",
    "We can then update the joint distribution of a Markovian VAE to write\n",
    "the joint distribution for a VDM as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b9275",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "p(\\boldsymbol{x}_{0:T}) &= p(\\boldsymbol{x}_T)\\prod_{t=1}^{T}p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t) \\\\\n",
    "\\text{where,}&\\nonumber\\\\\n",
    "p(\\boldsymbol{x}_T) &= \\mathcal{N}(\\boldsymbol{x}_T; \\boldsymbol{0}, \\textbf{I})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b30c0e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Noisification\n",
    "\n",
    "Collectively, what this set of assumptions describes is a steady\n",
    "noisification of an image input over time. We progressively corrupt an\n",
    "image by adding Gaussian noise until eventually it becomes completely\n",
    "identical to pure Gaussian noise.  See figure on next slide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10abd69a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Diffusion models, from <https://arxiv.org/abs/2208.11970>\n",
    "\n",
    "<!-- dom:FIGURE: [figures/figure2.png, width=800 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/figure2.png\" width=\"800\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17088d0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gaussian modeling\n",
    "\n",
    "Note that our encoder distributions $q(\\boldsymbol{x}_t|\\boldsymbol{x}_{t-1})$ are no\n",
    "longer parameterized by $\\boldsymbol{\\phi}$, as they are completely modeled as\n",
    "Gaussians with defined mean and variance parameters at each timestep.\n",
    "Therefore, in a VDM, we are only interested in learning conditionals\n",
    "$p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_{t})$, so that we can simulate\n",
    "new data.  After optimizing the VDM, the sampling procedure is as\n",
    "simple as sampling Gaussian noise from $p(\\boldsymbol{x}_T)$ and iteratively\n",
    "running the denoising transitions\n",
    "$p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_{t})$ for $T$ steps to generate a\n",
    "novel $\\boldsymbol{x}_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d026cd82",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Optimizing the variational diffusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75f7cb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\log p(\\boldsymbol{x})\n",
    "&= \\log \\int p(\\boldsymbol{x}_{0:T}) d\\boldsymbol{x}_{1:T}\\\\\n",
    "&= \\log \\int \\frac{p(\\boldsymbol{x}_{0:T})q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)} d\\boldsymbol{x}_{1:T}\\\\\n",
    "&= \\log \\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\frac{p(\\boldsymbol{x}_{0:T})}{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\right]\\\\\n",
    "&\\geq {\\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\log \\frac{p(\\boldsymbol{x}_{0:T})}{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\right]}\\\\\n",
    "&= {\\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\log \\frac{p(\\boldsymbol{x}_T)\\prod_{t=1}^{T}p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)}{\\prod_{t = 1}^{T}q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t-1})}\\right]}\\\\\n",
    "&= {\\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\log \\frac{p(\\boldsymbol{x}_T)p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_0|\\boldsymbol{x}_1)\\prod_{t=2}^{T}p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)}{q(\\boldsymbol{x}_T|\\boldsymbol{x}_{T-1})\\prod_{t = 1}^{T-1}q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t-1})}\\right]}\\\\\n",
    "&= {\\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\log \\frac{p(\\boldsymbol{x}_T)p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_0|\\boldsymbol{x}_1)\\prod_{t=1}^{T-1}p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t+1})}{q(\\boldsymbol{x}_T|\\boldsymbol{x}_{T-1})\\prod_{t = 1}^{T-1}q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t-1})}\\right]}\\\\\n",
    "&= {\\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\log \\frac{p(\\boldsymbol{x}_T)p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_0|\\boldsymbol{x}_1)}{q(\\boldsymbol{x}_T|\\boldsymbol{x}_{T-1})}\\right] + \\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\log \\prod_{t = 1}^{T-1}\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t+1})}{q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t-1})}\\right]}\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef390c7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Continues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e04b835",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\log p(\\boldsymbol{x})\n",
    "&= {\\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\log \\frac{p(\\boldsymbol{x}_T)p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_0|\\boldsymbol{x}_1)}{q(\\boldsymbol{x}_T|\\boldsymbol{x}_{T-1})}\\right] + \\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\log \\prod_{t = 1}^{T-1}\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t+1})}{q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t-1})}\\right]}\\\\\n",
    "&= {\\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_0|\\boldsymbol{x}_1)\\right] + \\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\log \\frac{p(\\boldsymbol{x}_T)}{q(\\boldsymbol{x}_T|\\boldsymbol{x}_{T-1})}\\right] + \\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[ \\sum_{t=1}^{T-1} \\log \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t+1})}{q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t-1})}\\right]}\\\\\n",
    "&= {\\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_0|\\boldsymbol{x}_1)\\right] + \\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\log \\frac{p(\\boldsymbol{x}_T)}{q(\\boldsymbol{x}_T|\\boldsymbol{x}_{T-1})}\\right] + \\sum_{t=1}^{T-1}\\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[ \\log \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t+1})}{q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t-1})}\\right]}\\\\\n",
    "&= {\\mathbb{E}_{q(\\boldsymbol{x}_{1}|\\boldsymbol{x}_0)}\\left[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_0|\\boldsymbol{x}_1)\\right] + \\mathbb{E}_{q(\\boldsymbol{x}_{T-1}, \\boldsymbol{x}_T|\\boldsymbol{x}_0)}\\left[\\log \\frac{p(\\boldsymbol{x}_T)}{q(\\boldsymbol{x}_T|\\boldsymbol{x}_{T-1})}\\right] + \\sum_{t=1}^{T-1}\\mathbb{E}_{q(\\boldsymbol{x}_{t-1}, \\boldsymbol{x}_t, \\boldsymbol{x}_{t+1}|\\boldsymbol{x}_0)}\\left[\\log \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t+1})}{q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t-1})}\\right]}\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3356b7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Interpretations\n",
    "\n",
    "These equations can be interpreted as\n",
    "\n",
    "* $\\mathbb{E}_{q(\\boldsymbol{x}_{1}|\\boldsymbol{x}_0)}\\left[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_0|\\boldsymbol{x}_1)\\right]$ can be interpreted as a **reconstruction term**, predicting the log probability of the original data sample given the first-step latent.  This term also appears in a vanilla VAE, and can be trained similarly.\n",
    "\n",
    "* $\\mathbb{E}_{q(\\boldsymbol{x}_{T-1}|\\boldsymbol{x}_0)}\\left[D_{KL}(q(\\boldsymbol{x}_T|\\boldsymbol{x}_{T-1})\\vert\\vert p(\\boldsymbol{x}_T))\\right]$ is a **prior matching term**; it is minimized when the final latent distribution matches the Gaussian prior.  This term requires no optimization, as it has no trainable parameters; furthermore, as we have assumed a large enough $T$ such that the final distribution is Gaussian, this term effectively becomes zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537cfa3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The last term\n",
    "\n",
    "* $\\mathbb{E}_{q(\\boldsymbol{x}_{t-1}, \\boldsymbol{x}_{t+1}|\\boldsymbol{x}_0)}\\left[D_{KL}(q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t-1})\\vert\\vert p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{t}|\\boldsymbol{x}_{t+1}))\\right]$ is a \\textit{consistency term}; it endeavors to make the distribution at $\\boldsymbol{x}_t$ consistent, from both forward and backward processes.  That is, a denoising step from a noisier image should match the corresponding noising step from a cleaner image, for every intermediate timestep; this is reflected mathematically by the KL Divergence.  This term is minimized when we train $p_{\\theta}(\\boldsymbol{x}_t|\\boldsymbol{x}_{t+1})$ to match the Gaussian distribution $q(\\boldsymbol{x}_t|\\boldsymbol{x}_{t-1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0110d2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Diffusion models, part 2, from <https://arxiv.org/abs/2208.11970>\n",
    "\n",
    "<!-- dom:FIGURE: [figures/figure3.png, width=800 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/figure3.png\" width=\"800\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e419f43",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Optimization cost\n",
    "\n",
    "The cost of optimizing a VDM is primarily dominated by the third term, since we must optimize over all timesteps $t$.\n",
    "\n",
    "Under this derivation, all three terms are computed as expectations,\n",
    "and can therefore be approximated using Monte Carlo estimates.\n",
    "However, actually optimizing the ELBO using the terms we just derived\n",
    "might be suboptimal; because the consistency term is computed as an\n",
    "expectation over two random variables $\\left\\{\\boldsymbol{x}_{t-1},\n",
    "\\boldsymbol{x}_{t+1}\\right\\}$ for every timestep, the variance of its Monte\n",
    "Carlo estimate could potentially be higher than a term that is\n",
    "estimated using only one random variable per timestep.  As it is\n",
    "computed by summing up $T-1$ consistency terms, the final estimated\n",
    "value may have high variance for large $T$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d0ffa",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More details\n",
    "\n",
    "For more details and implementaions, see Calvin Luo at <https://arxiv.org/abs/2208.11970>\n",
    "\n",
    "<!-- FIGURE: [figures/figure4.png, width=800 frac=1.0] -->"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
