{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa4f44b",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week12.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Advanced machine learning and data analysis for the physical sciences -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f7adf",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Advanced machine learning and data analysis for the physical sciences\n",
    "**Morten Hjorth-Jensen**, Department of Physics and Center for Computing in Science Education, University of Oslo, Norway\n",
    "\n",
    "Date: **April 10, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a2e6d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Plans for the week April 7-11, 2025\n",
    "\n",
    "**Generative methods, energy models and Boltzmann machines.**\n",
    "\n",
    "1. Summary of discussions on Restricted Boltzmann machines, reminder from last week\n",
    "\n",
    "2. Introduction to Variational Autoencoders (VAEs), basic mathematical formalims\n",
    "\n",
    "3. [Video of lecture](https://youtu.be/Mm9Xasy8qNw)\n",
    "\n",
    "4. [Whiteboard notes](https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/HandwrittenNotes/2025/NotesApril10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a58d4bf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reading recommendations\n",
    "1. Boltzmann machines: Goodfellow et al chapters 18.1-18.2,  20.1-20-7; To create Boltzmann machine using Keras, see Babcock and Bali chapter 4, see <https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_4/models/rbm.py>\n",
    "\n",
    "2. More on Boltzmann machines: see also Foster, chapter 7 on energy-based models at <https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/tree/main/notebooks/07_ebm/01_ebm>\n",
    "\n",
    "3. VAEs: Goodfellow et al, for VAEs see sections 20.10-20.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621e7cc",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Essential elements of generative models\n",
    "\n",
    "The aim of generative methods is to train a probability distribution $p$. The methods we will focus on are:\n",
    "1. Energy based models, with the family of Boltzmann distributions as a typical example\n",
    "\n",
    "2. Variational autoencoders, based on our discussions on autoencoders\n",
    "\n",
    "3. Generative adversarial networks (GANs) and\n",
    "\n",
    "4. Diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebb4012",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Energy models, reminders from last two weeks\n",
    "\n",
    "During the last two weeks we defined a domain $\\boldsymbol{X}$ of stochastic variables $\\boldsymbol{X}= \\{x_0,x_1, \\dots , x_{n-1}\\}$ with a pertinent probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be707d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{X})=\\prod_{x_i\\in \\boldsymbol{X}}p(x_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde60d9d",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have assumed that the random varaibles $x_i$ are all independent and identically distributed (iid).\n",
    "\n",
    "We will now assume that we can defined this function in terms of optimization parameters $\\boldsymbol{\\Theta}$, which could be the biases and weights of deep network, and a set of hidden variables we also assume to be random variables which also are iid. The domain of these variables is\n",
    "$\\boldsymbol{H}= \\{h_0,h_1, \\dots , h_{m-1}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803ee16",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Probability model\n",
    "\n",
    "We define a probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbb43f1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(x_i,h_j;\\boldsymbol{\\Theta}) = \\frac{f(x_i,h_j;\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e214c5b",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $f(x_i,h_j;\\boldsymbol{\\Theta})$ is a function which we assume is larger or\n",
    "equal than zero and obeys all properties required for a probability\n",
    "distribution and $Z(\\boldsymbol{\\Theta})$ is a normalization constant. Inspired by\n",
    "statistical mechanics, we call it often for the partition function.\n",
    "It is defined as (assuming that we have discrete probability distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076653ab",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "Z(\\boldsymbol{\\Theta})=\\sum_{x_i\\in \\boldsymbol{X}}\\sum_{h_j\\in \\boldsymbol{H}} f(x_i,h_j;\\boldsymbol{\\Theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc0e2b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Marginal and conditional probabilities\n",
    "\n",
    "We can in turn define the marginal probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d6dc7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(x_i;\\boldsymbol{\\Theta}) = \\frac{\\sum_{h_j\\in \\boldsymbol{H}}f(x_i,h_j;\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142c41c",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0648e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(h_i;\\boldsymbol{\\Theta}) = \\frac{\\sum_{x_i\\in \\boldsymbol{X}}f(x_i,h_j;\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41ff60",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Change of notation\n",
    "\n",
    "**Note the change to a vector notation**. A variable like $\\boldsymbol{x}$\n",
    "represents now a specific **configuration**. We can generate an infinity\n",
    "of such configurations. The final partition function is then the sum\n",
    "over all such possible configurations, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bfa220",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "Z(\\boldsymbol{\\Theta})=\\sum_{x_i\\in \\boldsymbol{X}}\\sum_{h_j\\in \\boldsymbol{H}} f(x_i,h_j;\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb4344",
   "metadata": {
    "editable": true
   },
   "source": [
    "changes to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788f92e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "Z(\\boldsymbol{\\Theta})=\\sum_{\\boldsymbol{x}}\\sum_{\\boldsymbol{h}} f(\\boldsymbol{x},\\boldsymbol{h};\\boldsymbol{\\Theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a2a10",
   "metadata": {
    "editable": true
   },
   "source": [
    "If we have a binary set of variable $x_i$ and $h_j$ and $M$ values of $x_i$ and $N$ values of $h_j$ we have in total $2^M$ and $2^N$ possible $\\boldsymbol{x}$ and $\\boldsymbol{h}$ configurations, respectively.\n",
    "\n",
    "We see that even for the modest binary case, we can easily approach a\n",
    "number of configuration which is not possible to deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834e410",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Optimization problem\n",
    "\n",
    "At the end, we are not interested in the probabilities of the hidden variables. The probability we thus want to optimize is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2a899",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{X};\\boldsymbol{\\Theta})=\\prod_{x_i\\in \\boldsymbol{X}}p(x_i;\\boldsymbol{\\Theta})=\\prod_{x_i\\in \\boldsymbol{X}}\\left(\\frac{\\sum_{h_j\\in \\boldsymbol{H}}f(x_i,h_j;\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})}\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760724f9",
   "metadata": {
    "editable": true
   },
   "source": [
    "which we rewrite as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af5bde",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{X};\\boldsymbol{\\Theta})=\\frac{1}{Z(\\boldsymbol{\\Theta})}\\prod_{x_i\\in \\boldsymbol{X}}\\left(\\sum_{h_j\\in \\boldsymbol{H}}f(x_i,h_j;\\boldsymbol{\\Theta})\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a2c1f4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Further simplifications\n",
    "\n",
    "We simplify further by rewriting it as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e17fd",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{X};\\boldsymbol{\\Theta})=\\frac{1}{Z(\\boldsymbol{\\Theta})}\\prod_{x_i\\in \\boldsymbol{X}}f(x_i;\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19302e44",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we used $p(x_i;\\boldsymbol{\\Theta}) = \\sum_{h_j\\in \\boldsymbol{H}}f(x_i,h_j;\\boldsymbol{\\Theta})$.\n",
    "The optimization problem is then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aa7603",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\mathrm{arg} \\hspace{0.1cm}\\max_{\\boldsymbol{\\boldsymbol{\\Theta}}\\in {\\mathbb{R}}^{p}}} \\hspace{0.1cm}p(\\boldsymbol{X};\\boldsymbol{\\Theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5378643",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Optimizing the logarithm instead\n",
    "\n",
    "Computing the derivatives with respect to the parameters $\\boldsymbol{\\Theta}$ is\n",
    "easier (and equivalent) with taking the logarithm of the\n",
    "probability. We will thus optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f1313b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\mathrm{arg} \\hspace{0.1cm}\\max_{\\boldsymbol{\\boldsymbol{\\Theta}}\\in {\\mathbb{R}}^{p}}} \\hspace{0.1cm}\\log{p(\\boldsymbol{X};\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8ecab6",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5065c5e6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{p(\\boldsymbol{X};\\boldsymbol{\\Theta})}=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d775a8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Expression for the gradients\n",
    "This leads to the following equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18578e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{p(\\boldsymbol{X};\\boldsymbol{\\Theta})}=\\nabla_{\\boldsymbol{\\Theta}}\\left(\\sum_{x_i\\in \\boldsymbol{X}}\\log{f(x_i;\\boldsymbol{\\Theta})}\\right)-\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53efe74",
   "metadata": {
    "editable": true
   },
   "source": [
    "The first term is called the positive phase and we assume that we have a model for the function $f$ from which we can sample values. Below we will develop an explicit model for this.\n",
    "The second term is called the negative phase and is the one which leads to more difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5512e140",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The derivative of the partition function\n",
    "\n",
    "The partition function, defined above as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffffe57",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "Z(\\boldsymbol{\\Theta})=\\sum_{x_i\\in \\boldsymbol{X}}\\sum_{h_j\\in \\boldsymbol{H}} f(x_i,h_j;\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc379c8",
   "metadata": {
    "editable": true
   },
   "source": [
    "is in general the most problematic term. In principle both $x$ and $h$ can span large degrees of freedom, if not even infinitely many ones, and computing the partition function itself is often not desirable or even feasible. The above derivative of the partition function can however be written in terms of an expectation value which is in turn evaluated  using Monte Carlo sampling and the theory of Markov chains, popularly shortened to MCMC (or just MC$^2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823fd81",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Explicit expression for the derivative\n",
    "We can rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c4752",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\frac{\\nabla_{\\boldsymbol{\\Theta}}Z(\\boldsymbol{\\Theta})}{Z(\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c37cc5a",
   "metadata": {
    "editable": true
   },
   "source": [
    "which reads in more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d40e2e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\frac{\\nabla_{\\boldsymbol{\\Theta}} \\sum_{x_i\\in \\boldsymbol{X}}f(x_i;\\boldsymbol{\\Theta})   }{Z(\\boldsymbol{\\Theta})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235456c4",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can rewrite the function $f$ (we have assumed that is larger or\n",
    "equal than zero) as $f=\\exp{\\log{f}}$. We can then reqrite the last\n",
    "equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058d3422",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\frac{ \\sum_{x_i\\in \\boldsymbol{X}} \\nabla_{\\boldsymbol{\\Theta}}\\exp{\\log{f(x_i;\\boldsymbol{\\Theta})}}   }{Z(\\boldsymbol{\\Theta})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba688ac",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Final expression\n",
    "\n",
    "Taking the derivative gives us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81107c5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\frac{ \\sum_{x_i\\in \\boldsymbol{X}}f(x_i;\\boldsymbol{\\Theta}) \\nabla_{\\boldsymbol{\\Theta}}\\log{f(x_i;\\boldsymbol{\\Theta})}   }{Z(\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f2154",
   "metadata": {
    "editable": true
   },
   "source": [
    "which is the expectation value of $\\log{f}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7d9ef",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\sum_{x_i\\in \\boldsymbol{X}}p(x_i;\\boldsymbol{\\Theta}) \\nabla_{\\boldsymbol{\\Theta}}\\log{f(x_i;\\boldsymbol{\\Theta})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d0bc0",
   "metadata": {
    "editable": true
   },
   "source": [
    "that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b52aa6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\Theta}}\\log{Z(\\boldsymbol{\\Theta})}=\\mathbb{E}(\\log{f(x_i;\\boldsymbol{\\Theta})}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e073c25",
   "metadata": {
    "editable": true
   },
   "source": [
    "This quantity is evaluated using Monte Carlo sampling, with Gibbs\n",
    "sampling as the standard sampling rule.  Before we discuss the\n",
    "explicit algorithms, we need to remind ourselves about Markov chains\n",
    "and sampling rules like the Metropolis-Hastings algorithm and Gibbs\n",
    "sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5db2f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Positive and negative phases\n",
    "As discussed earlier, the data-dependent term in the gradient is known as the positive phase\n",
    "of the gradient, while the model-dependent term is known as the\n",
    "negative phase of the gradient. The aim of the training is to lower\n",
    "the energy of configurations that are near observed data points\n",
    "(increasing their probability), and raising the energy of\n",
    "configurations that are far from observed data points (decreasing\n",
    "their probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3fcb57",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Theory of Variational Autoencoders\n",
    "\n",
    "Let us remind ourself about what an autoencoder is, see the jupyter-notebooks at <https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week8/ipynb/week8.ipynb> and <https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week9/ipynb/week9.ipynb>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b79118f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Autoencoder again\n",
    "\n",
    "Autoencoders are neural networks where the outputs are its own\n",
    "inputs. They are split into an **encoder part**\n",
    "which maps the input $\\boldsymbol{x}$ via a function $f(\\boldsymbol{x},\\boldsymbol{W})$ (this\n",
    "is the encoder part) to a **so-called code part** (or intermediate part)\n",
    "with the result $\\boldsymbol{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df03ccf",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{h} = f(\\boldsymbol{x},\\boldsymbol{W})),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e403f114",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\boldsymbol{W}$ are the weights to be determined.  The **decoder** parts maps, via its own parameters (weights given by the matrix $\\boldsymbol{V}$ and its own biases) to \n",
    "the final ouput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e20adb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{x}} = g(\\boldsymbol{h},\\boldsymbol{V})).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de8752",
   "metadata": {
    "editable": true
   },
   "source": [
    "The goal is to minimize the construction error, often done by optimizing the means squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6baf8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Schematic image of an Autoencoder\n",
    "\n",
    "<!-- dom:FIGURE: [figures/ae1.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/ae1.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c406364d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematics of Variational Autoencoders\n",
    "\n",
    "We have defined earlier a probability (marginal) distribution with hidden variables $\\boldsymbol{h}$ and parameters $\\boldsymbol{\\Theta}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec8ac4",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x};\\boldsymbol{\\Theta}) = \\int d\\boldsymbol{h}p(\\boldsymbol{x},\\boldsymbol{h};\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec62d71",
   "metadata": {
    "editable": true
   },
   "source": [
    "for continuous variables $\\boldsymbol{h}$ and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d800fb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x};\\boldsymbol{\\Theta}) = \\sum_{\\boldsymbol{h}}p(\\boldsymbol{x},\\boldsymbol{h};\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873c3a3b",
   "metadata": {
    "editable": true
   },
   "source": [
    "for discrete stochastic events $\\boldsymbol{h}$. The variables $\\boldsymbol{h}$ are normally called the **latent variables** in the theory of autoencoders. We will also call then for that here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bbd711",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using the conditional probability\n",
    "\n",
    "Using the the definition of the conditional probabilities $p(\\boldsymbol{x}\\vert\\boldsymbol{h};\\boldsymbol{\\Theta})$, $p(\\boldsymbol{h}\\vert\\boldsymbol{x};\\boldsymbol{\\Theta})$ and \n",
    "and the prior $p(\\boldsymbol{h})$, we can rewrite the above equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5026856",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x};\\boldsymbol{\\Theta}) = \\sum_{\\boldsymbol{h}}p(\\boldsymbol{x}\\vert\\boldsymbol{h};\\boldsymbol{\\Theta})p(\\boldsymbol{h}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d6699",
   "metadata": {
    "editable": true
   },
   "source": [
    "which allows us to make the dependence of $\\boldsymbol{x}$ on $\\boldsymbol{h}$\n",
    "explicit by using the law of total probability. The intuition behind\n",
    "this approach for finding the marginal probability for $\\boldsymbol{x}$ is to\n",
    "optimize the above equations with respect to the parameters\n",
    "$\\boldsymbol{\\Theta}$.  This is done normally by maximizing the probability,\n",
    "the so-called maximum-likelihood approach discussed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b554f42",
   "metadata": {
    "editable": true
   },
   "source": [
    "## VAEs versus autoencoders\n",
    "\n",
    "This trained probability is assumed to be able to produce similar\n",
    "samples as the input.  In VAEs it is then common to compare via for\n",
    "example the mean-squared error or the cross-entropy the predicted\n",
    "values with the input values.  Compared with autoencoders, we are now\n",
    "producing a probability instead of a functions which mimicks the\n",
    "input.\n",
    "\n",
    "In VAEs, the choice of this output distribution is often Gaussian,\n",
    "meaning that the conditional probability is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b86d4",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x}\\vert\\boldsymbol{h};\\boldsymbol{\\Theta})=N(\\boldsymbol{x}\\vert f(\\boldsymbol{h};\\boldsymbol{\\Theta}), \\sigma^2\\times \\boldsymbol{I}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c16da",
   "metadata": {
    "editable": true
   },
   "source": [
    "with mean value given by the function $f(\\boldsymbol{h};\\boldsymbol{\\Theta})$ and a\n",
    "diagonal covariance matrix multiplied by a parameter $\\sigma^2$ which\n",
    "is treated as a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63653ad1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "By having a Gaussian distribution, we can use gradient descent (or any\n",
    "other optimization technique) to increase $p(\\boldsymbol{x};\\boldsymbol{\\Theta})$ by\n",
    "making $f(\\boldsymbol{h};\\boldsymbol{\\Theta})$ approach $\\boldsymbol{x}$ for some $\\boldsymbol{h}$,\n",
    "gradually making the training data more likely under the generative\n",
    "model. The important property is simply that the marginal probability\n",
    "can be computed, and it is continuous in $\\boldsymbol{\\Theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6669ff8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Are VAEs just modified autoencoders?\n",
    "\n",
    "The mathematical basis of VAEs actually has relatively little to do\n",
    "with classical autoencoders, for example the sparse autoencoders or\n",
    "denoising autoencoders discussed earlier.\n",
    "\n",
    "VAEs approximately maximize the probability equation discussed\n",
    "above. They are called autoencoders only because the final training\n",
    "objective that derives from this setup does have an encoder and a\n",
    "decoder, and resembles a traditional autoencoder. Unlike sparse\n",
    "autoencoders, there are generally no tuning parameters analogous to\n",
    "the sparsity penalties. And unlike sparse and denoising autoencoders,\n",
    "we can sample directly from $p(\\boldsymbol{x})$ without performing Markov\n",
    "Chain Monte Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c22c72",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Training VAEs\n",
    "\n",
    "To solve the integral or sum for $p(\\boldsymbol{x})$, there are two problems\n",
    "that VAEs must deal with: how to define the latent variables $\\boldsymbol{h}$,\n",
    "that is decide what information they represent, and how to deal with\n",
    "the integral over $\\boldsymbol{h}$.  VAEs give a definite answer to both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089e717",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Motivation from Kingma and Welling, An Introduction to Variational Autoencoders, <https://arxiv.org/abs/1906.02691>\n",
    "\n",
    "*There are many reasons why generative modeling is attractive. First,\n",
    "we can express physical laws and constraints into the generative\n",
    "process while details that we donâ€™t know or care about, i.e. nuisance\n",
    "variables, are treated as noise. The resulting models are usually\n",
    "highly intuitive and interpretable and by testing them against\n",
    "observations we can confirm or reject our theories about how the world\n",
    "works.  Another reason for trying to understand the generative process\n",
    "of data is that it naturally expresses causal relations of the\n",
    "world. Causal relations have the great advantage that they generalize\n",
    "much better to new situations than mere correlations. For instance,\n",
    "once we understand the generative process of an earthquake, we can use\n",
    "that knowledge both in California and in Chile.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d105344d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematics of  VAEs\n",
    "\n",
    "We want to train the marginal probability with some latent varrables $\\boldsymbol{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c27f28",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x};\\boldsymbol{\\Theta}) = \\int d\\boldsymbol{h}p(\\boldsymbol{x},\\boldsymbol{h};\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378cf56",
   "metadata": {
    "editable": true
   },
   "source": [
    "for the continuous version (see previous slides for the discrete variant)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a93e0af",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using the KL divergence\n",
    "\n",
    "In practice, for most $\\boldsymbol{h}$, $p(\\boldsymbol{x}\\vert \\boldsymbol{h}; \\boldsymbol{\\Theta})$\n",
    "will be nearly zero, and hence contributes almost nothing to our\n",
    "estimate of $p(\\boldsymbol{x})$.\n",
    "\n",
    "The key idea behind the variational autoencoder is to attempt to\n",
    "sample values of $\\boldsymbol{h}$ that are likely to have produced $\\boldsymbol{x}$,\n",
    "and compute $p(\\boldsymbol{x})$ just from those.\n",
    "\n",
    "This means that we need a new function $Q(\\boldsymbol{h}|\\boldsymbol{x})$ which can\n",
    "take a value of $\\boldsymbol{x}$ and give us a distribution over $\\boldsymbol{h}$\n",
    "values that are likely to produce $\\boldsymbol{x}$.  Hopefully the space of\n",
    "$\\boldsymbol{h}$ values that are likely under $Q$ will be much smaller than\n",
    "the space of all $\\boldsymbol{h}$'s that are likely under the prior\n",
    "$p(\\boldsymbol{h})$.  This lets us, for example, compute $E_{\\boldsymbol{h}\\sim\n",
    "Q}p(\\boldsymbol{x}\\vert \\boldsymbol{h})$ relatively easily. Note that we drop\n",
    "$\\boldsymbol{\\Theta}$ from here and for notational simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45131d6b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Kullback-Leibler again\n",
    "\n",
    "However, if $\\boldsymbol{h}$ is sampled from an arbitrary distribution with\n",
    "PDF $Q(\\boldsymbol{h})$, which is not $\\mathcal{N}(0,I)$, then how does that\n",
    "help us optimize $p(\\boldsymbol{x})$?\n",
    "\n",
    "The first thing we need to do is relate\n",
    "$E_{\\boldsymbol{h}\\sim Q}P(\\boldsymbol{x}\\vert \\boldsymbol{h})$ and $p(\\boldsymbol{x})$.  We will see where $Q$ comes from later.\n",
    "\n",
    "The relationship between $E_{\\boldsymbol{h}\\sim Q}p(\\boldsymbol{x}\\vert \\boldsymbol{h})$ and $p(\\boldsymbol{x})$ is one of the cornerstones of variational Bayesian methods.\n",
    "We begin with the definition of Kullback-Leibler divergence (KL divergence or $\\mathcal{D}$) between $p(\\boldsymbol{h}\\vert \\boldsymbol{x})$ and $Q(\\boldsymbol{h})$, for some arbitrary $Q$ (which may or may not depend on $\\boldsymbol{x}$):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b7567",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathcal{D}\\left[Q(\\boldsymbol{h})\\|p(\\boldsymbol{h}|\\boldsymbol{x})\\right]=E_{\\boldsymbol{h}\\sim Q}\\left[\\log Q(\\boldsymbol{h}) - \\log p(\\boldsymbol{h}|\\boldsymbol{x}) \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d72517",
   "metadata": {
    "editable": true
   },
   "source": [
    "## And applying Bayes rule\n",
    "\n",
    "We can get both $p(\\boldsymbol{x})$ and $p(\\boldsymbol{x}\\vert \\boldsymbol{h})$ into this equation by applying Bayes rule to $p(\\boldsymbol{h}|\\boldsymbol{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b00a6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathcal{D}\\left[Q(\\boldsymbol{h})\\|p(\\boldsymbol{h}\\vert \\boldsymbol{x})\\right]=E_{\\boldsymbol{h}\\sim Q}\\left[\\log Q(\\boldsymbol{h}) - \\log p(\\boldsymbol{x}|\\boldsymbol{h}) - \\log p(\\boldsymbol{h}) \\right] + \\log p(\\boldsymbol{x}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa3a7c",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, $\\log p(\\boldsymbol{x})$ comes out of the expectation because it does not depend on $\\boldsymbol{h}$.\n",
    "Negating both sides, rearranging, and contracting part of $E_{\\boldsymbol{h}\\sim Q}$ into a KL-divergence terms yields:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b9edb3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\log p(\\boldsymbol{x}) - \\mathcal{D}\\left[Q(\\boldsymbol{h})\\|p(\\boldsymbol{h}\\vert \\boldsymbol{x})\\right]=E_{\\boldsymbol{h}\\sim Q}\\left[\\log p(\\boldsymbol{x}\\vert\\boldsymbol{h})  \\right] - \\mathcal{D}\\left[Q(\\boldsymbol{h})\\|P(\\boldsymbol{h})\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947832fd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Rearranging\n",
    "\n",
    "Using Bayes rule we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73da6a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "E_{\\boldsymbol{h}\\sim Q}\\left[\\log p(y_i|\\boldsymbol{h},x_i)\\right]=E_{\\boldsymbol{h}\\sim Q}\\left[\\log p(\\boldsymbol{h}|y_i,x_i) - \\log p(\\boldsymbol{h}|x_i) + \\log p(y_i|x_i) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589a3535",
   "metadata": {
    "editable": true
   },
   "source": [
    "Rearranging the terms and subtracting $E_{\\boldsymbol{h}\\sim Q}\\log Q(\\boldsymbol{h})$ from both sides gives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e098e48e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "\\log P(y_i|x_i) - E_{\\boldsymbol{h}\\sim Q}\\left[\\log Q(\\boldsymbol{h})-\\log p(\\boldsymbol{h}|x_i,y_i)\\right]=\\hspace{10em}\\\\\n",
    "\\hspace{10em}E_{\\boldsymbol{h}\\sim Q}\\left[\\log p(y_i|\\boldsymbol{h},x_i)+\\log p(\\boldsymbol{h}|x_i)-\\log Q(\\boldsymbol{h})\\right]\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e13be",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that $\\boldsymbol{x}$ is fixed, and $Q$ can be \\textit{any} distribution, not\n",
    "just a distribution which does a good job mapping $\\boldsymbol{x}$ to the $\\boldsymbol{h}$'s\n",
    "that can produce $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44758633",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Inferring the probability\n",
    "\n",
    "Since we are interested in inferring $p(\\boldsymbol{x})$, it makes sense to\n",
    "construct a $Q$ which \\textit{does} depend on $\\boldsymbol{x}$, and in particular,\n",
    "one which makes $\\mathcal{D}\\left[Q(\\boldsymbol{h})\\|p(\\boldsymbol{h}|\\boldsymbol{x})\\right]$ small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f411be0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\log p(\\boldsymbol{x}) - \\mathcal{D}\\left[Q(\\boldsymbol{h}|\\boldsymbol{x})\\|p(\\boldsymbol{h}|\\boldsymbol{x})\\right]=E_{\\boldsymbol{h}\\sim Q}\\left[\\log p(\\boldsymbol{x}|\\boldsymbol{h})  \\right] - \\mathcal{D}\\left[Q(\\boldsymbol{h}|\\boldsymbol{x})\\|p(\\boldsymbol{h})\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b5afe",
   "metadata": {
    "editable": true
   },
   "source": [
    "Hence, during training, it makes sense to choose a $Q$ which will make\n",
    "$E_{\\boldsymbol{h}\\sim Q}[\\log Q(\\boldsymbol{h})-$ $\\log p(\\boldsymbol{h}|x_i,y_i)]$ (a\n",
    "$\\mathcal{D}$-divergence) small, such that the right hand side is a\n",
    "close approximation to $\\log p(y_i|y_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f0e4a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Central equation of VAEs\n",
    "\n",
    "This equation serves as the core of the variational autoencoder, and\n",
    "it is worth spending some time thinking about what it means.\n",
    "\n",
    "1. The left hand side has the quantity we want to maximize, namely $\\log p(\\boldsymbol{x})$ plus an error term.\n",
    "\n",
    "2. The right hand side is something we can optimize via stochastic gradient descent given the right choice of $Q$."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
